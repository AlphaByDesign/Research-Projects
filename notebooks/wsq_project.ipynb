{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Creating a Time Series Model on BTCUSD\n",
    "\n",
    "* Using an AR model, with linear regression ML technique\n",
    "* If initial obervation shows promise, we will add features within the time series and possibly later correlated pair\n",
    "* We will be using a linear model but using Pytorch for the weight and bias"
   ],
   "id": "1344069638934c2e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:56.990937Z",
     "start_time": "2025-11-06T12:22:43.454607Z"
    }
   },
   "source": [
    "from binance.client import Client\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Initialize the Binance client (public data only)\n",
    "client = Client()\n",
    "\n",
    "# Parameters\n",
    "symbol = \"BTCUSDT\"  # Change to any pair you want\n",
    "interval = \"4h\"  # e.g. \"1m\", \"15m\", \"1h\", \"1d\"\n",
    "start_date = \"2020-01-01\"  # You can use e.g. \"1 Jan, 2020\"\n",
    "end_date = datetime.now().strftime(\"%d %b, %Y %H:%M:%S\")\n",
    "# Max number of Auto-regressive lags\n",
    "max_lags =4\n",
    "# Forecast horizon in steps\n",
    "forcast_horizon = 1\n",
    "\n",
    "# Download historical klines\n",
    "klines = client.get_historical_klines(symbol, interval, start_date, end_date)\n",
    "\n",
    "# Convert to DataFrame\n",
    "cols = [\n",
    "    \"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "    \"close_time\", \"quote_asset_volume\", \"num_trades\",\n",
    "    \"taker_buy_base\", \"taker_buy_quote\", \"ignore\"\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(klines, columns=cols)\n",
    "\n",
    "# Convert datatypes\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")\n",
    "numeric_cols = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "df[numeric_cols] = df[numeric_cols].astype(float)\n",
    "\n",
    "# Set index\n",
    "df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "# Drop unused columns\n",
    "df = df[[\"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
    "\n",
    "print(df.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        open     high      low    close       volume\n",
      "timestamp                                                           \n",
      "2020-01-01 00:00:00  7195.24  7245.00  7175.46  7225.01  2833.749180\n",
      "2020-01-01 04:00:00  7225.00  7236.27  7199.11  7209.83  2061.295051\n",
      "2020-01-01 08:00:00  7209.83  7237.73  7180.00  7197.20  3166.654361\n",
      "2020-01-01 12:00:00  7197.20  7255.00  7196.15  7234.19  3492.537459\n",
      "2020-01-01 16:00:00  7234.20  7249.99  7214.00  7229.48  2980.583291\n"
     ]
    }
   ],
   "execution_count": 523
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inspect data",
   "id": "8537a369fcdafa37"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:57.022863Z",
     "start_time": "2025-11-06T12:22:57.009344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df.describe(include=\"all\")\n",
    "df['close'].value_counts()\n",
    "df.isna().mean()"
   ],
   "id": "7fe00a8a3b230a69",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "open      0.0\n",
       "high      0.0\n",
       "low       0.0\n",
       "close     0.0\n",
       "volume    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 524
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:57.078197Z",
     "start_time": "2025-11-06T12:22:57.069166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ts = df\n",
    "ts"
   ],
   "id": "17ede1e519168225",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                          open       high        low      close       volume\n",
       "timestamp                                                                   \n",
       "2020-01-01 00:00:00    7195.24    7245.00    7175.46    7225.01  2833.749180\n",
       "2020-01-01 04:00:00    7225.00    7236.27    7199.11    7209.83  2061.295051\n",
       "2020-01-01 08:00:00    7209.83    7237.73    7180.00    7197.20  3166.654361\n",
       "2020-01-01 12:00:00    7197.20    7255.00    7196.15    7234.19  3492.537459\n",
       "2020-01-01 16:00:00    7234.20    7249.99    7214.00    7229.48  2980.583291\n",
       "...                        ...        ...        ...        ...          ...\n",
       "2025-11-05 20:00:00  104008.09  104534.74  103305.05  103885.16  2687.898330\n",
       "2025-11-06 00:00:00  103885.16  103933.33  102716.26  103636.03  3781.794370\n",
       "2025-11-06 04:00:00  103636.92  104200.00  102910.66  103185.48  2970.865520\n",
       "2025-11-06 08:00:00  103184.75  103440.00  102651.63  103227.58  2657.253150\n",
       "2025-11-06 12:00:00  103227.59  103333.18  102830.83  102867.05   290.340880\n",
       "\n",
       "[12819 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00</th>\n",
       "      <td>7195.24</td>\n",
       "      <td>7245.00</td>\n",
       "      <td>7175.46</td>\n",
       "      <td>7225.01</td>\n",
       "      <td>2833.749180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 04:00:00</th>\n",
       "      <td>7225.00</td>\n",
       "      <td>7236.27</td>\n",
       "      <td>7199.11</td>\n",
       "      <td>7209.83</td>\n",
       "      <td>2061.295051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 08:00:00</th>\n",
       "      <td>7209.83</td>\n",
       "      <td>7237.73</td>\n",
       "      <td>7180.00</td>\n",
       "      <td>7197.20</td>\n",
       "      <td>3166.654361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 12:00:00</th>\n",
       "      <td>7197.20</td>\n",
       "      <td>7255.00</td>\n",
       "      <td>7196.15</td>\n",
       "      <td>7234.19</td>\n",
       "      <td>3492.537459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 16:00:00</th>\n",
       "      <td>7234.20</td>\n",
       "      <td>7249.99</td>\n",
       "      <td>7214.00</td>\n",
       "      <td>7229.48</td>\n",
       "      <td>2980.583291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-05 20:00:00</th>\n",
       "      <td>104008.09</td>\n",
       "      <td>104534.74</td>\n",
       "      <td>103305.05</td>\n",
       "      <td>103885.16</td>\n",
       "      <td>2687.898330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-06 00:00:00</th>\n",
       "      <td>103885.16</td>\n",
       "      <td>103933.33</td>\n",
       "      <td>102716.26</td>\n",
       "      <td>103636.03</td>\n",
       "      <td>3781.794370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-06 04:00:00</th>\n",
       "      <td>103636.92</td>\n",
       "      <td>104200.00</td>\n",
       "      <td>102910.66</td>\n",
       "      <td>103185.48</td>\n",
       "      <td>2970.865520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-06 08:00:00</th>\n",
       "      <td>103184.75</td>\n",
       "      <td>103440.00</td>\n",
       "      <td>102651.63</td>\n",
       "      <td>103227.58</td>\n",
       "      <td>2657.253150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-06 12:00:00</th>\n",
       "      <td>103227.59</td>\n",
       "      <td>103333.18</td>\n",
       "      <td>102830.83</td>\n",
       "      <td>102867.05</td>\n",
       "      <td>290.340880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12819 rows Ã— 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 525
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:57.391754Z",
     "start_time": "2025-11-06T12:22:57.242595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ts['close'].plot(figsize=(10,5),title=symbol)\n",
    "# can experiment with Altair for dynamic charts later."
   ],
   "id": "5d88dc99f5a74a37",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'BTCUSDT'}, xlabel='timestamp'>"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 526
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Feature Engineering\n",
   "id": "681857bde29a646e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create target and lagged features using log returns",
   "id": "4b345632446ddccc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:57.451776Z",
     "start_time": "2025-11-06T12:22:57.440314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creating log return within ts dataframe.\n",
    "ts = ts.sort_index()\n",
    "ts['close_log_return']= np.log(ts['close']/ts['close'].shift(forcast_horizon))\n",
    "ts"
   ],
   "id": "4004e1b2505c135d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                          open       high        low      close       volume  \\\n",
       "timestamp                                                                      \n",
       "2020-01-01 00:00:00    7195.24    7245.00    7175.46    7225.01  2833.749180   \n",
       "2020-01-01 04:00:00    7225.00    7236.27    7199.11    7209.83  2061.295051   \n",
       "2020-01-01 08:00:00    7209.83    7237.73    7180.00    7197.20  3166.654361   \n",
       "2020-01-01 12:00:00    7197.20    7255.00    7196.15    7234.19  3492.537459   \n",
       "2020-01-01 16:00:00    7234.20    7249.99    7214.00    7229.48  2980.583291   \n",
       "...                        ...        ...        ...        ...          ...   \n",
       "2025-11-05 20:00:00  104008.09  104534.74  103305.05  103885.16  2687.898330   \n",
       "2025-11-06 00:00:00  103885.16  103933.33  102716.26  103636.03  3781.794370   \n",
       "2025-11-06 04:00:00  103636.92  104200.00  102910.66  103185.48  2970.865520   \n",
       "2025-11-06 08:00:00  103184.75  103440.00  102651.63  103227.58  2657.253150   \n",
       "2025-11-06 12:00:00  103227.59  103333.18  102830.83  102867.05   290.340880   \n",
       "\n",
       "                     close_log_return  \n",
       "timestamp                              \n",
       "2020-01-01 00:00:00               NaN  \n",
       "2020-01-01 04:00:00         -0.002103  \n",
       "2020-01-01 08:00:00         -0.001753  \n",
       "2020-01-01 12:00:00          0.005126  \n",
       "2020-01-01 16:00:00         -0.000651  \n",
       "...                               ...  \n",
       "2025-11-05 20:00:00         -0.001183  \n",
       "2025-11-06 00:00:00         -0.002401  \n",
       "2025-11-06 04:00:00         -0.004357  \n",
       "2025-11-06 08:00:00          0.000408  \n",
       "2025-11-06 12:00:00         -0.003499  \n",
       "\n",
       "[12819 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>close_log_return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00</th>\n",
       "      <td>7195.24</td>\n",
       "      <td>7245.00</td>\n",
       "      <td>7175.46</td>\n",
       "      <td>7225.01</td>\n",
       "      <td>2833.749180</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 04:00:00</th>\n",
       "      <td>7225.00</td>\n",
       "      <td>7236.27</td>\n",
       "      <td>7199.11</td>\n",
       "      <td>7209.83</td>\n",
       "      <td>2061.295051</td>\n",
       "      <td>-0.002103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 08:00:00</th>\n",
       "      <td>7209.83</td>\n",
       "      <td>7237.73</td>\n",
       "      <td>7180.00</td>\n",
       "      <td>7197.20</td>\n",
       "      <td>3166.654361</td>\n",
       "      <td>-0.001753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 12:00:00</th>\n",
       "      <td>7197.20</td>\n",
       "      <td>7255.00</td>\n",
       "      <td>7196.15</td>\n",
       "      <td>7234.19</td>\n",
       "      <td>3492.537459</td>\n",
       "      <td>0.005126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 16:00:00</th>\n",
       "      <td>7234.20</td>\n",
       "      <td>7249.99</td>\n",
       "      <td>7214.00</td>\n",
       "      <td>7229.48</td>\n",
       "      <td>2980.583291</td>\n",
       "      <td>-0.000651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-05 20:00:00</th>\n",
       "      <td>104008.09</td>\n",
       "      <td>104534.74</td>\n",
       "      <td>103305.05</td>\n",
       "      <td>103885.16</td>\n",
       "      <td>2687.898330</td>\n",
       "      <td>-0.001183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-06 00:00:00</th>\n",
       "      <td>103885.16</td>\n",
       "      <td>103933.33</td>\n",
       "      <td>102716.26</td>\n",
       "      <td>103636.03</td>\n",
       "      <td>3781.794370</td>\n",
       "      <td>-0.002401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-06 04:00:00</th>\n",
       "      <td>103636.92</td>\n",
       "      <td>104200.00</td>\n",
       "      <td>102910.66</td>\n",
       "      <td>103185.48</td>\n",
       "      <td>2970.865520</td>\n",
       "      <td>-0.004357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-06 08:00:00</th>\n",
       "      <td>103184.75</td>\n",
       "      <td>103440.00</td>\n",
       "      <td>102651.63</td>\n",
       "      <td>103227.58</td>\n",
       "      <td>2657.253150</td>\n",
       "      <td>0.000408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-06 12:00:00</th>\n",
       "      <td>103227.59</td>\n",
       "      <td>103333.18</td>\n",
       "      <td>102830.83</td>\n",
       "      <td>102867.05</td>\n",
       "      <td>290.340880</td>\n",
       "      <td>-0.003499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12819 rows Ã— 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 527
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:57.581713Z",
     "start_time": "2025-11-06T12:22:57.576027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create lagged features\n",
    "target = 'close_log_return'\n",
    "max_lags = 4\n",
    "forcast_horizon = 1\n",
    "\n",
    "# create 4 lagged features\n",
    "\n",
    "ts = ts.copy() # this is to avoid setting with copy warning\n",
    "\n",
    "ts[f'{target}_lag_1'] = ts[target].shift(forcast_horizon * 1)\n",
    "ts[f'{target}_lag_2'] = ts[target].shift(forcast_horizon * 2)\n",
    "ts[f'{target}_lag_3'] = ts[target].shift(forcast_horizon * 3)\n",
    "ts[f'{target}_lag_4'] = ts[target].shift(forcast_horizon * 4)\n",
    "\n"
   ],
   "id": "39151d4d9ce8b504",
   "outputs": [],
   "execution_count": 528
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:57.641390Z",
     "start_time": "2025-11-06T12:22:57.630693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# better practice would be ts = ts.dropna()\n",
    "ts.dropna(inplace=True)\n",
    "ts"
   ],
   "id": "70780129f756cd25",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                          open       high        low      close       volume  \\\n",
       "timestamp                                                                      \n",
       "2020-01-01 20:00:00    7229.48    7242.98    7175.15    7200.85  2257.568823   \n",
       "2020-01-02 00:00:00    7200.77    7212.50    7120.37    7129.61  3739.354832   \n",
       "2020-01-02 04:00:00    7129.25    7161.00    7105.00    7110.57  4057.961355   \n",
       "2020-01-02 08:00:00    7110.98    7180.00    7109.11    7139.79  4162.203010   \n",
       "2020-01-02 12:00:00    7139.73    7163.40    7107.43    7130.98  4179.041833   \n",
       "...                        ...        ...        ...        ...          ...   \n",
       "2025-11-05 20:00:00  104008.09  104534.74  103305.05  103885.16  2687.898330   \n",
       "2025-11-06 00:00:00  103885.16  103933.33  102716.26  103636.03  3781.794370   \n",
       "2025-11-06 04:00:00  103636.92  104200.00  102910.66  103185.48  2970.865520   \n",
       "2025-11-06 08:00:00  103184.75  103440.00  102651.63  103227.58  2657.253150   \n",
       "2025-11-06 12:00:00  103227.59  103333.18  102830.83  102867.05   290.340880   \n",
       "\n",
       "                     close_log_return  close_log_return_lag_1  \\\n",
       "timestamp                                                       \n",
       "2020-01-01 20:00:00         -0.003968               -0.000651   \n",
       "2020-01-02 00:00:00         -0.009943               -0.003968   \n",
       "2020-01-02 04:00:00         -0.002674               -0.009943   \n",
       "2020-01-02 08:00:00          0.004101               -0.002674   \n",
       "2020-01-02 12:00:00         -0.001235                0.004101   \n",
       "...                               ...                     ...   \n",
       "2025-11-05 20:00:00         -0.001183                0.002694   \n",
       "2025-11-06 00:00:00         -0.002401               -0.001183   \n",
       "2025-11-06 04:00:00         -0.004357               -0.002401   \n",
       "2025-11-06 08:00:00          0.000408               -0.004357   \n",
       "2025-11-06 12:00:00         -0.003499                0.000408   \n",
       "\n",
       "                     close_log_return_lag_2  close_log_return_lag_3  \\\n",
       "timestamp                                                             \n",
       "2020-01-01 20:00:00                0.005126               -0.001753   \n",
       "2020-01-02 00:00:00               -0.000651                0.005126   \n",
       "2020-01-02 04:00:00               -0.003968               -0.000651   \n",
       "2020-01-02 08:00:00               -0.009943               -0.003968   \n",
       "2020-01-02 12:00:00               -0.002674               -0.009943   \n",
       "...                                     ...                     ...   \n",
       "2025-11-05 20:00:00                0.016110                0.000760   \n",
       "2025-11-06 00:00:00                0.002694                0.016110   \n",
       "2025-11-06 04:00:00               -0.001183                0.002694   \n",
       "2025-11-06 08:00:00               -0.002401               -0.001183   \n",
       "2025-11-06 12:00:00               -0.004357               -0.002401   \n",
       "\n",
       "                     close_log_return_lag_4  \n",
       "timestamp                                    \n",
       "2020-01-01 20:00:00               -0.002103  \n",
       "2020-01-02 00:00:00               -0.001753  \n",
       "2020-01-02 04:00:00                0.005126  \n",
       "2020-01-02 08:00:00               -0.000651  \n",
       "2020-01-02 12:00:00               -0.003968  \n",
       "...                                     ...  \n",
       "2025-11-05 20:00:00               -0.001342  \n",
       "2025-11-06 00:00:00                0.000760  \n",
       "2025-11-06 04:00:00                0.016110  \n",
       "2025-11-06 08:00:00                0.002694  \n",
       "2025-11-06 12:00:00               -0.001183  \n",
       "\n",
       "[12814 rows x 10 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>close_log_return</th>\n",
       "      <th>close_log_return_lag_1</th>\n",
       "      <th>close_log_return_lag_2</th>\n",
       "      <th>close_log_return_lag_3</th>\n",
       "      <th>close_log_return_lag_4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01 20:00:00</th>\n",
       "      <td>7229.48</td>\n",
       "      <td>7242.98</td>\n",
       "      <td>7175.15</td>\n",
       "      <td>7200.85</td>\n",
       "      <td>2257.568823</td>\n",
       "      <td>-0.003968</td>\n",
       "      <td>-0.000651</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>-0.001753</td>\n",
       "      <td>-0.002103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 00:00:00</th>\n",
       "      <td>7200.77</td>\n",
       "      <td>7212.50</td>\n",
       "      <td>7120.37</td>\n",
       "      <td>7129.61</td>\n",
       "      <td>3739.354832</td>\n",
       "      <td>-0.009943</td>\n",
       "      <td>-0.003968</td>\n",
       "      <td>-0.000651</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>-0.001753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 04:00:00</th>\n",
       "      <td>7129.25</td>\n",
       "      <td>7161.00</td>\n",
       "      <td>7105.00</td>\n",
       "      <td>7110.57</td>\n",
       "      <td>4057.961355</td>\n",
       "      <td>-0.002674</td>\n",
       "      <td>-0.009943</td>\n",
       "      <td>-0.003968</td>\n",
       "      <td>-0.000651</td>\n",
       "      <td>0.005126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 08:00:00</th>\n",
       "      <td>7110.98</td>\n",
       "      <td>7180.00</td>\n",
       "      <td>7109.11</td>\n",
       "      <td>7139.79</td>\n",
       "      <td>4162.203010</td>\n",
       "      <td>0.004101</td>\n",
       "      <td>-0.002674</td>\n",
       "      <td>-0.009943</td>\n",
       "      <td>-0.003968</td>\n",
       "      <td>-0.000651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 12:00:00</th>\n",
       "      <td>7139.73</td>\n",
       "      <td>7163.40</td>\n",
       "      <td>7107.43</td>\n",
       "      <td>7130.98</td>\n",
       "      <td>4179.041833</td>\n",
       "      <td>-0.001235</td>\n",
       "      <td>0.004101</td>\n",
       "      <td>-0.002674</td>\n",
       "      <td>-0.009943</td>\n",
       "      <td>-0.003968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-05 20:00:00</th>\n",
       "      <td>104008.09</td>\n",
       "      <td>104534.74</td>\n",
       "      <td>103305.05</td>\n",
       "      <td>103885.16</td>\n",
       "      <td>2687.898330</td>\n",
       "      <td>-0.001183</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>0.016110</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>-0.001342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-06 00:00:00</th>\n",
       "      <td>103885.16</td>\n",
       "      <td>103933.33</td>\n",
       "      <td>102716.26</td>\n",
       "      <td>103636.03</td>\n",
       "      <td>3781.794370</td>\n",
       "      <td>-0.002401</td>\n",
       "      <td>-0.001183</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>0.016110</td>\n",
       "      <td>0.000760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-06 04:00:00</th>\n",
       "      <td>103636.92</td>\n",
       "      <td>104200.00</td>\n",
       "      <td>102910.66</td>\n",
       "      <td>103185.48</td>\n",
       "      <td>2970.865520</td>\n",
       "      <td>-0.004357</td>\n",
       "      <td>-0.002401</td>\n",
       "      <td>-0.001183</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>0.016110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-06 08:00:00</th>\n",
       "      <td>103184.75</td>\n",
       "      <td>103440.00</td>\n",
       "      <td>102651.63</td>\n",
       "      <td>103227.58</td>\n",
       "      <td>2657.253150</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>-0.004357</td>\n",
       "      <td>-0.002401</td>\n",
       "      <td>-0.001183</td>\n",
       "      <td>0.002694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-06 12:00:00</th>\n",
       "      <td>103227.59</td>\n",
       "      <td>103333.18</td>\n",
       "      <td>102830.83</td>\n",
       "      <td>102867.05</td>\n",
       "      <td>290.340880</td>\n",
       "      <td>-0.003499</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>-0.004357</td>\n",
       "      <td>-0.002401</td>\n",
       "      <td>-0.001183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12814 rows Ã— 10 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 529
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:57.861598Z",
     "start_time": "2025-11-06T12:22:57.758351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Plot distribution\n",
    "\n",
    "ts['close_log_return'].hist(bins=50, figsize=(10,5))\n",
    "plt.title('Close log return')\n",
    "plt.xlabel('Close log return')\n",
    "plt.ylabel('Number of trades')\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "73d8798594509bd4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 530
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:58.146960Z",
     "start_time": "2025-11-06T12:22:57.962680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(ts['close_log_return'], bins=50, kde=True)\n",
    "plt.title(\" Distribution of Log Returns\")\n"
   ],
   "id": "a6342a31e6abe1dd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, ' Distribution of Log Returns')"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 531
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Build Model",
   "id": "e111c59fa79b3cb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:58.196415Z",
     "start_time": "2025-11-06T12:22:58.189882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we will use a linear model from torch.\n",
    "# reason for linear model is the simplicity and interpretation\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_features):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ],
   "id": "c04f988e3febc490",
   "outputs": [],
   "execution_count": 532
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:58.217965Z",
     "start_time": "2025-11-06T12:22:58.214452Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a2594d32a6dac057",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Complexity of the model",
   "id": "3e07908eb67c840d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:58.232834Z",
     "start_time": "2025-11-06T12:22:58.227559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# y = w * x + b = linear model\n",
    "input_features = 1\n",
    "linear_model = LinearModel(input_features)\n",
    "\n",
    "from torchinfo import summary\n",
    "#summary(linear_model,input_size=(1, input_features))\n",
    "\n",
    "print(linear_model)\n",
    "print(summary(linear_model,input_size=(1, input_features)))\n"
   ],
   "id": "f01027ae924e012c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearModel(\n",
      "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "LinearModel                              [1, 1]                    --\n",
      "â”œâ”€Linear: 1-1                            [1, 1]                    2\n",
      "==========================================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0.00\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "execution_count": 533
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:58.280301Z",
     "start_time": "2025-11-06T12:22:58.277029Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ae33506a584f5a5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Split by time\n",
    "* Creating an AR1 model\n",
    "* We are aiming to predict one return by its own lag\n",
    "* splitting your data by scratch ensures no data leackage"
   ],
   "id": "8288476577ed07a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:58.296741Z",
     "start_time": "2025-11-06T12:22:58.293084Z"
    }
   },
   "cell_type": "code",
   "source": [
    " features = ['close_log_return_lag_1']\n",
    " target = 'close_log_return'\n",
    " test_size = 0.25 #"
   ],
   "id": "9e93e248bd02bb8",
   "outputs": [],
   "execution_count": 534
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:58.313658Z",
     "start_time": "2025-11-06T12:22:58.309522Z"
    }
   },
   "cell_type": "code",
   "source": "len(ts)",
   "id": "52dae32ebe5bc293",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12814"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 535
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:58.352325Z",
     "start_time": "2025-11-06T12:22:58.348222Z"
    }
   },
   "cell_type": "code",
   "source": "len(ts)* test_size",
   "id": "5f2fe3fc83daad85",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3203.5"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 536
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:58.377958Z",
     "start_time": "2025-11-06T12:22:58.371824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# to split the data by time, we will split it by the index.\n",
    "# this will give us the train size below\n",
    "split_idx = int(len(ts) *(1-test_size))\n",
    "split_idx"
   ],
   "id": "27f4d59037d52114",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9610"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 537
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:58.417760Z",
     "start_time": "2025-11-06T12:22:58.409608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# split time series into 2 parts\n",
    "\n",
    "ts_train,ts_test = ts[:split_idx], ts[split_idx:]\n",
    "\n",
    "ts_train.head()"
   ],
   "id": "ce95d1ea4aae1020",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                        open     high      low    close       volume  \\\n",
       "timestamp                                                              \n",
       "2020-01-01 20:00:00  7229.48  7242.98  7175.15  7200.85  2257.568823   \n",
       "2020-01-02 00:00:00  7200.77  7212.50  7120.37  7129.61  3739.354832   \n",
       "2020-01-02 04:00:00  7129.25  7161.00  7105.00  7110.57  4057.961355   \n",
       "2020-01-02 08:00:00  7110.98  7180.00  7109.11  7139.79  4162.203010   \n",
       "2020-01-02 12:00:00  7139.73  7163.40  7107.43  7130.98  4179.041833   \n",
       "\n",
       "                     close_log_return  close_log_return_lag_1  \\\n",
       "timestamp                                                       \n",
       "2020-01-01 20:00:00         -0.003968               -0.000651   \n",
       "2020-01-02 00:00:00         -0.009943               -0.003968   \n",
       "2020-01-02 04:00:00         -0.002674               -0.009943   \n",
       "2020-01-02 08:00:00          0.004101               -0.002674   \n",
       "2020-01-02 12:00:00         -0.001235                0.004101   \n",
       "\n",
       "                     close_log_return_lag_2  close_log_return_lag_3  \\\n",
       "timestamp                                                             \n",
       "2020-01-01 20:00:00                0.005126               -0.001753   \n",
       "2020-01-02 00:00:00               -0.000651                0.005126   \n",
       "2020-01-02 04:00:00               -0.003968               -0.000651   \n",
       "2020-01-02 08:00:00               -0.009943               -0.003968   \n",
       "2020-01-02 12:00:00               -0.002674               -0.009943   \n",
       "\n",
       "                     close_log_return_lag_4  \n",
       "timestamp                                    \n",
       "2020-01-01 20:00:00               -0.002103  \n",
       "2020-01-02 00:00:00               -0.001753  \n",
       "2020-01-02 04:00:00                0.005126  \n",
       "2020-01-02 08:00:00               -0.000651  \n",
       "2020-01-02 12:00:00               -0.003968  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>close_log_return</th>\n",
       "      <th>close_log_return_lag_1</th>\n",
       "      <th>close_log_return_lag_2</th>\n",
       "      <th>close_log_return_lag_3</th>\n",
       "      <th>close_log_return_lag_4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01 20:00:00</th>\n",
       "      <td>7229.48</td>\n",
       "      <td>7242.98</td>\n",
       "      <td>7175.15</td>\n",
       "      <td>7200.85</td>\n",
       "      <td>2257.568823</td>\n",
       "      <td>-0.003968</td>\n",
       "      <td>-0.000651</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>-0.001753</td>\n",
       "      <td>-0.002103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 00:00:00</th>\n",
       "      <td>7200.77</td>\n",
       "      <td>7212.50</td>\n",
       "      <td>7120.37</td>\n",
       "      <td>7129.61</td>\n",
       "      <td>3739.354832</td>\n",
       "      <td>-0.009943</td>\n",
       "      <td>-0.003968</td>\n",
       "      <td>-0.000651</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>-0.001753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 04:00:00</th>\n",
       "      <td>7129.25</td>\n",
       "      <td>7161.00</td>\n",
       "      <td>7105.00</td>\n",
       "      <td>7110.57</td>\n",
       "      <td>4057.961355</td>\n",
       "      <td>-0.002674</td>\n",
       "      <td>-0.009943</td>\n",
       "      <td>-0.003968</td>\n",
       "      <td>-0.000651</td>\n",
       "      <td>0.005126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 08:00:00</th>\n",
       "      <td>7110.98</td>\n",
       "      <td>7180.00</td>\n",
       "      <td>7109.11</td>\n",
       "      <td>7139.79</td>\n",
       "      <td>4162.203010</td>\n",
       "      <td>0.004101</td>\n",
       "      <td>-0.002674</td>\n",
       "      <td>-0.009943</td>\n",
       "      <td>-0.003968</td>\n",
       "      <td>-0.000651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 12:00:00</th>\n",
       "      <td>7139.73</td>\n",
       "      <td>7163.40</td>\n",
       "      <td>7107.43</td>\n",
       "      <td>7130.98</td>\n",
       "      <td>4179.041833</td>\n",
       "      <td>-0.001235</td>\n",
       "      <td>0.004101</td>\n",
       "      <td>-0.002674</td>\n",
       "      <td>-0.009943</td>\n",
       "      <td>-0.003968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 538
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:58.487329Z",
     "start_time": "2025-11-06T12:22:58.478367Z"
    }
   },
   "cell_type": "code",
   "source": "ts_test.tail()",
   "id": "e296efd9cde33cbf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                          open       high        low      close      volume  \\\n",
       "timestamp                                                                     \n",
       "2025-11-05 20:00:00  104008.09  104534.74  103305.05  103885.16  2687.89833   \n",
       "2025-11-06 00:00:00  103885.16  103933.33  102716.26  103636.03  3781.79437   \n",
       "2025-11-06 04:00:00  103636.92  104200.00  102910.66  103185.48  2970.86552   \n",
       "2025-11-06 08:00:00  103184.75  103440.00  102651.63  103227.58  2657.25315   \n",
       "2025-11-06 12:00:00  103227.59  103333.18  102830.83  102867.05   290.34088   \n",
       "\n",
       "                     close_log_return  close_log_return_lag_1  \\\n",
       "timestamp                                                       \n",
       "2025-11-05 20:00:00         -0.001183                0.002694   \n",
       "2025-11-06 00:00:00         -0.002401               -0.001183   \n",
       "2025-11-06 04:00:00         -0.004357               -0.002401   \n",
       "2025-11-06 08:00:00          0.000408               -0.004357   \n",
       "2025-11-06 12:00:00         -0.003499                0.000408   \n",
       "\n",
       "                     close_log_return_lag_2  close_log_return_lag_3  \\\n",
       "timestamp                                                             \n",
       "2025-11-05 20:00:00                0.016110                0.000760   \n",
       "2025-11-06 00:00:00                0.002694                0.016110   \n",
       "2025-11-06 04:00:00               -0.001183                0.002694   \n",
       "2025-11-06 08:00:00               -0.002401               -0.001183   \n",
       "2025-11-06 12:00:00               -0.004357               -0.002401   \n",
       "\n",
       "                     close_log_return_lag_4  \n",
       "timestamp                                    \n",
       "2025-11-05 20:00:00               -0.001342  \n",
       "2025-11-06 00:00:00                0.000760  \n",
       "2025-11-06 04:00:00                0.016110  \n",
       "2025-11-06 08:00:00                0.002694  \n",
       "2025-11-06 12:00:00               -0.001183  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>close_log_return</th>\n",
       "      <th>close_log_return_lag_1</th>\n",
       "      <th>close_log_return_lag_2</th>\n",
       "      <th>close_log_return_lag_3</th>\n",
       "      <th>close_log_return_lag_4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-11-05 20:00:00</th>\n",
       "      <td>104008.09</td>\n",
       "      <td>104534.74</td>\n",
       "      <td>103305.05</td>\n",
       "      <td>103885.16</td>\n",
       "      <td>2687.89833</td>\n",
       "      <td>-0.001183</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>0.016110</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>-0.001342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-06 00:00:00</th>\n",
       "      <td>103885.16</td>\n",
       "      <td>103933.33</td>\n",
       "      <td>102716.26</td>\n",
       "      <td>103636.03</td>\n",
       "      <td>3781.79437</td>\n",
       "      <td>-0.002401</td>\n",
       "      <td>-0.001183</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>0.016110</td>\n",
       "      <td>0.000760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-06 04:00:00</th>\n",
       "      <td>103636.92</td>\n",
       "      <td>104200.00</td>\n",
       "      <td>102910.66</td>\n",
       "      <td>103185.48</td>\n",
       "      <td>2970.86552</td>\n",
       "      <td>-0.004357</td>\n",
       "      <td>-0.002401</td>\n",
       "      <td>-0.001183</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>0.016110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-06 08:00:00</th>\n",
       "      <td>103184.75</td>\n",
       "      <td>103440.00</td>\n",
       "      <td>102651.63</td>\n",
       "      <td>103227.58</td>\n",
       "      <td>2657.25315</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>-0.004357</td>\n",
       "      <td>-0.002401</td>\n",
       "      <td>-0.001183</td>\n",
       "      <td>0.002694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-06 12:00:00</th>\n",
       "      <td>103227.59</td>\n",
       "      <td>103333.18</td>\n",
       "      <td>102830.83</td>\n",
       "      <td>102867.05</td>\n",
       "      <td>290.34088</td>\n",
       "      <td>-0.003499</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>-0.004357</td>\n",
       "      <td>-0.002401</td>\n",
       "      <td>-0.001183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 539
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:58.706516Z",
     "start_time": "2025-11-06T12:22:58.699828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# converting into torch tensors\n",
    "# splitting our input and output into separate variables\n",
    "X_train = torch.tensor(ts_train[features].to_numpy(), dtype=torch.float32)\n",
    "X_test = torch.tensor(ts_test[features].to_numpy(), dtype=torch.float32)\n",
    "Y_train = torch.tensor(ts_train[target].to_numpy(), dtype=torch.float32)\n",
    "Y_test = torch.tensor(ts_test[target].to_numpy(), dtype=torch.float32)\n",
    "\n"
   ],
   "id": "9cfe71ea85b7e02a",
   "outputs": [],
   "execution_count": 540
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:58.856858Z",
     "start_time": "2025-11-06T12:22:58.851341Z"
    }
   },
   "cell_type": "code",
   "source": "X_train.shape # row vector\n",
   "id": "8276b3c4d57bce57",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9610, 1])"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 541
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:59.035869Z",
     "start_time": "2025-11-06T12:22:59.031846Z"
    }
   },
   "cell_type": "code",
   "source": "Y_train.shape # one dimensional tensor",
   "id": "1290c3a4fc89890c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9610])"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 542
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:59.091052Z",
     "start_time": "2025-11-06T12:22:59.085608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We need to put it into a 2 dimensional\n",
    "\n",
    "Y_train = Y_train.reshape(-1, 1)\n",
    "Y_train.shape"
   ],
   "id": "299c41acca5ef0b8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9610, 1])"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 543
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:59.135995Z",
     "start_time": "2025-11-06T12:22:59.131881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Y_test = Y_test.reshape(-1, 1)\n",
    "Y_test.shape"
   ],
   "id": "ef487b59e8c33154",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3204, 1])"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 544
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:22:59.177345Z",
     "start_time": "2025-11-06T12:22:59.169726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fit scaler on TRAIN only\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x_scaler = StandardScaler().fit(ts_train[features].values)\n",
    "X_train_np = x_scaler.transform(ts_train[features].values)\n",
    "X_test_np  = x_scaler.transform(ts_test[features].values)\n",
    "\n",
    "# Replace your old torch tensor creation with:\n",
    "X_train = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "X_test  = torch.tensor(X_test_np,  dtype=torch.float32)\n"
   ],
   "id": "ccbbacbcd845f55c",
   "outputs": [],
   "execution_count": 545
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Batch Gradient Descent\n",
    "* this trains all the data at once."
   ],
   "id": "e4f3de9490cae570"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:23:04.160388Z",
     "start_time": "2025-11-06T12:22:59.192895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# specify hyperparameters which can be tweaked to improve model performance\n",
    "\n",
    "no_epochs = 1000 * 5\n",
    "lr = 0.0005\n",
    "\n",
    "# Create Model\n",
    "model = LinearModel(len(features))\n",
    "\n",
    "#Loss Function L1Loss/MSE\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "print('\\nTraining...')\n",
    "for epoch in range(no_epochs):\n",
    "    # forward pass\n",
    "    y_hat = model(X_train)\n",
    "    loss = criterion(y_hat, Y_train)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad() # 1. clear old gradients\n",
    "    loss.backward()       # 2. compute new gradients\n",
    "    optimizer.step()      # 3. update weights\n",
    "\n",
    "    # check for improvements by logging\n",
    "    train_loss = loss.item()\n",
    "\n",
    "    # logging\n",
    "    if (epoch+1) % 500 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{no_epochs}], Loss: {train_loss:.6f}')\n",
    "\n",
    "    print('\\nLearned parameters:')\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}:\\n {param.data.numpy()}')\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_hat = model(X_test)\n",
    "        test_loss = criterion(y_hat, Y_test)\n",
    "        print(f'\\nTest loss: {test_loss.item():.6f}, Train loss: {train_loss:.6f}')\n",
    "\n",
    "\n"
   ],
   "id": "9207028f35516ec7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.39072654]]\n",
      "linear.bias:\n",
      " [0.82768065]\n",
      "\n",
      "Test loss: 0.836617, Train loss: 0.844896\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.39022663]]\n",
      "linear.bias:\n",
      " [0.8271806]\n",
      "\n",
      "Test loss: 0.836096, Train loss: 0.844348\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.38972673]]\n",
      "linear.bias:\n",
      " [0.8266806]\n",
      "\n",
      "Test loss: 0.835575, Train loss: 0.843800\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.38922685]]\n",
      "linear.bias:\n",
      " [0.8261806]\n",
      "\n",
      "Test loss: 0.835054, Train loss: 0.843251\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.38872704]]\n",
      "linear.bias:\n",
      " [0.82568055]\n",
      "\n",
      "Test loss: 0.834533, Train loss: 0.842703\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.38822728]]\n",
      "linear.bias:\n",
      " [0.82518053]\n",
      "\n",
      "Test loss: 0.834012, Train loss: 0.842155\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.38772762]]\n",
      "linear.bias:\n",
      " [0.8246805]\n",
      "\n",
      "Test loss: 0.833491, Train loss: 0.841607\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.38722804]]\n",
      "linear.bias:\n",
      " [0.8241805]\n",
      "\n",
      "Test loss: 0.832970, Train loss: 0.841059\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3867285]]\n",
      "linear.bias:\n",
      " [0.82368046]\n",
      "\n",
      "Test loss: 0.832450, Train loss: 0.840511\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.386229]]\n",
      "linear.bias:\n",
      " [0.82318044]\n",
      "\n",
      "Test loss: 0.831929, Train loss: 0.839963\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.38572955]]\n",
      "linear.bias:\n",
      " [0.8226804]\n",
      "\n",
      "Test loss: 0.831409, Train loss: 0.839415\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.38523018]]\n",
      "linear.bias:\n",
      " [0.8221804]\n",
      "\n",
      "Test loss: 0.830888, Train loss: 0.838867\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3847309]]\n",
      "linear.bias:\n",
      " [0.82168037]\n",
      "\n",
      "Test loss: 0.830368, Train loss: 0.838319\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.38423178]]\n",
      "linear.bias:\n",
      " [0.8211803]\n",
      "\n",
      "Test loss: 0.829848, Train loss: 0.837772\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.38373297]]\n",
      "linear.bias:\n",
      " [0.8206802]\n",
      "\n",
      "Test loss: 0.829327, Train loss: 0.837224\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.38323444]]\n",
      "linear.bias:\n",
      " [0.8201801]\n",
      "\n",
      "Test loss: 0.828807, Train loss: 0.836677\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.38273612]]\n",
      "linear.bias:\n",
      " [0.81968]\n",
      "\n",
      "Test loss: 0.828287, Train loss: 0.836129\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.38223797]]\n",
      "linear.bias:\n",
      " [0.81917983]\n",
      "\n",
      "Test loss: 0.827767, Train loss: 0.835582\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.38173994]]\n",
      "linear.bias:\n",
      " [0.8186797]\n",
      "\n",
      "Test loss: 0.827248, Train loss: 0.835034\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.38124204]]\n",
      "linear.bias:\n",
      " [0.81817955]\n",
      "\n",
      "Test loss: 0.826728, Train loss: 0.834487\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.38074422]]\n",
      "linear.bias:\n",
      " [0.8176794]\n",
      "\n",
      "Test loss: 0.826208, Train loss: 0.833940\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3802465]]\n",
      "linear.bias:\n",
      " [0.81717926]\n",
      "\n",
      "Test loss: 0.825689, Train loss: 0.833392\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.37974882]]\n",
      "linear.bias:\n",
      " [0.8166791]\n",
      "\n",
      "Test loss: 0.825169, Train loss: 0.832845\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.37925118]]\n",
      "linear.bias:\n",
      " [0.816179]\n",
      "\n",
      "Test loss: 0.824649, Train loss: 0.832298\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3787537]]\n",
      "linear.bias:\n",
      " [0.81567883]\n",
      "\n",
      "Test loss: 0.824130, Train loss: 0.831750\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3782563]]\n",
      "linear.bias:\n",
      " [0.81517863]\n",
      "\n",
      "Test loss: 0.823610, Train loss: 0.831203\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.37775898]]\n",
      "linear.bias:\n",
      " [0.81467843]\n",
      "\n",
      "Test loss: 0.823090, Train loss: 0.830656\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.37726176]]\n",
      "linear.bias:\n",
      " [0.8141782]\n",
      "\n",
      "Test loss: 0.822570, Train loss: 0.830109\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3767646]]\n",
      "linear.bias:\n",
      " [0.813678]\n",
      "\n",
      "Test loss: 0.822051, Train loss: 0.829561\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.37626746]]\n",
      "linear.bias:\n",
      " [0.8131778]\n",
      "\n",
      "Test loss: 0.821531, Train loss: 0.829014\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.37577048]]\n",
      "linear.bias:\n",
      " [0.8126776]\n",
      "\n",
      "Test loss: 0.821011, Train loss: 0.828467\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.37527362]]\n",
      "linear.bias:\n",
      " [0.8121774]\n",
      "\n",
      "Test loss: 0.820492, Train loss: 0.827920\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.37477684]]\n",
      "linear.bias:\n",
      " [0.8116772]\n",
      "\n",
      "Test loss: 0.819972, Train loss: 0.827373\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.37428027]]\n",
      "linear.bias:\n",
      " [0.81117696]\n",
      "\n",
      "Test loss: 0.819453, Train loss: 0.826826\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.37378398]]\n",
      "linear.bias:\n",
      " [0.8106767]\n",
      "\n",
      "Test loss: 0.818933, Train loss: 0.826279\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.37328804]]\n",
      "linear.bias:\n",
      " [0.81017643]\n",
      "\n",
      "Test loss: 0.818414, Train loss: 0.825732\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.37279242]]\n",
      "linear.bias:\n",
      " [0.8096761]\n",
      "\n",
      "Test loss: 0.817894, Train loss: 0.825186\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.37229717]]\n",
      "linear.bias:\n",
      " [0.8091758]\n",
      "\n",
      "Test loss: 0.817375, Train loss: 0.824639\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.37180227]]\n",
      "linear.bias:\n",
      " [0.80867547]\n",
      "\n",
      "Test loss: 0.816856, Train loss: 0.824093\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.37130776]]\n",
      "linear.bias:\n",
      " [0.8081751]\n",
      "\n",
      "Test loss: 0.816336, Train loss: 0.823546\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3708136]]\n",
      "linear.bias:\n",
      " [0.8076747]\n",
      "\n",
      "Test loss: 0.815817, Train loss: 0.823000\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.37031972]]\n",
      "linear.bias:\n",
      " [0.80717427]\n",
      "\n",
      "Test loss: 0.815297, Train loss: 0.822454\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3698261]]\n",
      "linear.bias:\n",
      " [0.8066738]\n",
      "\n",
      "Test loss: 0.814778, Train loss: 0.821908\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.36933282]]\n",
      "linear.bias:\n",
      " [0.8061734]\n",
      "\n",
      "Test loss: 0.814259, Train loss: 0.821361\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.36884007]]\n",
      "linear.bias:\n",
      " [0.8056729]\n",
      "\n",
      "Test loss: 0.813739, Train loss: 0.820815\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.36834776]]\n",
      "linear.bias:\n",
      " [0.8051724]\n",
      "\n",
      "Test loss: 0.813220, Train loss: 0.820270\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.36785585]]\n",
      "linear.bias:\n",
      " [0.8046718]\n",
      "\n",
      "Test loss: 0.812700, Train loss: 0.819724\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3673643]]\n",
      "linear.bias:\n",
      " [0.80417126]\n",
      "\n",
      "Test loss: 0.812181, Train loss: 0.819178\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.366873]]\n",
      "linear.bias:\n",
      " [0.80367064]\n",
      "\n",
      "Test loss: 0.811661, Train loss: 0.818632\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.36638194]]\n",
      "linear.bias:\n",
      " [0.80317]\n",
      "\n",
      "Test loss: 0.811142, Train loss: 0.818087\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3658911]]\n",
      "linear.bias:\n",
      " [0.8026694]\n",
      "\n",
      "Test loss: 0.810623, Train loss: 0.817541\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.36540052]]\n",
      "linear.bias:\n",
      " [0.8021688]\n",
      "\n",
      "Test loss: 0.810103, Train loss: 0.816995\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.36491045]]\n",
      "linear.bias:\n",
      " [0.8016681]\n",
      "\n",
      "Test loss: 0.809584, Train loss: 0.816450\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.36442083]]\n",
      "linear.bias:\n",
      " [0.8011674]\n",
      "\n",
      "Test loss: 0.809064, Train loss: 0.815905\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3639316]]\n",
      "linear.bias:\n",
      " [0.8006667]\n",
      "\n",
      "Test loss: 0.808545, Train loss: 0.815359\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3634428]]\n",
      "linear.bias:\n",
      " [0.80016595]\n",
      "\n",
      "Test loss: 0.808025, Train loss: 0.814814\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.36295468]]\n",
      "linear.bias:\n",
      " [0.79966515]\n",
      "\n",
      "Test loss: 0.807506, Train loss: 0.814269\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3624671]]\n",
      "linear.bias:\n",
      " [0.7991643]\n",
      "\n",
      "Test loss: 0.806986, Train loss: 0.813725\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.36198002]]\n",
      "linear.bias:\n",
      " [0.79866344]\n",
      "\n",
      "Test loss: 0.806467, Train loss: 0.813180\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.36149335]]\n",
      "linear.bias:\n",
      " [0.7981625]\n",
      "\n",
      "Test loss: 0.805947, Train loss: 0.812635\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.36100703]]\n",
      "linear.bias:\n",
      " [0.7976616]\n",
      "\n",
      "Test loss: 0.805428, Train loss: 0.812091\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.36052114]]\n",
      "linear.bias:\n",
      " [0.7971607]\n",
      "\n",
      "Test loss: 0.804908, Train loss: 0.811546\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3600356]]\n",
      "linear.bias:\n",
      " [0.7966597]\n",
      "\n",
      "Test loss: 0.804389, Train loss: 0.811001\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3595505]]\n",
      "linear.bias:\n",
      " [0.79615873]\n",
      "\n",
      "Test loss: 0.803869, Train loss: 0.810457\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3590658]]\n",
      "linear.bias:\n",
      " [0.7956577]\n",
      "\n",
      "Test loss: 0.803350, Train loss: 0.809913\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3585814]]\n",
      "linear.bias:\n",
      " [0.79515666]\n",
      "\n",
      "Test loss: 0.802830, Train loss: 0.809368\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.35809726]]\n",
      "linear.bias:\n",
      " [0.7946556]\n",
      "\n",
      "Test loss: 0.802311, Train loss: 0.808824\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.35761333]]\n",
      "linear.bias:\n",
      " [0.7941545]\n",
      "\n",
      "Test loss: 0.801791, Train loss: 0.808280\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.35712957]]\n",
      "linear.bias:\n",
      " [0.7936534]\n",
      "\n",
      "Test loss: 0.801272, Train loss: 0.807736\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.35664594]]\n",
      "linear.bias:\n",
      " [0.79315233]\n",
      "\n",
      "Test loss: 0.800752, Train loss: 0.807192\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.35616243]]\n",
      "linear.bias:\n",
      " [0.79265124]\n",
      "\n",
      "Test loss: 0.800233, Train loss: 0.806647\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.35567898]]\n",
      "linear.bias:\n",
      " [0.79215014]\n",
      "\n",
      "Test loss: 0.799713, Train loss: 0.806103\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.35519555]]\n",
      "linear.bias:\n",
      " [0.79164904]\n",
      "\n",
      "Test loss: 0.799194, Train loss: 0.805559\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.35471216]]\n",
      "linear.bias:\n",
      " [0.79114795]\n",
      "\n",
      "Test loss: 0.798675, Train loss: 0.805015\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.35422876]]\n",
      "linear.bias:\n",
      " [0.79064685]\n",
      "\n",
      "Test loss: 0.798156, Train loss: 0.804471\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.35374534]]\n",
      "linear.bias:\n",
      " [0.79014575]\n",
      "\n",
      "Test loss: 0.797637, Train loss: 0.803926\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3532619]]\n",
      "linear.bias:\n",
      " [0.78964466]\n",
      "\n",
      "Test loss: 0.797118, Train loss: 0.803382\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.35277852]]\n",
      "linear.bias:\n",
      " [0.78914356]\n",
      "\n",
      "Test loss: 0.796598, Train loss: 0.802838\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.35229537]]\n",
      "linear.bias:\n",
      " [0.78864247]\n",
      "\n",
      "Test loss: 0.796079, Train loss: 0.802294\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.35181236]]\n",
      "linear.bias:\n",
      " [0.78814137]\n",
      "\n",
      "Test loss: 0.795560, Train loss: 0.801750\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.35132948]]\n",
      "linear.bias:\n",
      " [0.7876402]\n",
      "\n",
      "Test loss: 0.795041, Train loss: 0.801206\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.35084698]]\n",
      "linear.bias:\n",
      " [0.78713906]\n",
      "\n",
      "Test loss: 0.794522, Train loss: 0.800662\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.35036495]]\n",
      "linear.bias:\n",
      " [0.7866379]\n",
      "\n",
      "Test loss: 0.794003, Train loss: 0.800119\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.34988335]]\n",
      "linear.bias:\n",
      " [0.7861367]\n",
      "\n",
      "Test loss: 0.793483, Train loss: 0.799575\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.34940284]]\n",
      "linear.bias:\n",
      " [0.7856354]\n",
      "\n",
      "Test loss: 0.792964, Train loss: 0.799032\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3489233]]\n",
      "linear.bias:\n",
      " [0.7851341]\n",
      "\n",
      "Test loss: 0.792445, Train loss: 0.798489\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.34844458]]\n",
      "linear.bias:\n",
      " [0.7846327]\n",
      "\n",
      "Test loss: 0.791926, Train loss: 0.797947\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.34796658]]\n",
      "linear.bias:\n",
      " [0.7841312]\n",
      "\n",
      "Test loss: 0.791407, Train loss: 0.797404\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3474892]]\n",
      "linear.bias:\n",
      " [0.7836298]\n",
      "\n",
      "Test loss: 0.790887, Train loss: 0.796861\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3470124]]\n",
      "linear.bias:\n",
      " [0.78312826]\n",
      "\n",
      "Test loss: 0.790368, Train loss: 0.796319\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.34653604]]\n",
      "linear.bias:\n",
      " [0.78262675]\n",
      "\n",
      "Test loss: 0.789849, Train loss: 0.795776\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3460604]]\n",
      "linear.bias:\n",
      " [0.7821252]\n",
      "\n",
      "Test loss: 0.789330, Train loss: 0.795234\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3455855]]\n",
      "linear.bias:\n",
      " [0.78162354]\n",
      "\n",
      "Test loss: 0.788811, Train loss: 0.794692\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.34511125]]\n",
      "linear.bias:\n",
      " [0.78112185]\n",
      "\n",
      "Test loss: 0.788291, Train loss: 0.794150\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.34463757]]\n",
      "linear.bias:\n",
      " [0.78062016]\n",
      "\n",
      "Test loss: 0.787772, Train loss: 0.793608\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.34416485]]\n",
      "linear.bias:\n",
      " [0.7801184]\n",
      "\n",
      "Test loss: 0.787253, Train loss: 0.793066\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3436931]]\n",
      "linear.bias:\n",
      " [0.7796166]\n",
      "\n",
      "Test loss: 0.786734, Train loss: 0.792525\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3432222]]\n",
      "linear.bias:\n",
      " [0.7791147]\n",
      "\n",
      "Test loss: 0.786214, Train loss: 0.791983\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.34275204]]\n",
      "linear.bias:\n",
      " [0.7786128]\n",
      "\n",
      "Test loss: 0.785695, Train loss: 0.791442\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3422825]]\n",
      "linear.bias:\n",
      " [0.77811086]\n",
      "\n",
      "Test loss: 0.785176, Train loss: 0.790901\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3418135]]\n",
      "linear.bias:\n",
      " [0.7776089]\n",
      "\n",
      "Test loss: 0.784656, Train loss: 0.790360\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.34134513]]\n",
      "linear.bias:\n",
      " [0.7771069]\n",
      "\n",
      "Test loss: 0.784137, Train loss: 0.789818\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.34087744]]\n",
      "linear.bias:\n",
      " [0.77660483]\n",
      "\n",
      "Test loss: 0.783618, Train loss: 0.789278\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.34041032]]\n",
      "linear.bias:\n",
      " [0.7761027]\n",
      "\n",
      "Test loss: 0.783099, Train loss: 0.788737\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3399437]]\n",
      "linear.bias:\n",
      " [0.7756006]\n",
      "\n",
      "Test loss: 0.782579, Train loss: 0.788196\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3394775]]\n",
      "linear.bias:\n",
      " [0.7750985]\n",
      "\n",
      "Test loss: 0.782060, Train loss: 0.787655\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.33901167]]\n",
      "linear.bias:\n",
      " [0.77459633]\n",
      "\n",
      "Test loss: 0.781541, Train loss: 0.787115\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3385461]]\n",
      "linear.bias:\n",
      " [0.77409416]\n",
      "\n",
      "Test loss: 0.781021, Train loss: 0.786574\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.33808076]]\n",
      "linear.bias:\n",
      " [0.773592]\n",
      "\n",
      "Test loss: 0.780502, Train loss: 0.786033\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.33761576]]\n",
      "linear.bias:\n",
      " [0.7730898]\n",
      "\n",
      "Test loss: 0.779983, Train loss: 0.785493\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.33715102]]\n",
      "linear.bias:\n",
      " [0.7725876]\n",
      "\n",
      "Test loss: 0.779464, Train loss: 0.784952\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3366865]]\n",
      "linear.bias:\n",
      " [0.77208537]\n",
      "\n",
      "Test loss: 0.778944, Train loss: 0.784412\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.33622214]]\n",
      "linear.bias:\n",
      " [0.77158314]\n",
      "\n",
      "Test loss: 0.778425, Train loss: 0.783871\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3357579]]\n",
      "linear.bias:\n",
      " [0.7710809]\n",
      "\n",
      "Test loss: 0.777906, Train loss: 0.783331\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.33529374]]\n",
      "linear.bias:\n",
      " [0.7705787]\n",
      "\n",
      "Test loss: 0.777386, Train loss: 0.782790\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.33482963]]\n",
      "linear.bias:\n",
      " [0.77007645]\n",
      "\n",
      "Test loss: 0.776867, Train loss: 0.782250\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3343657]]\n",
      "linear.bias:\n",
      " [0.7695742]\n",
      "\n",
      "Test loss: 0.776348, Train loss: 0.781709\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.33390188]]\n",
      "linear.bias:\n",
      " [0.769072]\n",
      "\n",
      "Test loss: 0.775829, Train loss: 0.781169\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.33343816]]\n",
      "linear.bias:\n",
      " [0.76856977]\n",
      "\n",
      "Test loss: 0.775309, Train loss: 0.780629\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3329745]]\n",
      "linear.bias:\n",
      " [0.76806754]\n",
      "\n",
      "Test loss: 0.774790, Train loss: 0.780089\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.33251086]]\n",
      "linear.bias:\n",
      " [0.7675653]\n",
      "\n",
      "Test loss: 0.774271, Train loss: 0.779548\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.33204722]]\n",
      "linear.bias:\n",
      " [0.7670631]\n",
      "\n",
      "Test loss: 0.773751, Train loss: 0.779008\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3315837]]\n",
      "linear.bias:\n",
      " [0.76656085]\n",
      "\n",
      "Test loss: 0.773232, Train loss: 0.778468\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.33112046]]\n",
      "linear.bias:\n",
      " [0.76605856]\n",
      "\n",
      "Test loss: 0.772713, Train loss: 0.777928\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.33065742]]\n",
      "linear.bias:\n",
      " [0.7655563]\n",
      "\n",
      "Test loss: 0.772194, Train loss: 0.777388\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.33019456]]\n",
      "linear.bias:\n",
      " [0.765054]\n",
      "\n",
      "Test loss: 0.771674, Train loss: 0.776848\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.32973182]]\n",
      "linear.bias:\n",
      " [0.7645517]\n",
      "\n",
      "Test loss: 0.771155, Train loss: 0.776308\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.32926914]]\n",
      "linear.bias:\n",
      " [0.7640494]\n",
      "\n",
      "Test loss: 0.770636, Train loss: 0.775768\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.32880652]]\n",
      "linear.bias:\n",
      " [0.7635471]\n",
      "\n",
      "Test loss: 0.770117, Train loss: 0.775228\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.32834393]]\n",
      "linear.bias:\n",
      " [0.76304483]\n",
      "\n",
      "Test loss: 0.769597, Train loss: 0.774688\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3278815]]\n",
      "linear.bias:\n",
      " [0.76254255]\n",
      "\n",
      "Test loss: 0.769078, Train loss: 0.774148\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3274193]]\n",
      "linear.bias:\n",
      " [0.76204026]\n",
      "\n",
      "Test loss: 0.768559, Train loss: 0.773608\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.32695752]]\n",
      "linear.bias:\n",
      " [0.7615379]\n",
      "\n",
      "Test loss: 0.768040, Train loss: 0.773069\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.32649606]]\n",
      "linear.bias:\n",
      " [0.76103556]\n",
      "\n",
      "Test loss: 0.767520, Train loss: 0.772529\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.32603502]]\n",
      "linear.bias:\n",
      " [0.7605332]\n",
      "\n",
      "Test loss: 0.767001, Train loss: 0.771990\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.32557434]]\n",
      "linear.bias:\n",
      " [0.7600308]\n",
      "\n",
      "Test loss: 0.766482, Train loss: 0.771450\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.32511428]]\n",
      "linear.bias:\n",
      " [0.7595284]\n",
      "\n",
      "Test loss: 0.765962, Train loss: 0.770911\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3246548]]\n",
      "linear.bias:\n",
      " [0.75902593]\n",
      "\n",
      "Test loss: 0.765443, Train loss: 0.770372\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.32419574]]\n",
      "linear.bias:\n",
      " [0.75852346]\n",
      "\n",
      "Test loss: 0.764924, Train loss: 0.769833\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.32373708]]\n",
      "linear.bias:\n",
      " [0.75802094]\n",
      "\n",
      "Test loss: 0.764405, Train loss: 0.769294\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.32327893]]\n",
      "linear.bias:\n",
      " [0.7575184]\n",
      "\n",
      "Test loss: 0.763885, Train loss: 0.768755\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.32282117]]\n",
      "linear.bias:\n",
      " [0.7570159]\n",
      "\n",
      "Test loss: 0.763366, Train loss: 0.768216\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.32236376]]\n",
      "linear.bias:\n",
      " [0.7565133]\n",
      "\n",
      "Test loss: 0.762847, Train loss: 0.767677\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.32190663]]\n",
      "linear.bias:\n",
      " [0.7560107]\n",
      "\n",
      "Test loss: 0.762327, Train loss: 0.767138\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.32144973]]\n",
      "linear.bias:\n",
      " [0.7555081]\n",
      "\n",
      "Test loss: 0.761808, Train loss: 0.766600\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.320993]]\n",
      "linear.bias:\n",
      " [0.75500554]\n",
      "\n",
      "Test loss: 0.761289, Train loss: 0.766061\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.32053658]]\n",
      "linear.bias:\n",
      " [0.7545029]\n",
      "\n",
      "Test loss: 0.760770, Train loss: 0.765522\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.32008043]]\n",
      "linear.bias:\n",
      " [0.75400025]\n",
      "\n",
      "Test loss: 0.760250, Train loss: 0.764984\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.31962466]]\n",
      "linear.bias:\n",
      " [0.7534976]\n",
      "\n",
      "Test loss: 0.759731, Train loss: 0.764445\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.31916937]]\n",
      "linear.bias:\n",
      " [0.75299495]\n",
      "\n",
      "Test loss: 0.759212, Train loss: 0.763907\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3187145]]\n",
      "linear.bias:\n",
      " [0.75249225]\n",
      "\n",
      "Test loss: 0.758693, Train loss: 0.763369\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.31826013]]\n",
      "linear.bias:\n",
      " [0.75198954]\n",
      "\n",
      "Test loss: 0.758174, Train loss: 0.762830\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3178062]]\n",
      "linear.bias:\n",
      " [0.7514868]\n",
      "\n",
      "Test loss: 0.757655, Train loss: 0.762292\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.31735283]]\n",
      "linear.bias:\n",
      " [0.750984]\n",
      "\n",
      "Test loss: 0.757136, Train loss: 0.761754\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.31689993]]\n",
      "linear.bias:\n",
      " [0.75048125]\n",
      "\n",
      "Test loss: 0.756617, Train loss: 0.761216\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3164474]]\n",
      "linear.bias:\n",
      " [0.7499784]\n",
      "\n",
      "Test loss: 0.756099, Train loss: 0.760678\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.31599537]]\n",
      "linear.bias:\n",
      " [0.7494756]\n",
      "\n",
      "Test loss: 0.755580, Train loss: 0.760141\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.31554395]]\n",
      "linear.bias:\n",
      " [0.7489727]\n",
      "\n",
      "Test loss: 0.755062, Train loss: 0.759603\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.31509304]]\n",
      "linear.bias:\n",
      " [0.7484698]\n",
      "\n",
      "Test loss: 0.754543, Train loss: 0.759066\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.31464255]]\n",
      "linear.bias:\n",
      " [0.7479669]\n",
      "\n",
      "Test loss: 0.754024, Train loss: 0.758528\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3141924]]\n",
      "linear.bias:\n",
      " [0.74746394]\n",
      "\n",
      "Test loss: 0.753506, Train loss: 0.757991\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.31374258]]\n",
      "linear.bias:\n",
      " [0.746961]\n",
      "\n",
      "Test loss: 0.752987, Train loss: 0.757453\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.31329316]]\n",
      "linear.bias:\n",
      " [0.74645805]\n",
      "\n",
      "Test loss: 0.752469, Train loss: 0.756916\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.31284428]]\n",
      "linear.bias:\n",
      " [0.74595505]\n",
      "\n",
      "Test loss: 0.751950, Train loss: 0.756379\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.31239584]]\n",
      "linear.bias:\n",
      " [0.74545205]\n",
      "\n",
      "Test loss: 0.751432, Train loss: 0.755841\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.31194776]]\n",
      "linear.bias:\n",
      " [0.74494904]\n",
      "\n",
      "Test loss: 0.750914, Train loss: 0.755304\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.31149998]]\n",
      "linear.bias:\n",
      " [0.744446]\n",
      "\n",
      "Test loss: 0.750396, Train loss: 0.754767\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.31105244]]\n",
      "linear.bias:\n",
      " [0.7439429]\n",
      "\n",
      "Test loss: 0.749877, Train loss: 0.754230\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3106053]]\n",
      "linear.bias:\n",
      " [0.74343985]\n",
      "\n",
      "Test loss: 0.749360, Train loss: 0.753693\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.31015843]]\n",
      "linear.bias:\n",
      " [0.7429368]\n",
      "\n",
      "Test loss: 0.748842, Train loss: 0.753156\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3097118]]\n",
      "linear.bias:\n",
      " [0.74243367]\n",
      "\n",
      "Test loss: 0.748324, Train loss: 0.752620\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3092654]]\n",
      "linear.bias:\n",
      " [0.74193054]\n",
      "\n",
      "Test loss: 0.747806, Train loss: 0.752083\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.30881932]]\n",
      "linear.bias:\n",
      " [0.7414274]\n",
      "\n",
      "Test loss: 0.747288, Train loss: 0.751546\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3083735]]\n",
      "linear.bias:\n",
      " [0.7409243]\n",
      "\n",
      "Test loss: 0.746771, Train loss: 0.751009\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3079279]]\n",
      "linear.bias:\n",
      " [0.7404212]\n",
      "\n",
      "Test loss: 0.746253, Train loss: 0.750472\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.30748248]]\n",
      "linear.bias:\n",
      " [0.73991805]\n",
      "\n",
      "Test loss: 0.745735, Train loss: 0.749936\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.30703717]]\n",
      "linear.bias:\n",
      " [0.73941493]\n",
      "\n",
      "Test loss: 0.745217, Train loss: 0.749399\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.30659196]]\n",
      "linear.bias:\n",
      " [0.7389118]\n",
      "\n",
      "Test loss: 0.744699, Train loss: 0.748862\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3061468]]\n",
      "linear.bias:\n",
      " [0.7384087]\n",
      "\n",
      "Test loss: 0.744182, Train loss: 0.748326\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.30570203]]\n",
      "linear.bias:\n",
      " [0.7379055]\n",
      "\n",
      "Test loss: 0.743664, Train loss: 0.747789\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.30525756]]\n",
      "linear.bias:\n",
      " [0.7374023]\n",
      "\n",
      "Test loss: 0.743146, Train loss: 0.747253\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.30481336]]\n",
      "linear.bias:\n",
      " [0.73689914]\n",
      "\n",
      "Test loss: 0.742628, Train loss: 0.746717\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.30436936]]\n",
      "linear.bias:\n",
      " [0.73639596]\n",
      "\n",
      "Test loss: 0.742110, Train loss: 0.746180\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.30392572]]\n",
      "linear.bias:\n",
      " [0.7358927]\n",
      "\n",
      "Test loss: 0.741593, Train loss: 0.745644\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.30348235]]\n",
      "linear.bias:\n",
      " [0.7353895]\n",
      "\n",
      "Test loss: 0.741075, Train loss: 0.745108\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.30303922]]\n",
      "linear.bias:\n",
      " [0.7348862]\n",
      "\n",
      "Test loss: 0.740557, Train loss: 0.744572\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.30259645]]\n",
      "linear.bias:\n",
      " [0.734383]\n",
      "\n",
      "Test loss: 0.740039, Train loss: 0.744036\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.30215397]]\n",
      "linear.bias:\n",
      " [0.73387975]\n",
      "\n",
      "Test loss: 0.739522, Train loss: 0.743500\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.30171195]]\n",
      "linear.bias:\n",
      " [0.73337644]\n",
      "\n",
      "Test loss: 0.739004, Train loss: 0.742964\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.30127028]]\n",
      "linear.bias:\n",
      " [0.73287314]\n",
      "\n",
      "Test loss: 0.738487, Train loss: 0.742428\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.3008289]]\n",
      "linear.bias:\n",
      " [0.73236984]\n",
      "\n",
      "Test loss: 0.737970, Train loss: 0.741892\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.30038777]]\n",
      "linear.bias:\n",
      " [0.73186654]\n",
      "\n",
      "Test loss: 0.737452, Train loss: 0.741356\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.29994684]]\n",
      "linear.bias:\n",
      " [0.73136324]\n",
      "\n",
      "Test loss: 0.736935, Train loss: 0.740821\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.29950604]]\n",
      "linear.bias:\n",
      " [0.73085994]\n",
      "\n",
      "Test loss: 0.736417, Train loss: 0.740285\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.29906535]]\n",
      "linear.bias:\n",
      " [0.7303566]\n",
      "\n",
      "Test loss: 0.735900, Train loss: 0.739749\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.29862472]]\n",
      "linear.bias:\n",
      " [0.7298532]\n",
      "\n",
      "Test loss: 0.735382, Train loss: 0.739214\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.29818413]]\n",
      "linear.bias:\n",
      " [0.7293499]\n",
      "\n",
      "Test loss: 0.734865, Train loss: 0.738678\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.29774353]]\n",
      "linear.bias:\n",
      " [0.7288466]\n",
      "\n",
      "Test loss: 0.734347, Train loss: 0.738142\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.29730293]]\n",
      "linear.bias:\n",
      " [0.7283433]\n",
      "\n",
      "Test loss: 0.733830, Train loss: 0.737607\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.29686227]]\n",
      "linear.bias:\n",
      " [0.72784]\n",
      "\n",
      "Test loss: 0.733313, Train loss: 0.737071\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.29642156]]\n",
      "linear.bias:\n",
      " [0.7273367]\n",
      "\n",
      "Test loss: 0.732795, Train loss: 0.736535\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.29598075]]\n",
      "linear.bias:\n",
      " [0.7268334]\n",
      "\n",
      "Test loss: 0.732278, Train loss: 0.735999\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.29553983]]\n",
      "linear.bias:\n",
      " [0.7263301]\n",
      "\n",
      "Test loss: 0.731760, Train loss: 0.735464\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.295099]]\n",
      "linear.bias:\n",
      " [0.7258268]\n",
      "\n",
      "Test loss: 0.731243, Train loss: 0.734928\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2946582]]\n",
      "linear.bias:\n",
      " [0.7253235]\n",
      "\n",
      "Test loss: 0.730726, Train loss: 0.734393\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.29421744]]\n",
      "linear.bias:\n",
      " [0.7248202]\n",
      "\n",
      "Test loss: 0.730208, Train loss: 0.733857\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.29377666]]\n",
      "linear.bias:\n",
      " [0.7243169]\n",
      "\n",
      "Test loss: 0.729691, Train loss: 0.733321\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.29333586]]\n",
      "linear.bias:\n",
      " [0.7238136]\n",
      "\n",
      "Test loss: 0.729173, Train loss: 0.732786\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.292895]]\n",
      "linear.bias:\n",
      " [0.72331035]\n",
      "\n",
      "Test loss: 0.728656, Train loss: 0.732250\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.29245403]]\n",
      "linear.bias:\n",
      " [0.7228071]\n",
      "\n",
      "Test loss: 0.728139, Train loss: 0.731715\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.292013]]\n",
      "linear.bias:\n",
      " [0.72230387]\n",
      "\n",
      "Test loss: 0.727621, Train loss: 0.731179\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.29157203]]\n",
      "linear.bias:\n",
      " [0.7218006]\n",
      "\n",
      "Test loss: 0.727104, Train loss: 0.730644\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.29113114]]\n",
      "linear.bias:\n",
      " [0.7212974]\n",
      "\n",
      "Test loss: 0.726587, Train loss: 0.730108\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.29069027]]\n",
      "linear.bias:\n",
      " [0.72079414]\n",
      "\n",
      "Test loss: 0.726069, Train loss: 0.729573\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.29024938]]\n",
      "linear.bias:\n",
      " [0.7202909]\n",
      "\n",
      "Test loss: 0.725552, Train loss: 0.729038\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.28980866]]\n",
      "linear.bias:\n",
      " [0.71978766]\n",
      "\n",
      "Test loss: 0.725035, Train loss: 0.728502\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.28936845]]\n",
      "linear.bias:\n",
      " [0.71928436]\n",
      "\n",
      "Test loss: 0.724517, Train loss: 0.727967\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2889287]]\n",
      "linear.bias:\n",
      " [0.71878105]\n",
      "\n",
      "Test loss: 0.724000, Train loss: 0.727432\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2884895]]\n",
      "linear.bias:\n",
      " [0.71827775]\n",
      "\n",
      "Test loss: 0.723482, Train loss: 0.726897\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2880508]]\n",
      "linear.bias:\n",
      " [0.7177744]\n",
      "\n",
      "Test loss: 0.722965, Train loss: 0.726362\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.28761253]]\n",
      "linear.bias:\n",
      " [0.71727103]\n",
      "\n",
      "Test loss: 0.722448, Train loss: 0.725828\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.287175]]\n",
      "linear.bias:\n",
      " [0.7167676]\n",
      "\n",
      "Test loss: 0.721930, Train loss: 0.725293\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.28673849]]\n",
      "linear.bias:\n",
      " [0.7162642]\n",
      "\n",
      "Test loss: 0.721413, Train loss: 0.724759\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.28630307]]\n",
      "linear.bias:\n",
      " [0.7157607]\n",
      "\n",
      "Test loss: 0.720896, Train loss: 0.724225\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.28586861]]\n",
      "linear.bias:\n",
      " [0.71525717]\n",
      "\n",
      "Test loss: 0.720378, Train loss: 0.723691\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.285435]]\n",
      "linear.bias:\n",
      " [0.71475357]\n",
      "\n",
      "Test loss: 0.719861, Train loss: 0.723157\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2850021]]\n",
      "linear.bias:\n",
      " [0.71424997]\n",
      "\n",
      "Test loss: 0.719343, Train loss: 0.722623\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.28457007]]\n",
      "linear.bias:\n",
      " [0.7137463]\n",
      "\n",
      "Test loss: 0.718826, Train loss: 0.722089\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.28413892]]\n",
      "linear.bias:\n",
      " [0.7132426]\n",
      "\n",
      "Test loss: 0.718309, Train loss: 0.721556\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.28370857]]\n",
      "linear.bias:\n",
      " [0.7127389]\n",
      "\n",
      "Test loss: 0.717791, Train loss: 0.721022\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2832789]]\n",
      "linear.bias:\n",
      " [0.7122351]\n",
      "\n",
      "Test loss: 0.717274, Train loss: 0.720489\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.28284985]]\n",
      "linear.bias:\n",
      " [0.7117313]\n",
      "\n",
      "Test loss: 0.716757, Train loss: 0.719955\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.28242147]]\n",
      "linear.bias:\n",
      " [0.7112275]\n",
      "\n",
      "Test loss: 0.716239, Train loss: 0.719422\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.28199372]]\n",
      "linear.bias:\n",
      " [0.71072364]\n",
      "\n",
      "Test loss: 0.715722, Train loss: 0.718889\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.28156647]]\n",
      "linear.bias:\n",
      " [0.71021974]\n",
      "\n",
      "Test loss: 0.715205, Train loss: 0.718355\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.28113967]]\n",
      "linear.bias:\n",
      " [0.70971584]\n",
      "\n",
      "Test loss: 0.714688, Train loss: 0.717822\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.28071323]]\n",
      "linear.bias:\n",
      " [0.70921195]\n",
      "\n",
      "Test loss: 0.714171, Train loss: 0.717289\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.28028712]]\n",
      "linear.bias:\n",
      " [0.70870805]\n",
      "\n",
      "Test loss: 0.713654, Train loss: 0.716756\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.27986145]]\n",
      "linear.bias:\n",
      " [0.7082041]\n",
      "\n",
      "Test loss: 0.713137, Train loss: 0.716223\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.27943617]]\n",
      "linear.bias:\n",
      " [0.70770013]\n",
      "\n",
      "Test loss: 0.712620, Train loss: 0.715690\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2790112]]\n",
      "linear.bias:\n",
      " [0.7071962]\n",
      "\n",
      "Test loss: 0.712103, Train loss: 0.715157\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2785867]]\n",
      "linear.bias:\n",
      " [0.7066922]\n",
      "\n",
      "Test loss: 0.711586, Train loss: 0.714625\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.27816278]]\n",
      "linear.bias:\n",
      " [0.7061882]\n",
      "\n",
      "Test loss: 0.711069, Train loss: 0.714092\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.27773958]]\n",
      "linear.bias:\n",
      " [0.7056842]\n",
      "\n",
      "Test loss: 0.710552, Train loss: 0.713559\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.27731702]]\n",
      "linear.bias:\n",
      " [0.7051801]\n",
      "\n",
      "Test loss: 0.710035, Train loss: 0.713027\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.276895]]\n",
      "linear.bias:\n",
      " [0.70467603]\n",
      "\n",
      "Test loss: 0.709518, Train loss: 0.712495\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2764734]]\n",
      "linear.bias:\n",
      " [0.7041719]\n",
      "\n",
      "Test loss: 0.709001, Train loss: 0.711962\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.27605218]]\n",
      "linear.bias:\n",
      " [0.70366776]\n",
      "\n",
      "Test loss: 0.708484, Train loss: 0.711430\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.27563128]]\n",
      "linear.bias:\n",
      " [0.7031636]\n",
      "\n",
      "Test loss: 0.707967, Train loss: 0.710898\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.27521083]]\n",
      "linear.bias:\n",
      " [0.7026595]\n",
      "\n",
      "Test loss: 0.707450, Train loss: 0.710365\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.27479076]]\n",
      "linear.bias:\n",
      " [0.70215535]\n",
      "\n",
      "Test loss: 0.706933, Train loss: 0.709833\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.27437124]]\n",
      "linear.bias:\n",
      " [0.70165116]\n",
      "\n",
      "Test loss: 0.706416, Train loss: 0.709301\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2739522]]\n",
      "linear.bias:\n",
      " [0.70114696]\n",
      "\n",
      "Test loss: 0.705899, Train loss: 0.708769\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.27353352]]\n",
      "linear.bias:\n",
      " [0.70064276]\n",
      "\n",
      "Test loss: 0.705383, Train loss: 0.708238\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2731152]]\n",
      "linear.bias:\n",
      " [0.7001385]\n",
      "\n",
      "Test loss: 0.704866, Train loss: 0.707706\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2726971]]\n",
      "linear.bias:\n",
      " [0.69963425]\n",
      "\n",
      "Test loss: 0.704349, Train loss: 0.707174\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2722792]]\n",
      "linear.bias:\n",
      " [0.69913]\n",
      "\n",
      "Test loss: 0.703833, Train loss: 0.706642\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2718617]]\n",
      "linear.bias:\n",
      " [0.69862574]\n",
      "\n",
      "Test loss: 0.703316, Train loss: 0.706110\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2714447]]\n",
      "linear.bias:\n",
      " [0.6981215]\n",
      "\n",
      "Test loss: 0.702799, Train loss: 0.705579\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.27102837]]\n",
      "linear.bias:\n",
      " [0.6976172]\n",
      "\n",
      "Test loss: 0.702283, Train loss: 0.705047\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2706126]]\n",
      "linear.bias:\n",
      " [0.69711286]\n",
      "\n",
      "Test loss: 0.701766, Train loss: 0.704516\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2701973]]\n",
      "linear.bias:\n",
      " [0.69660854]\n",
      "\n",
      "Test loss: 0.701249, Train loss: 0.703985\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.26978263]]\n",
      "linear.bias:\n",
      " [0.69610417]\n",
      "\n",
      "Test loss: 0.700733, Train loss: 0.703453\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2693685]]\n",
      "linear.bias:\n",
      " [0.6955998]\n",
      "\n",
      "Test loss: 0.700216, Train loss: 0.702922\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2689548]]\n",
      "linear.bias:\n",
      " [0.6950954]\n",
      "\n",
      "Test loss: 0.699699, Train loss: 0.702391\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2685417]]\n",
      "linear.bias:\n",
      " [0.694591]\n",
      "\n",
      "Test loss: 0.699183, Train loss: 0.701860\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2681293]]\n",
      "linear.bias:\n",
      " [0.69408655]\n",
      "\n",
      "Test loss: 0.698666, Train loss: 0.701329\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.26771772]]\n",
      "linear.bias:\n",
      " [0.69358206]\n",
      "\n",
      "Test loss: 0.698150, Train loss: 0.700799\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.26730683]]\n",
      "linear.bias:\n",
      " [0.69307756]\n",
      "\n",
      "Test loss: 0.697633, Train loss: 0.700268\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.26689702]]\n",
      "linear.bias:\n",
      " [0.692573]\n",
      "\n",
      "Test loss: 0.697116, Train loss: 0.699737\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.26648855]]\n",
      "linear.bias:\n",
      " [0.6920684]\n",
      "\n",
      "Test loss: 0.696600, Train loss: 0.699207\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.26608127]]\n",
      "linear.bias:\n",
      " [0.6915637]\n",
      "\n",
      "Test loss: 0.696083, Train loss: 0.698677\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.265675]]\n",
      "linear.bias:\n",
      " [0.691059]\n",
      "\n",
      "Test loss: 0.695566, Train loss: 0.698147\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.26526967]]\n",
      "linear.bias:\n",
      " [0.69055426]\n",
      "\n",
      "Test loss: 0.695050, Train loss: 0.697618\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2648653]]\n",
      "linear.bias:\n",
      " [0.69004947]\n",
      "\n",
      "Test loss: 0.694533, Train loss: 0.697088\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.26446205]]\n",
      "linear.bias:\n",
      " [0.6895446]\n",
      "\n",
      "Test loss: 0.694016, Train loss: 0.696558\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.26406]]\n",
      "linear.bias:\n",
      " [0.6890397]\n",
      "\n",
      "Test loss: 0.693500, Train loss: 0.696029\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.26365894]]\n",
      "linear.bias:\n",
      " [0.6885348]\n",
      "\n",
      "Test loss: 0.692983, Train loss: 0.695500\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.26325902]]\n",
      "linear.bias:\n",
      " [0.6880298]\n",
      "\n",
      "Test loss: 0.692466, Train loss: 0.694970\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2628603]]\n",
      "linear.bias:\n",
      " [0.6875248]\n",
      "\n",
      "Test loss: 0.691950, Train loss: 0.694441\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.26246265]]\n",
      "linear.bias:\n",
      " [0.6870197]\n",
      "\n",
      "Test loss: 0.691433, Train loss: 0.693912\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.26206592]]\n",
      "linear.bias:\n",
      " [0.6865146]\n",
      "\n",
      "Test loss: 0.690916, Train loss: 0.693384\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.26166996]]\n",
      "linear.bias:\n",
      " [0.68600947]\n",
      "\n",
      "Test loss: 0.690400, Train loss: 0.692855\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2612747]]\n",
      "linear.bias:\n",
      " [0.6855043]\n",
      "\n",
      "Test loss: 0.689883, Train loss: 0.692326\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.26088002]]\n",
      "linear.bias:\n",
      " [0.6849991]\n",
      "\n",
      "Test loss: 0.689366, Train loss: 0.691797\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.26048586]]\n",
      "linear.bias:\n",
      " [0.6844939]\n",
      "\n",
      "Test loss: 0.688850, Train loss: 0.691269\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2600921]]\n",
      "linear.bias:\n",
      " [0.6839887]\n",
      "\n",
      "Test loss: 0.688333, Train loss: 0.690740\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.25969872]]\n",
      "linear.bias:\n",
      " [0.6834835]\n",
      "\n",
      "Test loss: 0.687816, Train loss: 0.690212\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.25930583]]\n",
      "linear.bias:\n",
      " [0.6829783]\n",
      "\n",
      "Test loss: 0.687300, Train loss: 0.689683\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2589134]]\n",
      "linear.bias:\n",
      " [0.682473]\n",
      "\n",
      "Test loss: 0.686783, Train loss: 0.689155\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.25852132]]\n",
      "linear.bias:\n",
      " [0.68196774]\n",
      "\n",
      "Test loss: 0.686267, Train loss: 0.688626\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.25812954]]\n",
      "linear.bias:\n",
      " [0.68146247]\n",
      "\n",
      "Test loss: 0.685750, Train loss: 0.688098\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.257738]]\n",
      "linear.bias:\n",
      " [0.6809572]\n",
      "\n",
      "Test loss: 0.685234, Train loss: 0.687569\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.25734663]]\n",
      "linear.bias:\n",
      " [0.6804519]\n",
      "\n",
      "Test loss: 0.684718, Train loss: 0.687041\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.25695542]]\n",
      "linear.bias:\n",
      " [0.67994666]\n",
      "\n",
      "Test loss: 0.684202, Train loss: 0.686513\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.25656453]]\n",
      "linear.bias:\n",
      " [0.6794414]\n",
      "\n",
      "Test loss: 0.683685, Train loss: 0.685985\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.25617436]]\n",
      "linear.bias:\n",
      " [0.67893606]\n",
      "\n",
      "Test loss: 0.683169, Train loss: 0.685457\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2557848]]\n",
      "linear.bias:\n",
      " [0.67843074]\n",
      "\n",
      "Test loss: 0.682653, Train loss: 0.684929\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2553958]]\n",
      "linear.bias:\n",
      " [0.6779254]\n",
      "\n",
      "Test loss: 0.682137, Train loss: 0.684401\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2550072]]\n",
      "linear.bias:\n",
      " [0.6774201]\n",
      "\n",
      "Test loss: 0.681620, Train loss: 0.683873\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.254619]]\n",
      "linear.bias:\n",
      " [0.6769147]\n",
      "\n",
      "Test loss: 0.681104, Train loss: 0.683345\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.25423133]]\n",
      "linear.bias:\n",
      " [0.6764093]\n",
      "\n",
      "Test loss: 0.680588, Train loss: 0.682817\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2538441]]\n",
      "linear.bias:\n",
      " [0.6759039]\n",
      "\n",
      "Test loss: 0.680072, Train loss: 0.682290\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.25345728]]\n",
      "linear.bias:\n",
      " [0.6753985]\n",
      "\n",
      "Test loss: 0.679555, Train loss: 0.681762\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.25307074]]\n",
      "linear.bias:\n",
      " [0.67489314]\n",
      "\n",
      "Test loss: 0.679039, Train loss: 0.681234\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.25268447]]\n",
      "linear.bias:\n",
      " [0.67438775]\n",
      "\n",
      "Test loss: 0.678523, Train loss: 0.680707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.25229838]]\n",
      "linear.bias:\n",
      " [0.67388237]\n",
      "\n",
      "Test loss: 0.678007, Train loss: 0.680179\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.25191244]]\n",
      "linear.bias:\n",
      " [0.673377]\n",
      "\n",
      "Test loss: 0.677491, Train loss: 0.679652\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.25152662]]\n",
      "linear.bias:\n",
      " [0.6728716]\n",
      "\n",
      "Test loss: 0.676974, Train loss: 0.679124\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.25114086]]\n",
      "linear.bias:\n",
      " [0.6723662]\n",
      "\n",
      "Test loss: 0.676458, Train loss: 0.678597\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.25075513]]\n",
      "linear.bias:\n",
      " [0.6718608]\n",
      "\n",
      "Test loss: 0.675942, Train loss: 0.678069\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2503694]]\n",
      "linear.bias:\n",
      " [0.6713554]\n",
      "\n",
      "Test loss: 0.675426, Train loss: 0.677542\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24998365]]\n",
      "linear.bias:\n",
      " [0.67085004]\n",
      "\n",
      "Test loss: 0.674910, Train loss: 0.677014\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24959785]]\n",
      "linear.bias:\n",
      " [0.67034465]\n",
      "\n",
      "Test loss: 0.674394, Train loss: 0.676487\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24921197]]\n",
      "linear.bias:\n",
      " [0.66983926]\n",
      "\n",
      "Test loss: 0.673877, Train loss: 0.675959\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.248826]]\n",
      "linear.bias:\n",
      " [0.66933393]\n",
      "\n",
      "Test loss: 0.673361, Train loss: 0.675432\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24843991]]\n",
      "linear.bias:\n",
      " [0.6688286]\n",
      "\n",
      "Test loss: 0.672845, Train loss: 0.674904\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24805368]]\n",
      "linear.bias:\n",
      " [0.6683233]\n",
      "\n",
      "Test loss: 0.672329, Train loss: 0.674377\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24766731]]\n",
      "linear.bias:\n",
      " [0.66781795]\n",
      "\n",
      "Test loss: 0.671813, Train loss: 0.673850\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24728101]]\n",
      "linear.bias:\n",
      " [0.6673126]\n",
      "\n",
      "Test loss: 0.671297, Train loss: 0.673322\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24689476]]\n",
      "linear.bias:\n",
      " [0.6668073]\n",
      "\n",
      "Test loss: 0.670781, Train loss: 0.672795\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24650851]]\n",
      "linear.bias:\n",
      " [0.666302]\n",
      "\n",
      "Test loss: 0.670264, Train loss: 0.672267\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24612224]]\n",
      "linear.bias:\n",
      " [0.66579676]\n",
      "\n",
      "Test loss: 0.669748, Train loss: 0.671740\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24573593]]\n",
      "linear.bias:\n",
      " [0.6652915]\n",
      "\n",
      "Test loss: 0.669232, Train loss: 0.671213\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24534954]]\n",
      "linear.bias:\n",
      " [0.6647862]\n",
      "\n",
      "Test loss: 0.668716, Train loss: 0.670686\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24496306]]\n",
      "linear.bias:\n",
      " [0.66428095]\n",
      "\n",
      "Test loss: 0.668200, Train loss: 0.670158\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24457648]]\n",
      "linear.bias:\n",
      " [0.6637757]\n",
      "\n",
      "Test loss: 0.667684, Train loss: 0.669631\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24418978]]\n",
      "linear.bias:\n",
      " [0.6632705]\n",
      "\n",
      "Test loss: 0.667168, Train loss: 0.669104\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24380293]]\n",
      "linear.bias:\n",
      " [0.66276526]\n",
      "\n",
      "Test loss: 0.666652, Train loss: 0.668576\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24341594]]\n",
      "linear.bias:\n",
      " [0.66226006]\n",
      "\n",
      "Test loss: 0.666136, Train loss: 0.668049\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24302877]]\n",
      "linear.bias:\n",
      " [0.66175485]\n",
      "\n",
      "Test loss: 0.665620, Train loss: 0.667522\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24264169]]\n",
      "linear.bias:\n",
      " [0.66124964]\n",
      "\n",
      "Test loss: 0.665104, Train loss: 0.666994\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24225464]]\n",
      "linear.bias:\n",
      " [0.6607444]\n",
      "\n",
      "Test loss: 0.664588, Train loss: 0.666467\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2418681]]\n",
      "linear.bias:\n",
      " [0.6602392]\n",
      "\n",
      "Test loss: 0.664072, Train loss: 0.665940\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24148196]]\n",
      "linear.bias:\n",
      " [0.659734]\n",
      "\n",
      "Test loss: 0.663556, Train loss: 0.665413\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24109618]]\n",
      "linear.bias:\n",
      " [0.6592288]\n",
      "\n",
      "Test loss: 0.663040, Train loss: 0.664886\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24071069]]\n",
      "linear.bias:\n",
      " [0.6587236]\n",
      "\n",
      "Test loss: 0.662524, Train loss: 0.664359\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.24032542]]\n",
      "linear.bias:\n",
      " [0.6582184]\n",
      "\n",
      "Test loss: 0.662008, Train loss: 0.663833\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.23994033]]\n",
      "linear.bias:\n",
      " [0.6577132]\n",
      "\n",
      "Test loss: 0.661492, Train loss: 0.663306\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.23955563]]\n",
      "linear.bias:\n",
      " [0.6572079]\n",
      "\n",
      "Test loss: 0.660976, Train loss: 0.662779\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.23917124]]\n",
      "linear.bias:\n",
      " [0.65670264]\n",
      "\n",
      "Test loss: 0.660460, Train loss: 0.662252\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.23878711]]\n",
      "linear.bias:\n",
      " [0.65619737]\n",
      "\n",
      "Test loss: 0.659944, Train loss: 0.661726\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.23840342]]\n",
      "linear.bias:\n",
      " [0.6556921]\n",
      "\n",
      "Test loss: 0.659428, Train loss: 0.661199\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2380201]]\n",
      "linear.bias:\n",
      " [0.65518683]\n",
      "\n",
      "Test loss: 0.658912, Train loss: 0.660673\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2376371]]\n",
      "linear.bias:\n",
      " [0.65468156]\n",
      "\n",
      "Test loss: 0.658396, Train loss: 0.660146\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2372546]]\n",
      "linear.bias:\n",
      " [0.6541763]\n",
      "\n",
      "Test loss: 0.657880, Train loss: 0.659620\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.23687252]]\n",
      "linear.bias:\n",
      " [0.65367097]\n",
      "\n",
      "Test loss: 0.657364, Train loss: 0.659093\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2364908]]\n",
      "linear.bias:\n",
      " [0.65316564]\n",
      "\n",
      "Test loss: 0.656848, Train loss: 0.658567\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.23610938]]\n",
      "linear.bias:\n",
      " [0.6526603]\n",
      "\n",
      "Test loss: 0.656332, Train loss: 0.658041\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.23572819]]\n",
      "linear.bias:\n",
      " [0.652155]\n",
      "\n",
      "Test loss: 0.655816, Train loss: 0.657515\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2353472]]\n",
      "linear.bias:\n",
      " [0.65164965]\n",
      "\n",
      "Test loss: 0.655300, Train loss: 0.656988\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.23496634]]\n",
      "linear.bias:\n",
      " [0.6511443]\n",
      "\n",
      "Test loss: 0.654784, Train loss: 0.656462\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.23458558]]\n",
      "linear.bias:\n",
      " [0.650639]\n",
      "\n",
      "Test loss: 0.654269, Train loss: 0.655936\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2342049]]\n",
      "linear.bias:\n",
      " [0.65013367]\n",
      "\n",
      "Test loss: 0.653753, Train loss: 0.655410\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.23382425]]\n",
      "linear.bias:\n",
      " [0.64962834]\n",
      "\n",
      "Test loss: 0.653237, Train loss: 0.654884\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2334436]]\n",
      "linear.bias:\n",
      " [0.649123]\n",
      "\n",
      "Test loss: 0.652721, Train loss: 0.654357\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.23306318]]\n",
      "linear.bias:\n",
      " [0.6486177]\n",
      "\n",
      "Test loss: 0.652205, Train loss: 0.653831\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.23268293]]\n",
      "linear.bias:\n",
      " [0.64811236]\n",
      "\n",
      "Test loss: 0.651689, Train loss: 0.653305\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.23230281]]\n",
      "linear.bias:\n",
      " [0.647607]\n",
      "\n",
      "Test loss: 0.651173, Train loss: 0.652779\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.23192304]]\n",
      "linear.bias:\n",
      " [0.6471017]\n",
      "\n",
      "Test loss: 0.650657, Train loss: 0.652253\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.23154405]]\n",
      "linear.bias:\n",
      " [0.6465964]\n",
      "\n",
      "Test loss: 0.650141, Train loss: 0.651727\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.23116574]]\n",
      "linear.bias:\n",
      " [0.646091]\n",
      "\n",
      "Test loss: 0.649625, Train loss: 0.651202\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.230788]]\n",
      "linear.bias:\n",
      " [0.6455856]\n",
      "\n",
      "Test loss: 0.649110, Train loss: 0.650676\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.23041077]]\n",
      "linear.bias:\n",
      " [0.6450802]\n",
      "\n",
      "Test loss: 0.648594, Train loss: 0.650151\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.23003395]]\n",
      "linear.bias:\n",
      " [0.6445748]\n",
      "\n",
      "Test loss: 0.648078, Train loss: 0.649625\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.22965749]]\n",
      "linear.bias:\n",
      " [0.64406943]\n",
      "\n",
      "Test loss: 0.647562, Train loss: 0.649100\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.22928157]]\n",
      "linear.bias:\n",
      " [0.643564]\n",
      "\n",
      "Test loss: 0.647046, Train loss: 0.648574\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.22890638]]\n",
      "linear.bias:\n",
      " [0.64305854]\n",
      "\n",
      "Test loss: 0.646530, Train loss: 0.648049\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.22853181]]\n",
      "linear.bias:\n",
      " [0.6425531]\n",
      "\n",
      "Test loss: 0.646015, Train loss: 0.647524\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.22815776]]\n",
      "linear.bias:\n",
      " [0.6420476]\n",
      "\n",
      "Test loss: 0.645499, Train loss: 0.646999\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.22778444]]\n",
      "linear.bias:\n",
      " [0.6415421]\n",
      "\n",
      "Test loss: 0.644983, Train loss: 0.646474\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.227412]]\n",
      "linear.bias:\n",
      " [0.64103657]\n",
      "\n",
      "Test loss: 0.644467, Train loss: 0.645949\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.22704032]]\n",
      "linear.bias:\n",
      " [0.640531]\n",
      "\n",
      "Test loss: 0.643951, Train loss: 0.645424\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2266693]]\n",
      "linear.bias:\n",
      " [0.64002544]\n",
      "\n",
      "Test loss: 0.643436, Train loss: 0.644899\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2262991]]\n",
      "linear.bias:\n",
      " [0.6395198]\n",
      "\n",
      "Test loss: 0.642920, Train loss: 0.644375\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.22592965]]\n",
      "linear.bias:\n",
      " [0.6390142]\n",
      "\n",
      "Test loss: 0.642404, Train loss: 0.643850\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.22556081]]\n",
      "linear.bias:\n",
      " [0.63850856]\n",
      "\n",
      "Test loss: 0.641888, Train loss: 0.643326\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.22519277]]\n",
      "linear.bias:\n",
      " [0.6380029]\n",
      "\n",
      "Test loss: 0.641372, Train loss: 0.642801\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2248254]]\n",
      "linear.bias:\n",
      " [0.6374972]\n",
      "\n",
      "Test loss: 0.640857, Train loss: 0.642277\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.22445862]]\n",
      "linear.bias:\n",
      " [0.6369915]\n",
      "\n",
      "Test loss: 0.640341, Train loss: 0.641752\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.22409233]]\n",
      "linear.bias:\n",
      " [0.63648576]\n",
      "\n",
      "Test loss: 0.639826, Train loss: 0.641228\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.22372673]]\n",
      "linear.bias:\n",
      " [0.63598]\n",
      "\n",
      "Test loss: 0.639310, Train loss: 0.640704\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.22336173]]\n",
      "linear.bias:\n",
      " [0.63547426]\n",
      "\n",
      "Test loss: 0.638795, Train loss: 0.640180\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.22299749]]\n",
      "linear.bias:\n",
      " [0.63496846]\n",
      "\n",
      "Test loss: 0.638279, Train loss: 0.639656\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.22263391]]\n",
      "linear.bias:\n",
      " [0.63446265]\n",
      "\n",
      "Test loss: 0.637764, Train loss: 0.639132\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2222709]]\n",
      "linear.bias:\n",
      " [0.63395685]\n",
      "\n",
      "Test loss: 0.637249, Train loss: 0.638608\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.22190839]]\n",
      "linear.bias:\n",
      " [0.63345104]\n",
      "\n",
      "Test loss: 0.636733, Train loss: 0.638084\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.22154629]]\n",
      "linear.bias:\n",
      " [0.63294524]\n",
      "\n",
      "Test loss: 0.636218, Train loss: 0.637560\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.22118454]]\n",
      "linear.bias:\n",
      " [0.6324394]\n",
      "\n",
      "Test loss: 0.635702, Train loss: 0.637036\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.22082306]]\n",
      "linear.bias:\n",
      " [0.6319335]\n",
      "\n",
      "Test loss: 0.635187, Train loss: 0.636513\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.22046183]]\n",
      "linear.bias:\n",
      " [0.63142765]\n",
      "\n",
      "Test loss: 0.634672, Train loss: 0.635989\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.22010078]]\n",
      "linear.bias:\n",
      " [0.6309218]\n",
      "\n",
      "Test loss: 0.634156, Train loss: 0.635465\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21974038]]\n",
      "linear.bias:\n",
      " [0.6304159]\n",
      "\n",
      "Test loss: 0.633641, Train loss: 0.634941\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21938078]]\n",
      "linear.bias:\n",
      " [0.62991005]\n",
      "\n",
      "Test loss: 0.633126, Train loss: 0.634418\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2190219]]\n",
      "linear.bias:\n",
      " [0.6294041]\n",
      "\n",
      "Test loss: 0.632611, Train loss: 0.633895\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21866363]]\n",
      "linear.bias:\n",
      " [0.6288982]\n",
      "\n",
      "Test loss: 0.632096, Train loss: 0.633371\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21830589]]\n",
      "linear.bias:\n",
      " [0.6283923]\n",
      "\n",
      "Test loss: 0.631581, Train loss: 0.632848\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21794859]]\n",
      "linear.bias:\n",
      " [0.62788635]\n",
      "\n",
      "Test loss: 0.631066, Train loss: 0.632324\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21759166]]\n",
      "linear.bias:\n",
      " [0.6273804]\n",
      "\n",
      "Test loss: 0.630551, Train loss: 0.631801\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21723504]]\n",
      "linear.bias:\n",
      " [0.6268744]\n",
      "\n",
      "Test loss: 0.630036, Train loss: 0.631278\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2168792]]\n",
      "linear.bias:\n",
      " [0.6263684]\n",
      "\n",
      "Test loss: 0.629521, Train loss: 0.630755\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21652403]]\n",
      "linear.bias:\n",
      " [0.6258624]\n",
      "\n",
      "Test loss: 0.629006, Train loss: 0.630232\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21616945]]\n",
      "linear.bias:\n",
      " [0.6253564]\n",
      "\n",
      "Test loss: 0.628491, Train loss: 0.629709\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21581535]]\n",
      "linear.bias:\n",
      " [0.62485033]\n",
      "\n",
      "Test loss: 0.627976, Train loss: 0.629186\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21546167]]\n",
      "linear.bias:\n",
      " [0.6243443]\n",
      "\n",
      "Test loss: 0.627461, Train loss: 0.628663\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21510835]]\n",
      "linear.bias:\n",
      " [0.62383825]\n",
      "\n",
      "Test loss: 0.626946, Train loss: 0.628140\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21475533]]\n",
      "linear.bias:\n",
      " [0.6233322]\n",
      "\n",
      "Test loss: 0.626431, Train loss: 0.627617\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21440254]]\n",
      "linear.bias:\n",
      " [0.62282616]\n",
      "\n",
      "Test loss: 0.625916, Train loss: 0.627095\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21404994]]\n",
      "linear.bias:\n",
      " [0.6223201]\n",
      "\n",
      "Test loss: 0.625401, Train loss: 0.626572\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21369748]]\n",
      "linear.bias:\n",
      " [0.6218141]\n",
      "\n",
      "Test loss: 0.624887, Train loss: 0.626049\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21334513]]\n",
      "linear.bias:\n",
      " [0.621308]\n",
      "\n",
      "Test loss: 0.624372, Train loss: 0.625526\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21299283]]\n",
      "linear.bias:\n",
      " [0.620802]\n",
      "\n",
      "Test loss: 0.623857, Train loss: 0.625003\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21264058]]\n",
      "linear.bias:\n",
      " [0.62029594]\n",
      "\n",
      "Test loss: 0.623342, Train loss: 0.624481\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21228833]]\n",
      "linear.bias:\n",
      " [0.6197899]\n",
      "\n",
      "Test loss: 0.622827, Train loss: 0.623958\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21193607]]\n",
      "linear.bias:\n",
      " [0.61928385]\n",
      "\n",
      "Test loss: 0.622312, Train loss: 0.623435\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21158376]]\n",
      "linear.bias:\n",
      " [0.6187779]\n",
      "\n",
      "Test loss: 0.621797, Train loss: 0.622912\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21123138]]\n",
      "linear.bias:\n",
      " [0.6182719]\n",
      "\n",
      "Test loss: 0.621282, Train loss: 0.622390\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2108792]]\n",
      "linear.bias:\n",
      " [0.6177659]\n",
      "\n",
      "Test loss: 0.620767, Train loss: 0.621867\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21052718]]\n",
      "linear.bias:\n",
      " [0.6172599]\n",
      "\n",
      "Test loss: 0.620252, Train loss: 0.621344\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.21017526]]\n",
      "linear.bias:\n",
      " [0.61675394]\n",
      "\n",
      "Test loss: 0.619738, Train loss: 0.620822\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20982341]]\n",
      "linear.bias:\n",
      " [0.61624795]\n",
      "\n",
      "Test loss: 0.619223, Train loss: 0.620299\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20947161]]\n",
      "linear.bias:\n",
      " [0.61574197]\n",
      "\n",
      "Test loss: 0.618708, Train loss: 0.619777\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20911983]]\n",
      "linear.bias:\n",
      " [0.615236]\n",
      "\n",
      "Test loss: 0.618193, Train loss: 0.619254\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20876803]]\n",
      "linear.bias:\n",
      " [0.61473]\n",
      "\n",
      "Test loss: 0.617678, Train loss: 0.618731\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20841618]]\n",
      "linear.bias:\n",
      " [0.614224]\n",
      "\n",
      "Test loss: 0.617163, Train loss: 0.618209\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20806427]]\n",
      "linear.bias:\n",
      " [0.6137181]\n",
      "\n",
      "Test loss: 0.616648, Train loss: 0.617686\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2077123]]\n",
      "linear.bias:\n",
      " [0.61321217]\n",
      "\n",
      "Test loss: 0.616134, Train loss: 0.617164\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20736022]]\n",
      "linear.bias:\n",
      " [0.61270624]\n",
      "\n",
      "Test loss: 0.615619, Train loss: 0.616641\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2070086]]\n",
      "linear.bias:\n",
      " [0.6122003]\n",
      "\n",
      "Test loss: 0.615104, Train loss: 0.616119\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20665736]]\n",
      "linear.bias:\n",
      " [0.6116944]\n",
      "\n",
      "Test loss: 0.614589, Train loss: 0.615597\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20630646]]\n",
      "linear.bias:\n",
      " [0.6111885]\n",
      "\n",
      "Test loss: 0.614075, Train loss: 0.615074\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20595582]]\n",
      "linear.bias:\n",
      " [0.61068255]\n",
      "\n",
      "Test loss: 0.613560, Train loss: 0.614552\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20560539]]\n",
      "linear.bias:\n",
      " [0.6101766]\n",
      "\n",
      "Test loss: 0.613045, Train loss: 0.614030\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20525512]]\n",
      "linear.bias:\n",
      " [0.6096707]\n",
      "\n",
      "Test loss: 0.612530, Train loss: 0.613508\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20490499]]\n",
      "linear.bias:\n",
      " [0.6091648]\n",
      "\n",
      "Test loss: 0.612015, Train loss: 0.612986\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20455524]]\n",
      "linear.bias:\n",
      " [0.60865885]\n",
      "\n",
      "Test loss: 0.611501, Train loss: 0.612464\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20420581]]\n",
      "linear.bias:\n",
      " [0.6081529]\n",
      "\n",
      "Test loss: 0.610986, Train loss: 0.611941\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20385693]]\n",
      "linear.bias:\n",
      " [0.60764694]\n",
      "\n",
      "Test loss: 0.610471, Train loss: 0.611420\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20350853]]\n",
      "linear.bias:\n",
      " [0.60714096]\n",
      "\n",
      "Test loss: 0.609957, Train loss: 0.610898\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20316081]]\n",
      "linear.bias:\n",
      " [0.606635]\n",
      "\n",
      "Test loss: 0.609442, Train loss: 0.610376\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20281367]]\n",
      "linear.bias:\n",
      " [0.606129]\n",
      "\n",
      "Test loss: 0.608927, Train loss: 0.609854\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20246732]]\n",
      "linear.bias:\n",
      " [0.60562295]\n",
      "\n",
      "Test loss: 0.608412, Train loss: 0.609333\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20212168]]\n",
      "linear.bias:\n",
      " [0.6051169]\n",
      "\n",
      "Test loss: 0.607898, Train loss: 0.608811\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20177662]]\n",
      "linear.bias:\n",
      " [0.60461086]\n",
      "\n",
      "Test loss: 0.607383, Train loss: 0.608289\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20143236]]\n",
      "linear.bias:\n",
      " [0.6041048]\n",
      "\n",
      "Test loss: 0.606868, Train loss: 0.607768\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.2010888]]\n",
      "linear.bias:\n",
      " [0.6035987]\n",
      "\n",
      "Test loss: 0.606354, Train loss: 0.607247\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20074584]]\n",
      "linear.bias:\n",
      " [0.6030926]\n",
      "\n",
      "Test loss: 0.605839, Train loss: 0.606725\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20040338]]\n",
      "linear.bias:\n",
      " [0.6025865]\n",
      "\n",
      "Test loss: 0.605324, Train loss: 0.606204\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.20006137]]\n",
      "linear.bias:\n",
      " [0.6020804]\n",
      "\n",
      "Test loss: 0.604810, Train loss: 0.605683\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19972004]]\n",
      "linear.bias:\n",
      " [0.60157424]\n",
      "\n",
      "Test loss: 0.604295, Train loss: 0.605162\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19937932]]\n",
      "linear.bias:\n",
      " [0.6010681]\n",
      "\n",
      "Test loss: 0.603781, Train loss: 0.604641\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19903912]]\n",
      "linear.bias:\n",
      " [0.6005619]\n",
      "\n",
      "Test loss: 0.603266, Train loss: 0.604120\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19869936]]\n",
      "linear.bias:\n",
      " [0.60005575]\n",
      "\n",
      "Test loss: 0.602751, Train loss: 0.603599\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19836026]]\n",
      "linear.bias:\n",
      " [0.5995496]\n",
      "\n",
      "Test loss: 0.602237, Train loss: 0.603078\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19802175]]\n",
      "linear.bias:\n",
      " [0.59904337]\n",
      "\n",
      "Test loss: 0.601723, Train loss: 0.602557\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19768374]]\n",
      "linear.bias:\n",
      " [0.59853715]\n",
      "\n",
      "Test loss: 0.601208, Train loss: 0.602036\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19734643]]\n",
      "linear.bias:\n",
      " [0.5980309]\n",
      "\n",
      "Test loss: 0.600694, Train loss: 0.601515\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19700976]]\n",
      "linear.bias:\n",
      " [0.5975247]\n",
      "\n",
      "Test loss: 0.600180, Train loss: 0.600995\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19667362]]\n",
      "linear.bias:\n",
      " [0.5970185]\n",
      "\n",
      "Test loss: 0.599666, Train loss: 0.600474\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19633794]]\n",
      "linear.bias:\n",
      " [0.59651226]\n",
      "\n",
      "Test loss: 0.599152, Train loss: 0.599954\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19600326]]\n",
      "linear.bias:\n",
      " [0.596006]\n",
      "\n",
      "Test loss: 0.598638, Train loss: 0.599433\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19566944]]\n",
      "linear.bias:\n",
      " [0.5954997]\n",
      "\n",
      "Test loss: 0.598124, Train loss: 0.598913\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19533637]]\n",
      "linear.bias:\n",
      " [0.59499335]\n",
      "\n",
      "Test loss: 0.597610, Train loss: 0.598393\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19500396]]\n",
      "linear.bias:\n",
      " [0.594487]\n",
      "\n",
      "Test loss: 0.597097, Train loss: 0.597872\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19467211]]\n",
      "linear.bias:\n",
      " [0.59398067]\n",
      "\n",
      "Test loss: 0.596583, Train loss: 0.597352\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19434075]]\n",
      "linear.bias:\n",
      " [0.5934743]\n",
      "\n",
      "Test loss: 0.596070, Train loss: 0.596832\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19401011]]\n",
      "linear.bias:\n",
      " [0.592968]\n",
      "\n",
      "Test loss: 0.595556, Train loss: 0.596312\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19368039]]\n",
      "linear.bias:\n",
      " [0.5924616]\n",
      "\n",
      "Test loss: 0.595043, Train loss: 0.595792\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1933521]]\n",
      "linear.bias:\n",
      " [0.5919552]\n",
      "\n",
      "Test loss: 0.594529, Train loss: 0.595272\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19302537]]\n",
      "linear.bias:\n",
      " [0.5914487]\n",
      "\n",
      "Test loss: 0.594016, Train loss: 0.594753\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19270033]]\n",
      "linear.bias:\n",
      " [0.5909422]\n",
      "\n",
      "Test loss: 0.593502, Train loss: 0.594233\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19237678]]\n",
      "linear.bias:\n",
      " [0.5904356]\n",
      "\n",
      "Test loss: 0.592988, Train loss: 0.593714\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19205455]]\n",
      "linear.bias:\n",
      " [0.58992904]\n",
      "\n",
      "Test loss: 0.592475, Train loss: 0.593195\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1917338]]\n",
      "linear.bias:\n",
      " [0.5894224]\n",
      "\n",
      "Test loss: 0.591961, Train loss: 0.592676\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19141436]]\n",
      "linear.bias:\n",
      " [0.5889157]\n",
      "\n",
      "Test loss: 0.591448, Train loss: 0.592157\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19109607]]\n",
      "linear.bias:\n",
      " [0.588409]\n",
      "\n",
      "Test loss: 0.590934, Train loss: 0.591638\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19077908]]\n",
      "linear.bias:\n",
      " [0.58790225]\n",
      "\n",
      "Test loss: 0.590421, Train loss: 0.591119\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19046324]]\n",
      "linear.bias:\n",
      " [0.5873955]\n",
      "\n",
      "Test loss: 0.589907, Train loss: 0.590600\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.19014843]]\n",
      "linear.bias:\n",
      " [0.5868887]\n",
      "\n",
      "Test loss: 0.589394, Train loss: 0.590081\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1898345]]\n",
      "linear.bias:\n",
      " [0.58638185]\n",
      "\n",
      "Test loss: 0.588880, Train loss: 0.589563\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18952136]]\n",
      "linear.bias:\n",
      " [0.58587503]\n",
      "\n",
      "Test loss: 0.588367, Train loss: 0.589044\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18920918]]\n",
      "linear.bias:\n",
      " [0.58536816]\n",
      "\n",
      "Test loss: 0.587853, Train loss: 0.588525\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18889786]]\n",
      "linear.bias:\n",
      " [0.5848613]\n",
      "\n",
      "Test loss: 0.587340, Train loss: 0.588007\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1885876]]\n",
      "linear.bias:\n",
      " [0.5843544]\n",
      "\n",
      "Test loss: 0.586826, Train loss: 0.587488\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18827827]]\n",
      "linear.bias:\n",
      " [0.58384746]\n",
      "\n",
      "Test loss: 0.586313, Train loss: 0.586970\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18796976]]\n",
      "linear.bias:\n",
      " [0.5833405]\n",
      "\n",
      "Test loss: 0.585799, Train loss: 0.586452\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18766195]]\n",
      "linear.bias:\n",
      " [0.5828336]\n",
      "\n",
      "Test loss: 0.585286, Train loss: 0.585934\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18735474]]\n",
      "linear.bias:\n",
      " [0.58232665]\n",
      "\n",
      "Test loss: 0.584772, Train loss: 0.585415\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18704839]]\n",
      "linear.bias:\n",
      " [0.58181965]\n",
      "\n",
      "Test loss: 0.584259, Train loss: 0.584897\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18674277]]\n",
      "linear.bias:\n",
      " [0.58131266]\n",
      "\n",
      "Test loss: 0.583746, Train loss: 0.584379\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1864381]]\n",
      "linear.bias:\n",
      " [0.58080566]\n",
      "\n",
      "Test loss: 0.583232, Train loss: 0.583861\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18613426]]\n",
      "linear.bias:\n",
      " [0.5802986]\n",
      "\n",
      "Test loss: 0.582719, Train loss: 0.583343\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18583146]]\n",
      "linear.bias:\n",
      " [0.57979155]\n",
      "\n",
      "Test loss: 0.582205, Train loss: 0.582825\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18552987]]\n",
      "linear.bias:\n",
      " [0.5792844]\n",
      "\n",
      "Test loss: 0.581692, Train loss: 0.582308\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18522969]]\n",
      "linear.bias:\n",
      " [0.5787773]\n",
      "\n",
      "Test loss: 0.581178, Train loss: 0.581790\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18493074]]\n",
      "linear.bias:\n",
      " [0.57827014]\n",
      "\n",
      "Test loss: 0.580665, Train loss: 0.581273\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18463288]]\n",
      "linear.bias:\n",
      " [0.57776296]\n",
      "\n",
      "Test loss: 0.580152, Train loss: 0.580755\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18433629]]\n",
      "linear.bias:\n",
      " [0.5772557]\n",
      "\n",
      "Test loss: 0.579638, Train loss: 0.580238\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18404081]]\n",
      "linear.bias:\n",
      " [0.5767485]\n",
      "\n",
      "Test loss: 0.579125, Train loss: 0.579721\n",
      "Epoch [500/5000], Loss: 0.579203\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18374665]]\n",
      "linear.bias:\n",
      " [0.5762412]\n",
      "\n",
      "Test loss: 0.578611, Train loss: 0.579203\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18345363]]\n",
      "linear.bias:\n",
      " [0.5757339]\n",
      "\n",
      "Test loss: 0.578098, Train loss: 0.578686\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18316165]]\n",
      "linear.bias:\n",
      " [0.5752266]\n",
      "\n",
      "Test loss: 0.577585, Train loss: 0.578169\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18287054]]\n",
      "linear.bias:\n",
      " [0.57471925]\n",
      "\n",
      "Test loss: 0.577071, Train loss: 0.577652\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18258022]]\n",
      "linear.bias:\n",
      " [0.5742119]\n",
      "\n",
      "Test loss: 0.576558, Train loss: 0.577135\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18229057]]\n",
      "linear.bias:\n",
      " [0.57370454]\n",
      "\n",
      "Test loss: 0.576045, Train loss: 0.576618\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18200152]]\n",
      "linear.bias:\n",
      " [0.5731972]\n",
      "\n",
      "Test loss: 0.575531, Train loss: 0.576101\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18171297]]\n",
      "linear.bias:\n",
      " [0.57268983]\n",
      "\n",
      "Test loss: 0.575018, Train loss: 0.575584\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18142486]]\n",
      "linear.bias:\n",
      " [0.5721825]\n",
      "\n",
      "Test loss: 0.574505, Train loss: 0.575067\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1811371]]\n",
      "linear.bias:\n",
      " [0.5716751]\n",
      "\n",
      "Test loss: 0.573992, Train loss: 0.574551\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18084966]]\n",
      "linear.bias:\n",
      " [0.57116777]\n",
      "\n",
      "Test loss: 0.573478, Train loss: 0.574034\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1805628]]\n",
      "linear.bias:\n",
      " [0.57066035]\n",
      "\n",
      "Test loss: 0.572965, Train loss: 0.573517\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.18027645]]\n",
      "linear.bias:\n",
      " [0.57015294]\n",
      "\n",
      "Test loss: 0.572452, Train loss: 0.573000\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17999054]]\n",
      "linear.bias:\n",
      " [0.5696455]\n",
      "\n",
      "Test loss: 0.571938, Train loss: 0.572484\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.179705]]\n",
      "linear.bias:\n",
      " [0.5691381]\n",
      "\n",
      "Test loss: 0.571425, Train loss: 0.571967\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17942008]]\n",
      "linear.bias:\n",
      " [0.5686307]\n",
      "\n",
      "Test loss: 0.570912, Train loss: 0.571450\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17913572]]\n",
      "linear.bias:\n",
      " [0.5681233]\n",
      "\n",
      "Test loss: 0.570399, Train loss: 0.570934\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17885183]]\n",
      "linear.bias:\n",
      " [0.56761587]\n",
      "\n",
      "Test loss: 0.569885, Train loss: 0.570417\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17856833]]\n",
      "linear.bias:\n",
      " [0.56710845]\n",
      "\n",
      "Test loss: 0.569372, Train loss: 0.569901\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17828517]]\n",
      "linear.bias:\n",
      " [0.56660104]\n",
      "\n",
      "Test loss: 0.568859, Train loss: 0.569384\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17800228]]\n",
      "linear.bias:\n",
      " [0.5660936]\n",
      "\n",
      "Test loss: 0.568346, Train loss: 0.568868\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17771964]]\n",
      "linear.bias:\n",
      " [0.5655862]\n",
      "\n",
      "Test loss: 0.567833, Train loss: 0.568351\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17743717]]\n",
      "linear.bias:\n",
      " [0.5650788]\n",
      "\n",
      "Test loss: 0.567320, Train loss: 0.567835\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17715484]]\n",
      "linear.bias:\n",
      " [0.5645714]\n",
      "\n",
      "Test loss: 0.566806, Train loss: 0.567318\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17687261]]\n",
      "linear.bias:\n",
      " [0.56406397]\n",
      "\n",
      "Test loss: 0.566293, Train loss: 0.566802\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17659046]]\n",
      "linear.bias:\n",
      " [0.5635566]\n",
      "\n",
      "Test loss: 0.565780, Train loss: 0.566286\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17630868]]\n",
      "linear.bias:\n",
      " [0.56304926]\n",
      "\n",
      "Test loss: 0.565267, Train loss: 0.565769\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1760272]]\n",
      "linear.bias:\n",
      " [0.5625419]\n",
      "\n",
      "Test loss: 0.564754, Train loss: 0.565253\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17574596]]\n",
      "linear.bias:\n",
      " [0.56203455]\n",
      "\n",
      "Test loss: 0.564241, Train loss: 0.564737\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17546493]]\n",
      "linear.bias:\n",
      " [0.5615272]\n",
      "\n",
      "Test loss: 0.563728, Train loss: 0.564221\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17518406]]\n",
      "linear.bias:\n",
      " [0.56101984]\n",
      "\n",
      "Test loss: 0.563215, Train loss: 0.563704\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1749033]]\n",
      "linear.bias:\n",
      " [0.5605125]\n",
      "\n",
      "Test loss: 0.562702, Train loss: 0.563188\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17462264]]\n",
      "linear.bias:\n",
      " [0.5600051]\n",
      "\n",
      "Test loss: 0.562190, Train loss: 0.562672\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17434204]]\n",
      "linear.bias:\n",
      " [0.5594978]\n",
      "\n",
      "Test loss: 0.561677, Train loss: 0.562156\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17406146]]\n",
      "linear.bias:\n",
      " [0.5589904]\n",
      "\n",
      "Test loss: 0.561164, Train loss: 0.561640\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17378089]]\n",
      "linear.bias:\n",
      " [0.5584831]\n",
      "\n",
      "Test loss: 0.560651, Train loss: 0.561124\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17350028]]\n",
      "linear.bias:\n",
      " [0.5579758]\n",
      "\n",
      "Test loss: 0.560138, Train loss: 0.560607\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17321965]]\n",
      "linear.bias:\n",
      " [0.55746853]\n",
      "\n",
      "Test loss: 0.559626, Train loss: 0.560091\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17293896]]\n",
      "linear.bias:\n",
      " [0.55696124]\n",
      "\n",
      "Test loss: 0.559113, Train loss: 0.559575\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17265818]]\n",
      "linear.bias:\n",
      " [0.55645394]\n",
      "\n",
      "Test loss: 0.558600, Train loss: 0.559059\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17237765]]\n",
      "linear.bias:\n",
      " [0.55594665]\n",
      "\n",
      "Test loss: 0.558087, Train loss: 0.558543\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17209733]]\n",
      "linear.bias:\n",
      " [0.55543935]\n",
      "\n",
      "Test loss: 0.557575, Train loss: 0.558027\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17181717]]\n",
      "linear.bias:\n",
      " [0.5549321]\n",
      "\n",
      "Test loss: 0.557062, Train loss: 0.557511\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17153713]]\n",
      "linear.bias:\n",
      " [0.5544249]\n",
      "\n",
      "Test loss: 0.556549, Train loss: 0.556995\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17125718]]\n",
      "linear.bias:\n",
      " [0.55391765]\n",
      "\n",
      "Test loss: 0.556037, Train loss: 0.556479\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1709773]]\n",
      "linear.bias:\n",
      " [0.5534104]\n",
      "\n",
      "Test loss: 0.555524, Train loss: 0.555963\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17069744]]\n",
      "linear.bias:\n",
      " [0.5529032]\n",
      "\n",
      "Test loss: 0.555011, Train loss: 0.555447\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17041758]]\n",
      "linear.bias:\n",
      " [0.55239594]\n",
      "\n",
      "Test loss: 0.554498, Train loss: 0.554931\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.17013769]]\n",
      "linear.bias:\n",
      " [0.5518887]\n",
      "\n",
      "Test loss: 0.553986, Train loss: 0.554415\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16985777]]\n",
      "linear.bias:\n",
      " [0.5513815]\n",
      "\n",
      "Test loss: 0.553473, Train loss: 0.553899\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16957779]]\n",
      "linear.bias:\n",
      " [0.55087435]\n",
      "\n",
      "Test loss: 0.552961, Train loss: 0.553383\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16929774]]\n",
      "linear.bias:\n",
      " [0.5503672]\n",
      "\n",
      "Test loss: 0.552448, Train loss: 0.552868\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1690176]]\n",
      "linear.bias:\n",
      " [0.54986]\n",
      "\n",
      "Test loss: 0.551935, Train loss: 0.552352\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16873772]]\n",
      "linear.bias:\n",
      " [0.5493528]\n",
      "\n",
      "Test loss: 0.551423, Train loss: 0.551836\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16845806]]\n",
      "linear.bias:\n",
      " [0.54884565]\n",
      "\n",
      "Test loss: 0.550910, Train loss: 0.551320\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16817892]]\n",
      "linear.bias:\n",
      " [0.5483385]\n",
      "\n",
      "Test loss: 0.550397, Train loss: 0.550804\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16790022]]\n",
      "linear.bias:\n",
      " [0.5478313]\n",
      "\n",
      "Test loss: 0.549885, Train loss: 0.550289\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16762191]]\n",
      "linear.bias:\n",
      " [0.5473241]\n",
      "\n",
      "Test loss: 0.549372, Train loss: 0.549773\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16734391]]\n",
      "linear.bias:\n",
      " [0.54681695]\n",
      "\n",
      "Test loss: 0.548860, Train loss: 0.549257\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16706619]]\n",
      "linear.bias:\n",
      " [0.54630977]\n",
      "\n",
      "Test loss: 0.548347, Train loss: 0.548742\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16678901]]\n",
      "linear.bias:\n",
      " [0.5458026]\n",
      "\n",
      "Test loss: 0.547835, Train loss: 0.548226\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1665123]]\n",
      "linear.bias:\n",
      " [0.5452954]\n",
      "\n",
      "Test loss: 0.547322, Train loss: 0.547711\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16623598]]\n",
      "linear.bias:\n",
      " [0.54478824]\n",
      "\n",
      "Test loss: 0.546809, Train loss: 0.547195\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16596001]]\n",
      "linear.bias:\n",
      " [0.54428107]\n",
      "\n",
      "Test loss: 0.546297, Train loss: 0.546680\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16568434]]\n",
      "linear.bias:\n",
      " [0.5437739]\n",
      "\n",
      "Test loss: 0.545785, Train loss: 0.546164\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1654089]]\n",
      "linear.bias:\n",
      " [0.5432667]\n",
      "\n",
      "Test loss: 0.545272, Train loss: 0.545649\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16513364]]\n",
      "linear.bias:\n",
      " [0.54275954]\n",
      "\n",
      "Test loss: 0.544760, Train loss: 0.545134\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16485853]]\n",
      "linear.bias:\n",
      " [0.54225236]\n",
      "\n",
      "Test loss: 0.544247, Train loss: 0.544618\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16458355]]\n",
      "linear.bias:\n",
      " [0.54174525]\n",
      "\n",
      "Test loss: 0.543735, Train loss: 0.544103\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16430864]]\n",
      "linear.bias:\n",
      " [0.5412381]\n",
      "\n",
      "Test loss: 0.543222, Train loss: 0.543588\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16403379]]\n",
      "linear.bias:\n",
      " [0.540731]\n",
      "\n",
      "Test loss: 0.542710, Train loss: 0.543072\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16375895]]\n",
      "linear.bias:\n",
      " [0.5402239]\n",
      "\n",
      "Test loss: 0.542197, Train loss: 0.542557\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16348411]]\n",
      "linear.bias:\n",
      " [0.5397168]\n",
      "\n",
      "Test loss: 0.541685, Train loss: 0.542042\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16320926]]\n",
      "linear.bias:\n",
      " [0.53920966]\n",
      "\n",
      "Test loss: 0.541173, Train loss: 0.541526\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16293436]]\n",
      "linear.bias:\n",
      " [0.53870255]\n",
      "\n",
      "Test loss: 0.540660, Train loss: 0.541011\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16265978]]\n",
      "linear.bias:\n",
      " [0.5381955]\n",
      "\n",
      "Test loss: 0.540148, Train loss: 0.540496\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16238545]]\n",
      "linear.bias:\n",
      " [0.53768843]\n",
      "\n",
      "Test loss: 0.539635, Train loss: 0.539981\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16211131]]\n",
      "linear.bias:\n",
      " [0.5371814]\n",
      "\n",
      "Test loss: 0.539123, Train loss: 0.539465\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16183735]]\n",
      "linear.bias:\n",
      " [0.5366743]\n",
      "\n",
      "Test loss: 0.538611, Train loss: 0.538950\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16156352]]\n",
      "linear.bias:\n",
      " [0.53616726]\n",
      "\n",
      "Test loss: 0.538098, Train loss: 0.538435\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16129018]]\n",
      "linear.bias:\n",
      " [0.5356602]\n",
      "\n",
      "Test loss: 0.537586, Train loss: 0.537920\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16101728]]\n",
      "linear.bias:\n",
      " [0.53515315]\n",
      "\n",
      "Test loss: 0.537074, Train loss: 0.537405\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16074474]]\n",
      "linear.bias:\n",
      " [0.5346461]\n",
      "\n",
      "Test loss: 0.536561, Train loss: 0.536890\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16047251]]\n",
      "linear.bias:\n",
      " [0.53413904]\n",
      "\n",
      "Test loss: 0.536049, Train loss: 0.536375\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.16020054]]\n",
      "linear.bias:\n",
      " [0.533632]\n",
      "\n",
      "Test loss: 0.535537, Train loss: 0.535860\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15992877]]\n",
      "linear.bias:\n",
      " [0.5331249]\n",
      "\n",
      "Test loss: 0.535025, Train loss: 0.535345\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15965752]]\n",
      "linear.bias:\n",
      " [0.53261787]\n",
      "\n",
      "Test loss: 0.534512, Train loss: 0.534831\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15938671]]\n",
      "linear.bias:\n",
      " [0.5321108]\n",
      "\n",
      "Test loss: 0.534000, Train loss: 0.534316\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15911627]]\n",
      "linear.bias:\n",
      " [0.53160375]\n",
      "\n",
      "Test loss: 0.533488, Train loss: 0.533801\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15884614]]\n",
      "linear.bias:\n",
      " [0.5310967]\n",
      "\n",
      "Test loss: 0.532976, Train loss: 0.533286\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15857628]]\n",
      "linear.bias:\n",
      " [0.53058964]\n",
      "\n",
      "Test loss: 0.532463, Train loss: 0.532771\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15830663]]\n",
      "linear.bias:\n",
      " [0.5300826]\n",
      "\n",
      "Test loss: 0.531951, Train loss: 0.532257\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15803716]]\n",
      "linear.bias:\n",
      " [0.5295755]\n",
      "\n",
      "Test loss: 0.531439, Train loss: 0.531742\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15776782]]\n",
      "linear.bias:\n",
      " [0.52906847]\n",
      "\n",
      "Test loss: 0.530927, Train loss: 0.531227\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15749858]]\n",
      "linear.bias:\n",
      " [0.5285614]\n",
      "\n",
      "Test loss: 0.530415, Train loss: 0.530713\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15722941]]\n",
      "linear.bias:\n",
      " [0.52805436]\n",
      "\n",
      "Test loss: 0.529902, Train loss: 0.530198\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15696028]]\n",
      "linear.bias:\n",
      " [0.52754736]\n",
      "\n",
      "Test loss: 0.529390, Train loss: 0.529683\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15669154]]\n",
      "linear.bias:\n",
      " [0.52704036]\n",
      "\n",
      "Test loss: 0.528878, Train loss: 0.529169\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15642312]]\n",
      "linear.bias:\n",
      " [0.52653337]\n",
      "\n",
      "Test loss: 0.528366, Train loss: 0.528654\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15615499]]\n",
      "linear.bias:\n",
      " [0.52602637]\n",
      "\n",
      "Test loss: 0.527854, Train loss: 0.528140\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15588708]]\n",
      "linear.bias:\n",
      " [0.5255194]\n",
      "\n",
      "Test loss: 0.527342, Train loss: 0.527625\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15561937]]\n",
      "linear.bias:\n",
      " [0.5250124]\n",
      "\n",
      "Test loss: 0.526830, Train loss: 0.527111\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15535179]]\n",
      "linear.bias:\n",
      " [0.5245054]\n",
      "\n",
      "Test loss: 0.526317, Train loss: 0.526596\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15508433]]\n",
      "linear.bias:\n",
      " [0.5239984]\n",
      "\n",
      "Test loss: 0.525805, Train loss: 0.526082\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15481694]]\n",
      "linear.bias:\n",
      " [0.5234914]\n",
      "\n",
      "Test loss: 0.525293, Train loss: 0.525567\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15454999]]\n",
      "linear.bias:\n",
      " [0.5229844]\n",
      "\n",
      "Test loss: 0.524781, Train loss: 0.525053\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15428339]]\n",
      "linear.bias:\n",
      " [0.5224774]\n",
      "\n",
      "Test loss: 0.524269, Train loss: 0.524538\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15401709]]\n",
      "linear.bias:\n",
      " [0.5219704]\n",
      "\n",
      "Test loss: 0.523757, Train loss: 0.524024\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15375106]]\n",
      "linear.bias:\n",
      " [0.5214634]\n",
      "\n",
      "Test loss: 0.523245, Train loss: 0.523510\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15348558]]\n",
      "linear.bias:\n",
      " [0.5209564]\n",
      "\n",
      "Test loss: 0.522733, Train loss: 0.522995\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15322058]]\n",
      "linear.bias:\n",
      " [0.5204494]\n",
      "\n",
      "Test loss: 0.522221, Train loss: 0.522481\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.152956]]\n",
      "linear.bias:\n",
      " [0.5199424]\n",
      "\n",
      "Test loss: 0.521709, Train loss: 0.521967\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15269177]]\n",
      "linear.bias:\n",
      " [0.5194354]\n",
      "\n",
      "Test loss: 0.521197, Train loss: 0.521453\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15242822]]\n",
      "linear.bias:\n",
      " [0.5189284]\n",
      "\n",
      "Test loss: 0.520685, Train loss: 0.520939\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15216526]]\n",
      "linear.bias:\n",
      " [0.5184214]\n",
      "\n",
      "Test loss: 0.520173, Train loss: 0.520425\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15190282]]\n",
      "linear.bias:\n",
      " [0.5179144]\n",
      "\n",
      "Test loss: 0.519661, Train loss: 0.519911\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15164083]]\n",
      "linear.bias:\n",
      " [0.51740736]\n",
      "\n",
      "Test loss: 0.519149, Train loss: 0.519397\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15137921]]\n",
      "linear.bias:\n",
      " [0.5169003]\n",
      "\n",
      "Test loss: 0.518637, Train loss: 0.518883\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15111792]]\n",
      "linear.bias:\n",
      " [0.51639324]\n",
      "\n",
      "Test loss: 0.518125, Train loss: 0.518369\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1508569]]\n",
      "linear.bias:\n",
      " [0.5158862]\n",
      "\n",
      "Test loss: 0.517613, Train loss: 0.517855\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1505961]]\n",
      "linear.bias:\n",
      " [0.5153792]\n",
      "\n",
      "Test loss: 0.517101, Train loss: 0.517341\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.15033548]]\n",
      "linear.bias:\n",
      " [0.5148722]\n",
      "\n",
      "Test loss: 0.516589, Train loss: 0.516827\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.150075]]\n",
      "linear.bias:\n",
      " [0.5143652]\n",
      "\n",
      "Test loss: 0.516077, Train loss: 0.516313\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14981464]]\n",
      "linear.bias:\n",
      " [0.5138582]\n",
      "\n",
      "Test loss: 0.515565, Train loss: 0.515799\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14955434]]\n",
      "linear.bias:\n",
      " [0.5133512]\n",
      "\n",
      "Test loss: 0.515053, Train loss: 0.515285\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14929411]]\n",
      "linear.bias:\n",
      " [0.5128442]\n",
      "\n",
      "Test loss: 0.514541, Train loss: 0.514771\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14903389]]\n",
      "linear.bias:\n",
      " [0.5123372]\n",
      "\n",
      "Test loss: 0.514030, Train loss: 0.514257\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14877367]]\n",
      "linear.bias:\n",
      " [0.5118302]\n",
      "\n",
      "Test loss: 0.513518, Train loss: 0.513743\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14851344]]\n",
      "linear.bias:\n",
      " [0.5113233]\n",
      "\n",
      "Test loss: 0.513006, Train loss: 0.513229\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14825317]]\n",
      "linear.bias:\n",
      " [0.51081634]\n",
      "\n",
      "Test loss: 0.512494, Train loss: 0.512716\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14799285]]\n",
      "linear.bias:\n",
      " [0.5103094]\n",
      "\n",
      "Test loss: 0.511982, Train loss: 0.512202\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14773247]]\n",
      "linear.bias:\n",
      " [0.50980246]\n",
      "\n",
      "Test loss: 0.511470, Train loss: 0.511688\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.147472]]\n",
      "linear.bias:\n",
      " [0.5092955]\n",
      "\n",
      "Test loss: 0.510958, Train loss: 0.511174\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14721143]]\n",
      "linear.bias:\n",
      " [0.50878865]\n",
      "\n",
      "Test loss: 0.510447, Train loss: 0.510660\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14695077]]\n",
      "linear.bias:\n",
      " [0.50828177]\n",
      "\n",
      "Test loss: 0.509935, Train loss: 0.510146\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14669]]\n",
      "linear.bias:\n",
      " [0.5077749]\n",
      "\n",
      "Test loss: 0.509423, Train loss: 0.509633\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14642948]]\n",
      "linear.bias:\n",
      " [0.507268]\n",
      "\n",
      "Test loss: 0.508911, Train loss: 0.509119\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14616917]]\n",
      "linear.bias:\n",
      " [0.50676113]\n",
      "\n",
      "Test loss: 0.508399, Train loss: 0.508605\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14590904]]\n",
      "linear.bias:\n",
      " [0.50625426]\n",
      "\n",
      "Test loss: 0.507888, Train loss: 0.508092\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14564903]]\n",
      "linear.bias:\n",
      " [0.5057474]\n",
      "\n",
      "Test loss: 0.507376, Train loss: 0.507578\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14538912]]\n",
      "linear.bias:\n",
      " [0.5052405]\n",
      "\n",
      "Test loss: 0.506864, Train loss: 0.507064\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1451293]]\n",
      "linear.bias:\n",
      " [0.5047336]\n",
      "\n",
      "Test loss: 0.506352, Train loss: 0.506550\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1448695]]\n",
      "linear.bias:\n",
      " [0.5042268]\n",
      "\n",
      "Test loss: 0.505841, Train loss: 0.506037\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14461012]]\n",
      "linear.bias:\n",
      " [0.50372]\n",
      "\n",
      "Test loss: 0.505329, Train loss: 0.505523\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14435108]]\n",
      "linear.bias:\n",
      " [0.50321317]\n",
      "\n",
      "Test loss: 0.504817, Train loss: 0.505010\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14409234]]\n",
      "linear.bias:\n",
      " [0.50270635]\n",
      "\n",
      "Test loss: 0.504305, Train loss: 0.504496\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14383383]]\n",
      "linear.bias:\n",
      " [0.50219953]\n",
      "\n",
      "Test loss: 0.503794, Train loss: 0.503983\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14357553]]\n",
      "linear.bias:\n",
      " [0.5016927]\n",
      "\n",
      "Test loss: 0.503282, Train loss: 0.503469\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1433174]]\n",
      "linear.bias:\n",
      " [0.5011859]\n",
      "\n",
      "Test loss: 0.502770, Train loss: 0.502956\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14305939]]\n",
      "linear.bias:\n",
      " [0.5006791]\n",
      "\n",
      "Test loss: 0.502259, Train loss: 0.502442\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14280227]]\n",
      "linear.bias:\n",
      " [0.50017226]\n",
      "\n",
      "Test loss: 0.501747, Train loss: 0.501929\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14254592]]\n",
      "linear.bias:\n",
      " [0.4996654]\n",
      "\n",
      "Test loss: 0.501235, Train loss: 0.501416\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14229026]]\n",
      "linear.bias:\n",
      " [0.49915856]\n",
      "\n",
      "Test loss: 0.500724, Train loss: 0.500902\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14203522]]\n",
      "linear.bias:\n",
      " [0.49865168]\n",
      "\n",
      "Test loss: 0.500212, Train loss: 0.500389\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14178069]]\n",
      "linear.bias:\n",
      " [0.4981448]\n",
      "\n",
      "Test loss: 0.499701, Train loss: 0.499876\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14152661]]\n",
      "linear.bias:\n",
      " [0.49763793]\n",
      "\n",
      "Test loss: 0.499189, Train loss: 0.499363\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14127292]]\n",
      "linear.bias:\n",
      " [0.49713105]\n",
      "\n",
      "Test loss: 0.498677, Train loss: 0.498849\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14101955]]\n",
      "linear.bias:\n",
      " [0.49662417]\n",
      "\n",
      "Test loss: 0.498166, Train loss: 0.498336\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14076646]]\n",
      "linear.bias:\n",
      " [0.4961173]\n",
      "\n",
      "Test loss: 0.497654, Train loss: 0.497823\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1405136]]\n",
      "linear.bias:\n",
      " [0.49561042]\n",
      "\n",
      "Test loss: 0.497143, Train loss: 0.497310\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14026092]]\n",
      "linear.bias:\n",
      " [0.49510357]\n",
      "\n",
      "Test loss: 0.496631, Train loss: 0.496797\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.14000839]]\n",
      "linear.bias:\n",
      " [0.49459672]\n",
      "\n",
      "Test loss: 0.496119, Train loss: 0.496284\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13975598]]\n",
      "linear.bias:\n",
      " [0.49408987]\n",
      "\n",
      "Test loss: 0.495608, Train loss: 0.495771\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13950366]]\n",
      "linear.bias:\n",
      " [0.49358302]\n",
      "\n",
      "Test loss: 0.495096, Train loss: 0.495258\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1392514]]\n",
      "linear.bias:\n",
      " [0.49307618]\n",
      "\n",
      "Test loss: 0.494585, Train loss: 0.494744\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13899916]]\n",
      "linear.bias:\n",
      " [0.49256936]\n",
      "\n",
      "Test loss: 0.494073, Train loss: 0.494231\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13874695]]\n",
      "linear.bias:\n",
      " [0.49206254]\n",
      "\n",
      "Test loss: 0.493562, Train loss: 0.493718\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13849472]]\n",
      "linear.bias:\n",
      " [0.49155572]\n",
      "\n",
      "Test loss: 0.493050, Train loss: 0.493205\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13824245]]\n",
      "linear.bias:\n",
      " [0.49104893]\n",
      "\n",
      "Test loss: 0.492539, Train loss: 0.492692\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13799016]]\n",
      "linear.bias:\n",
      " [0.49054214]\n",
      "\n",
      "Test loss: 0.492027, Train loss: 0.492179\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13773781]]\n",
      "linear.bias:\n",
      " [0.49003536]\n",
      "\n",
      "Test loss: 0.491516, Train loss: 0.491666\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13748538]]\n",
      "linear.bias:\n",
      " [0.4895286]\n",
      "\n",
      "Test loss: 0.491004, Train loss: 0.491153\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13723287]]\n",
      "linear.bias:\n",
      " [0.48902184]\n",
      "\n",
      "Test loss: 0.490493, Train loss: 0.490640\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13698027]]\n",
      "linear.bias:\n",
      " [0.4885151]\n",
      "\n",
      "Test loss: 0.489982, Train loss: 0.490127\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13672756]]\n",
      "linear.bias:\n",
      " [0.48800838]\n",
      "\n",
      "Test loss: 0.489470, Train loss: 0.489614\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13647474]]\n",
      "linear.bias:\n",
      " [0.48750165]\n",
      "\n",
      "Test loss: 0.488959, Train loss: 0.489101\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1362218]]\n",
      "linear.bias:\n",
      " [0.48699495]\n",
      "\n",
      "Test loss: 0.488447, Train loss: 0.488588\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13596873]]\n",
      "linear.bias:\n",
      " [0.48648825]\n",
      "\n",
      "Test loss: 0.487936, Train loss: 0.488075\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13571596]]\n",
      "linear.bias:\n",
      " [0.48598155]\n",
      "\n",
      "Test loss: 0.487425, Train loss: 0.487562\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13546343]]\n",
      "linear.bias:\n",
      " [0.48547485]\n",
      "\n",
      "Test loss: 0.486913, Train loss: 0.487050\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1352111]]\n",
      "linear.bias:\n",
      " [0.48496816]\n",
      "\n",
      "Test loss: 0.486402, Train loss: 0.486537\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13495892]]\n",
      "linear.bias:\n",
      " [0.4844615]\n",
      "\n",
      "Test loss: 0.485890, Train loss: 0.486024\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13470688]]\n",
      "linear.bias:\n",
      " [0.48395482]\n",
      "\n",
      "Test loss: 0.485379, Train loss: 0.485511\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13445494]]\n",
      "linear.bias:\n",
      " [0.48344815]\n",
      "\n",
      "Test loss: 0.484868, Train loss: 0.484998\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13420306]]\n",
      "linear.bias:\n",
      " [0.48294148]\n",
      "\n",
      "Test loss: 0.484356, Train loss: 0.484485\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13395166]]\n",
      "linear.bias:\n",
      " [0.4824348]\n",
      "\n",
      "Test loss: 0.483845, Train loss: 0.483973\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13370068]]\n",
      "linear.bias:\n",
      " [0.48192814]\n",
      "\n",
      "Test loss: 0.483334, Train loss: 0.483460\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1334505]]\n",
      "linear.bias:\n",
      " [0.48142147]\n",
      "\n",
      "Test loss: 0.482822, Train loss: 0.482947\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13320099]]\n",
      "linear.bias:\n",
      " [0.4809148]\n",
      "\n",
      "Test loss: 0.482311, Train loss: 0.482435\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13295208]]\n",
      "linear.bias:\n",
      " [0.4804081]\n",
      "\n",
      "Test loss: 0.481800, Train loss: 0.481922\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13270369]]\n",
      "linear.bias:\n",
      " [0.4799014]\n",
      "\n",
      "Test loss: 0.481288, Train loss: 0.481410\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13245575]]\n",
      "linear.bias:\n",
      " [0.4793947]\n",
      "\n",
      "Test loss: 0.480777, Train loss: 0.480897\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1322082]]\n",
      "linear.bias:\n",
      " [0.478888]\n",
      "\n",
      "Test loss: 0.480266, Train loss: 0.480385\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13196099]]\n",
      "linear.bias:\n",
      " [0.4783813]\n",
      "\n",
      "Test loss: 0.479755, Train loss: 0.479872\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13171406]]\n",
      "linear.bias:\n",
      " [0.4778746]\n",
      "\n",
      "Test loss: 0.479243, Train loss: 0.479360\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1314678]]\n",
      "linear.bias:\n",
      " [0.4773679]\n",
      "\n",
      "Test loss: 0.478732, Train loss: 0.478847\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13122214]]\n",
      "linear.bias:\n",
      " [0.4768612]\n",
      "\n",
      "Test loss: 0.478221, Train loss: 0.478335\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13097699]]\n",
      "linear.bias:\n",
      " [0.4763545]\n",
      "\n",
      "Test loss: 0.477710, Train loss: 0.477823\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13073228]]\n",
      "linear.bias:\n",
      " [0.4758478]\n",
      "\n",
      "Test loss: 0.477199, Train loss: 0.477310\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13048796]]\n",
      "linear.bias:\n",
      " [0.4753411]\n",
      "\n",
      "Test loss: 0.476689, Train loss: 0.476798\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13024397]]\n",
      "linear.bias:\n",
      " [0.4748344]\n",
      "\n",
      "Test loss: 0.476178, Train loss: 0.476286\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.13000071]]\n",
      "linear.bias:\n",
      " [0.4743277]\n",
      "\n",
      "Test loss: 0.475667, Train loss: 0.475774\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1297581]]\n",
      "linear.bias:\n",
      " [0.47382098]\n",
      "\n",
      "Test loss: 0.475157, Train loss: 0.475262\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12951605]]\n",
      "linear.bias:\n",
      " [0.47331426]\n",
      "\n",
      "Test loss: 0.474646, Train loss: 0.474749\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12927449]]\n",
      "linear.bias:\n",
      " [0.47280753]\n",
      "\n",
      "Test loss: 0.474135, Train loss: 0.474237\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12903334]]\n",
      "linear.bias:\n",
      " [0.4723008]\n",
      "\n",
      "Test loss: 0.473625, Train loss: 0.473725\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12879255]]\n",
      "linear.bias:\n",
      " [0.47179407]\n",
      "\n",
      "Test loss: 0.473114, Train loss: 0.473213\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12855208]]\n",
      "linear.bias:\n",
      " [0.47128734]\n",
      "\n",
      "Test loss: 0.472603, Train loss: 0.472701\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12831187]]\n",
      "linear.bias:\n",
      " [0.4707806]\n",
      "\n",
      "Test loss: 0.472093, Train loss: 0.472189\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12807187]]\n",
      "linear.bias:\n",
      " [0.4702739]\n",
      "\n",
      "Test loss: 0.471582, Train loss: 0.471677\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12783206]]\n",
      "linear.bias:\n",
      " [0.4697672]\n",
      "\n",
      "Test loss: 0.471072, Train loss: 0.471165\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12759237]]\n",
      "linear.bias:\n",
      " [0.4692605]\n",
      "\n",
      "Test loss: 0.470561, Train loss: 0.470653\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1273528]]\n",
      "linear.bias:\n",
      " [0.4687538]\n",
      "\n",
      "Test loss: 0.470051, Train loss: 0.470141\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12711331]]\n",
      "linear.bias:\n",
      " [0.46824712]\n",
      "\n",
      "Test loss: 0.469540, Train loss: 0.469629\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12687433]]\n",
      "linear.bias:\n",
      " [0.46774042]\n",
      "\n",
      "Test loss: 0.469029, Train loss: 0.469117\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12663579]]\n",
      "linear.bias:\n",
      " [0.46723372]\n",
      "\n",
      "Test loss: 0.468519, Train loss: 0.468605\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12639809]]\n",
      "linear.bias:\n",
      " [0.46672702]\n",
      "\n",
      "Test loss: 0.468008, Train loss: 0.468093\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12616111]]\n",
      "linear.bias:\n",
      " [0.46622032]\n",
      "\n",
      "Test loss: 0.467498, Train loss: 0.467582\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12592478]]\n",
      "linear.bias:\n",
      " [0.46571362]\n",
      "\n",
      "Test loss: 0.466987, Train loss: 0.467070\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12568901]]\n",
      "linear.bias:\n",
      " [0.4652069]\n",
      "\n",
      "Test loss: 0.466477, Train loss: 0.466558\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12545374]]\n",
      "linear.bias:\n",
      " [0.46470016]\n",
      "\n",
      "Test loss: 0.465966, Train loss: 0.466047\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12521933]]\n",
      "linear.bias:\n",
      " [0.46419343]\n",
      "\n",
      "Test loss: 0.465456, Train loss: 0.465535\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12498568]]\n",
      "linear.bias:\n",
      " [0.4636867]\n",
      "\n",
      "Test loss: 0.464945, Train loss: 0.465023\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1247527]]\n",
      "linear.bias:\n",
      " [0.46317995]\n",
      "\n",
      "Test loss: 0.464435, Train loss: 0.464512\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1245203]]\n",
      "linear.bias:\n",
      " [0.4626732]\n",
      "\n",
      "Test loss: 0.463924, Train loss: 0.464000\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12428841]]\n",
      "linear.bias:\n",
      " [0.46216643]\n",
      "\n",
      "Test loss: 0.463414, Train loss: 0.463489\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12405697]]\n",
      "linear.bias:\n",
      " [0.46165967]\n",
      "\n",
      "Test loss: 0.462903, Train loss: 0.462977\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12382636]]\n",
      "linear.bias:\n",
      " [0.4611529]\n",
      "\n",
      "Test loss: 0.462393, Train loss: 0.462466\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1235965]]\n",
      "linear.bias:\n",
      " [0.46064615]\n",
      "\n",
      "Test loss: 0.461883, Train loss: 0.461954\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12336728]]\n",
      "linear.bias:\n",
      " [0.46013936]\n",
      "\n",
      "Test loss: 0.461372, Train loss: 0.461443\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12313864]]\n",
      "linear.bias:\n",
      " [0.45963258]\n",
      "\n",
      "Test loss: 0.460862, Train loss: 0.460932\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12291048]]\n",
      "linear.bias:\n",
      " [0.4591258]\n",
      "\n",
      "Test loss: 0.460351, Train loss: 0.460420\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12268323]]\n",
      "linear.bias:\n",
      " [0.458619]\n",
      "\n",
      "Test loss: 0.459841, Train loss: 0.459909\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12245675]]\n",
      "linear.bias:\n",
      " [0.4581122]\n",
      "\n",
      "Test loss: 0.459331, Train loss: 0.459398\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12223097]]\n",
      "linear.bias:\n",
      " [0.4576054]\n",
      "\n",
      "Test loss: 0.458820, Train loss: 0.458887\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1220058]]\n",
      "linear.bias:\n",
      " [0.45709857]\n",
      "\n",
      "Test loss: 0.458310, Train loss: 0.458375\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12178116]]\n",
      "linear.bias:\n",
      " [0.45659176]\n",
      "\n",
      "Test loss: 0.457800, Train loss: 0.457864\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12155697]]\n",
      "linear.bias:\n",
      " [0.45608494]\n",
      "\n",
      "Test loss: 0.457289, Train loss: 0.457353\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12133363]]\n",
      "linear.bias:\n",
      " [0.45557812]\n",
      "\n",
      "Test loss: 0.456779, Train loss: 0.456842\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12111104]]\n",
      "linear.bias:\n",
      " [0.4550713]\n",
      "\n",
      "Test loss: 0.456269, Train loss: 0.456331\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12088911]]\n",
      "linear.bias:\n",
      " [0.45456445]\n",
      "\n",
      "Test loss: 0.455758, Train loss: 0.455820\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12066776]]\n",
      "linear.bias:\n",
      " [0.4540576]\n",
      "\n",
      "Test loss: 0.455248, Train loss: 0.455309\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12044691]]\n",
      "linear.bias:\n",
      " [0.45355076]\n",
      "\n",
      "Test loss: 0.454738, Train loss: 0.454798\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1202265]]\n",
      "linear.bias:\n",
      " [0.4530439]\n",
      "\n",
      "Test loss: 0.454228, Train loss: 0.454287\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.12000646]]\n",
      "linear.bias:\n",
      " [0.45253706]\n",
      "\n",
      "Test loss: 0.453717, Train loss: 0.453776\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11978675]]\n",
      "linear.bias:\n",
      " [0.4520302]\n",
      "\n",
      "Test loss: 0.453207, Train loss: 0.453265\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1195673]]\n",
      "linear.bias:\n",
      " [0.45152336]\n",
      "\n",
      "Test loss: 0.452697, Train loss: 0.452754\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11934809]]\n",
      "linear.bias:\n",
      " [0.45101655]\n",
      "\n",
      "Test loss: 0.452186, Train loss: 0.452243\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11912908]]\n",
      "linear.bias:\n",
      " [0.45050973]\n",
      "\n",
      "Test loss: 0.451676, Train loss: 0.451732\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11891021]]\n",
      "linear.bias:\n",
      " [0.4500029]\n",
      "\n",
      "Test loss: 0.451166, Train loss: 0.451222\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11869147]]\n",
      "linear.bias:\n",
      " [0.4494961]\n",
      "\n",
      "Test loss: 0.450656, Train loss: 0.450711\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11847282]]\n",
      "linear.bias:\n",
      " [0.44898927]\n",
      "\n",
      "Test loss: 0.450146, Train loss: 0.450200\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11825424]]\n",
      "linear.bias:\n",
      " [0.44848248]\n",
      "\n",
      "Test loss: 0.449635, Train loss: 0.449689\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11803571]]\n",
      "linear.bias:\n",
      " [0.4479757]\n",
      "\n",
      "Test loss: 0.449125, Train loss: 0.449178\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1178172]]\n",
      "linear.bias:\n",
      " [0.4474689]\n",
      "\n",
      "Test loss: 0.448615, Train loss: 0.448667\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1175987]]\n",
      "linear.bias:\n",
      " [0.44696215]\n",
      "\n",
      "Test loss: 0.448105, Train loss: 0.448156\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11738019]]\n",
      "linear.bias:\n",
      " [0.4464554]\n",
      "\n",
      "Test loss: 0.447595, Train loss: 0.447646\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11716165]]\n",
      "linear.bias:\n",
      " [0.44594863]\n",
      "\n",
      "Test loss: 0.447085, Train loss: 0.447135\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11694307]]\n",
      "linear.bias:\n",
      " [0.4454419]\n",
      "\n",
      "Test loss: 0.446574, Train loss: 0.446624\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11672444]]\n",
      "linear.bias:\n",
      " [0.44493517]\n",
      "\n",
      "Test loss: 0.446064, Train loss: 0.446113\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11650621]]\n",
      "linear.bias:\n",
      " [0.44442844]\n",
      "\n",
      "Test loss: 0.445554, Train loss: 0.445603\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11628833]]\n",
      "linear.bias:\n",
      " [0.44392172]\n",
      "\n",
      "Test loss: 0.445044, Train loss: 0.445092\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11607075]]\n",
      "linear.bias:\n",
      " [0.443415]\n",
      "\n",
      "Test loss: 0.444534, Train loss: 0.444581\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11585343]]\n",
      "linear.bias:\n",
      " [0.44290826]\n",
      "\n",
      "Test loss: 0.444024, Train loss: 0.444070\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11563631]]\n",
      "linear.bias:\n",
      " [0.44240153]\n",
      "\n",
      "Test loss: 0.443514, Train loss: 0.443560\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1154198]]\n",
      "linear.bias:\n",
      " [0.4418948]\n",
      "\n",
      "Test loss: 0.443004, Train loss: 0.443049\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11520381]]\n",
      "linear.bias:\n",
      " [0.44138807]\n",
      "\n",
      "Test loss: 0.442494, Train loss: 0.442539\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11498827]]\n",
      "linear.bias:\n",
      " [0.44088134]\n",
      "\n",
      "Test loss: 0.441984, Train loss: 0.442028\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11477361]]\n",
      "linear.bias:\n",
      " [0.4403746]\n",
      "\n",
      "Test loss: 0.441474, Train loss: 0.441517\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11455973]]\n",
      "linear.bias:\n",
      " [0.43986785]\n",
      "\n",
      "Test loss: 0.440964, Train loss: 0.441007\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11434654]]\n",
      "linear.bias:\n",
      " [0.4393611]\n",
      "\n",
      "Test loss: 0.440454, Train loss: 0.440497\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11413395]]\n",
      "linear.bias:\n",
      " [0.43885434]\n",
      "\n",
      "Test loss: 0.439944, Train loss: 0.439986\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11392236]]\n",
      "linear.bias:\n",
      " [0.43834758]\n",
      "\n",
      "Test loss: 0.439434, Train loss: 0.439476\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11371165]]\n",
      "linear.bias:\n",
      " [0.4378408]\n",
      "\n",
      "Test loss: 0.438924, Train loss: 0.438965\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11350171]]\n",
      "linear.bias:\n",
      " [0.437334]\n",
      "\n",
      "Test loss: 0.438414, Train loss: 0.438455\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11329246]]\n",
      "linear.bias:\n",
      " [0.4368272]\n",
      "\n",
      "Test loss: 0.437904, Train loss: 0.437945\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11308429]]\n",
      "linear.bias:\n",
      " [0.4363204]\n",
      "\n",
      "Test loss: 0.437394, Train loss: 0.437434\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11287706]]\n",
      "linear.bias:\n",
      " [0.43581358]\n",
      "\n",
      "Test loss: 0.436884, Train loss: 0.436924\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11267068]]\n",
      "linear.bias:\n",
      " [0.43530673]\n",
      "\n",
      "Test loss: 0.436374, Train loss: 0.436414\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11246505]]\n",
      "linear.bias:\n",
      " [0.43479988]\n",
      "\n",
      "Test loss: 0.435864, Train loss: 0.435904\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11226007]]\n",
      "linear.bias:\n",
      " [0.43429303]\n",
      "\n",
      "Test loss: 0.435354, Train loss: 0.435394\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11205566]]\n",
      "linear.bias:\n",
      " [0.43378618]\n",
      "\n",
      "Test loss: 0.434844, Train loss: 0.434884\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11185175]]\n",
      "linear.bias:\n",
      " [0.43327934]\n",
      "\n",
      "Test loss: 0.434335, Train loss: 0.434373\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11164828]]\n",
      "linear.bias:\n",
      " [0.4327725]\n",
      "\n",
      "Test loss: 0.433825, Train loss: 0.433863\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11144518]]\n",
      "linear.bias:\n",
      " [0.43226564]\n",
      "\n",
      "Test loss: 0.433315, Train loss: 0.433353\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11124241]]\n",
      "linear.bias:\n",
      " [0.4317588]\n",
      "\n",
      "Test loss: 0.432805, Train loss: 0.432843\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11103992]]\n",
      "linear.bias:\n",
      " [0.43125194]\n",
      "\n",
      "Test loss: 0.432296, Train loss: 0.432333\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11083767]]\n",
      "linear.bias:\n",
      " [0.4307451]\n",
      "\n",
      "Test loss: 0.431786, Train loss: 0.431823\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11063561]]\n",
      "linear.bias:\n",
      " [0.43023825]\n",
      "\n",
      "Test loss: 0.431276, Train loss: 0.431313\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11043422]]\n",
      "linear.bias:\n",
      " [0.4297314]\n",
      "\n",
      "Test loss: 0.430767, Train loss: 0.430803\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11023343]]\n",
      "linear.bias:\n",
      " [0.42922455]\n",
      "\n",
      "Test loss: 0.430257, Train loss: 0.430293\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.11003315]]\n",
      "linear.bias:\n",
      " [0.4287177]\n",
      "\n",
      "Test loss: 0.429747, Train loss: 0.429783\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10983332]]\n",
      "linear.bias:\n",
      " [0.42821085]\n",
      "\n",
      "Test loss: 0.429238, Train loss: 0.429273\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10963389]]\n",
      "linear.bias:\n",
      " [0.427704]\n",
      "\n",
      "Test loss: 0.428728, Train loss: 0.428763\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10943478]]\n",
      "linear.bias:\n",
      " [0.42719716]\n",
      "\n",
      "Test loss: 0.428219, Train loss: 0.428253\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10923649]]\n",
      "linear.bias:\n",
      " [0.4266903]\n",
      "\n",
      "Test loss: 0.427709, Train loss: 0.427743\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10903892]]\n",
      "linear.bias:\n",
      " [0.42618346]\n",
      "\n",
      "Test loss: 0.427199, Train loss: 0.427233\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10884197]]\n",
      "linear.bias:\n",
      " [0.4256766]\n",
      "\n",
      "Test loss: 0.426690, Train loss: 0.426724\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10864557]]\n",
      "linear.bias:\n",
      " [0.42516977]\n",
      "\n",
      "Test loss: 0.426180, Train loss: 0.426214\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10844966]]\n",
      "linear.bias:\n",
      " [0.42466292]\n",
      "\n",
      "Test loss: 0.425671, Train loss: 0.425704\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10825416]]\n",
      "linear.bias:\n",
      " [0.42415607]\n",
      "\n",
      "Test loss: 0.425161, Train loss: 0.425194\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10805903]]\n",
      "linear.bias:\n",
      " [0.42364922]\n",
      "\n",
      "Test loss: 0.424652, Train loss: 0.424685\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10786421]]\n",
      "linear.bias:\n",
      " [0.42314237]\n",
      "\n",
      "Test loss: 0.424142, Train loss: 0.424175\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10766965]]\n",
      "linear.bias:\n",
      " [0.42263553]\n",
      "\n",
      "Test loss: 0.423633, Train loss: 0.423665\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10747532]]\n",
      "linear.bias:\n",
      " [0.42212868]\n",
      "\n",
      "Test loss: 0.423123, Train loss: 0.423156\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10728118]]\n",
      "linear.bias:\n",
      " [0.42162183]\n",
      "\n",
      "Test loss: 0.422614, Train loss: 0.422646\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10708719]]\n",
      "linear.bias:\n",
      " [0.421115]\n",
      "\n",
      "Test loss: 0.422104, Train loss: 0.422136\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10689333]]\n",
      "linear.bias:\n",
      " [0.4206082]\n",
      "\n",
      "Test loss: 0.421595, Train loss: 0.421626\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10670008]]\n",
      "linear.bias:\n",
      " [0.42010137]\n",
      "\n",
      "Test loss: 0.421085, Train loss: 0.421117\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10650735]]\n",
      "linear.bias:\n",
      " [0.41959456]\n",
      "\n",
      "Test loss: 0.420576, Train loss: 0.420607\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10631509]]\n",
      "linear.bias:\n",
      " [0.41908774]\n",
      "\n",
      "Test loss: 0.420066, Train loss: 0.420098\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10612323]]\n",
      "linear.bias:\n",
      " [0.41858092]\n",
      "\n",
      "Test loss: 0.419557, Train loss: 0.419588\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10593172]]\n",
      "linear.bias:\n",
      " [0.4180741]\n",
      "\n",
      "Test loss: 0.419047, Train loss: 0.419079\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10574051]]\n",
      "linear.bias:\n",
      " [0.41756728]\n",
      "\n",
      "Test loss: 0.418538, Train loss: 0.418569\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10554955]]\n",
      "linear.bias:\n",
      " [0.41706046]\n",
      "\n",
      "Test loss: 0.418028, Train loss: 0.418060\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10535881]]\n",
      "linear.bias:\n",
      " [0.41655365]\n",
      "\n",
      "Test loss: 0.417519, Train loss: 0.417550\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10516825]]\n",
      "linear.bias:\n",
      " [0.41604686]\n",
      "\n",
      "Test loss: 0.417010, Train loss: 0.417040\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10497784]]\n",
      "linear.bias:\n",
      " [0.41554007]\n",
      "\n",
      "Test loss: 0.416500, Train loss: 0.416531\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10478754]]\n",
      "linear.bias:\n",
      " [0.41503328]\n",
      "\n",
      "Test loss: 0.415991, Train loss: 0.416022\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10459734]]\n",
      "linear.bias:\n",
      " [0.4145265]\n",
      "\n",
      "Test loss: 0.415482, Train loss: 0.415512\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10440721]]\n",
      "linear.bias:\n",
      " [0.4140197]\n",
      "\n",
      "Test loss: 0.414972, Train loss: 0.415003\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10421713]]\n",
      "linear.bias:\n",
      " [0.41351295]\n",
      "\n",
      "Test loss: 0.414463, Train loss: 0.414493\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10402709]]\n",
      "linear.bias:\n",
      " [0.4130062]\n",
      "\n",
      "Test loss: 0.413953, Train loss: 0.413984\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10383757]]\n",
      "linear.bias:\n",
      " [0.41249943]\n",
      "\n",
      "Test loss: 0.413444, Train loss: 0.413474\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10364853]]\n",
      "linear.bias:\n",
      " [0.41199267]\n",
      "\n",
      "Test loss: 0.412935, Train loss: 0.412965\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10345989]]\n",
      "linear.bias:\n",
      " [0.4114859]\n",
      "\n",
      "Test loss: 0.412426, Train loss: 0.412456\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10327162]]\n",
      "linear.bias:\n",
      " [0.41097915]\n",
      "\n",
      "Test loss: 0.411916, Train loss: 0.411946\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10308365]]\n",
      "linear.bias:\n",
      " [0.4104724]\n",
      "\n",
      "Test loss: 0.411407, Train loss: 0.411437\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10289648]]\n",
      "linear.bias:\n",
      " [0.40996563]\n",
      "\n",
      "Test loss: 0.410898, Train loss: 0.410928\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10271002]]\n",
      "linear.bias:\n",
      " [0.40945888]\n",
      "\n",
      "Test loss: 0.410388, Train loss: 0.410418\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10252419]]\n",
      "linear.bias:\n",
      " [0.40895212]\n",
      "\n",
      "Test loss: 0.409879, Train loss: 0.409909\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1023389]]\n",
      "linear.bias:\n",
      " [0.40844536]\n",
      "\n",
      "Test loss: 0.409370, Train loss: 0.409400\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1021541]]\n",
      "linear.bias:\n",
      " [0.4079386]\n",
      "\n",
      "Test loss: 0.408861, Train loss: 0.408891\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10196971]]\n",
      "linear.bias:\n",
      " [0.40743184]\n",
      "\n",
      "Test loss: 0.408351, Train loss: 0.408382\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.1017862]]\n",
      "linear.bias:\n",
      " [0.40692508]\n",
      "\n",
      "Test loss: 0.407842, Train loss: 0.407872\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10160348]]\n",
      "linear.bias:\n",
      " [0.4064183]\n",
      "\n",
      "Test loss: 0.407333, Train loss: 0.407363\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10142144]]\n",
      "linear.bias:\n",
      " [0.4059115]\n",
      "\n",
      "Test loss: 0.406824, Train loss: 0.406854\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10124]]\n",
      "linear.bias:\n",
      " [0.40540472]\n",
      "\n",
      "Test loss: 0.406315, Train loss: 0.406345\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10105909]]\n",
      "linear.bias:\n",
      " [0.40489793]\n",
      "\n",
      "Test loss: 0.405805, Train loss: 0.405836\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10087866]]\n",
      "linear.bias:\n",
      " [0.40439114]\n",
      "\n",
      "Test loss: 0.405296, Train loss: 0.405327\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10069862]]\n",
      "linear.bias:\n",
      " [0.40388435]\n",
      "\n",
      "Test loss: 0.404787, Train loss: 0.404818\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10051894]]\n",
      "linear.bias:\n",
      " [0.40337756]\n",
      "\n",
      "Test loss: 0.404278, Train loss: 0.404309\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10033956]]\n",
      "linear.bias:\n",
      " [0.40287077]\n",
      "\n",
      "Test loss: 0.403769, Train loss: 0.403800\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.10016044]]\n",
      "linear.bias:\n",
      " [0.402364]\n",
      "\n",
      "Test loss: 0.403260, Train loss: 0.403291\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09998155]]\n",
      "linear.bias:\n",
      " [0.40185723]\n",
      "\n",
      "Test loss: 0.402750, Train loss: 0.402782\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09980284]]\n",
      "linear.bias:\n",
      " [0.40135047]\n",
      "\n",
      "Test loss: 0.402241, Train loss: 0.402273\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09962428]]\n",
      "linear.bias:\n",
      " [0.4008437]\n",
      "\n",
      "Test loss: 0.401732, Train loss: 0.401764\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09944584]]\n",
      "linear.bias:\n",
      " [0.40033695]\n",
      "\n",
      "Test loss: 0.401223, Train loss: 0.401255\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09926751]]\n",
      "linear.bias:\n",
      " [0.39983022]\n",
      "\n",
      "Test loss: 0.400714, Train loss: 0.400746\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09908926]]\n",
      "linear.bias:\n",
      " [0.3993235]\n",
      "\n",
      "Test loss: 0.400205, Train loss: 0.400237\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09891105]]\n",
      "linear.bias:\n",
      " [0.39881676]\n",
      "\n",
      "Test loss: 0.399696, Train loss: 0.399728\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09873289]]\n",
      "linear.bias:\n",
      " [0.39831004]\n",
      "\n",
      "Test loss: 0.399187, Train loss: 0.399219\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09855475]]\n",
      "linear.bias:\n",
      " [0.39780334]\n",
      "\n",
      "Test loss: 0.398678, Train loss: 0.398710\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0983766]]\n",
      "linear.bias:\n",
      " [0.39729664]\n",
      "\n",
      "Test loss: 0.398169, Train loss: 0.398201\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09819845]]\n",
      "linear.bias:\n",
      " [0.39678994]\n",
      "\n",
      "Test loss: 0.397660, Train loss: 0.397693\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09802028]]\n",
      "linear.bias:\n",
      " [0.39628327]\n",
      "\n",
      "Test loss: 0.397151, Train loss: 0.397184\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09784207]]\n",
      "linear.bias:\n",
      " [0.3957766]\n",
      "\n",
      "Test loss: 0.396642, Train loss: 0.396675\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09766383]]\n",
      "linear.bias:\n",
      " [0.39526993]\n",
      "\n",
      "Test loss: 0.396133, Train loss: 0.396166\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09748553]]\n",
      "linear.bias:\n",
      " [0.3947633]\n",
      "\n",
      "Test loss: 0.395624, Train loss: 0.395657\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09730717]]\n",
      "linear.bias:\n",
      " [0.39425665]\n",
      "\n",
      "Test loss: 0.395115, Train loss: 0.395148\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09712873]]\n",
      "linear.bias:\n",
      " [0.39375]\n",
      "\n",
      "Test loss: 0.394606, Train loss: 0.394639\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09695023]]\n",
      "linear.bias:\n",
      " [0.3932434]\n",
      "\n",
      "Test loss: 0.394097, Train loss: 0.394131\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09677164]]\n",
      "linear.bias:\n",
      " [0.3927368]\n",
      "\n",
      "Test loss: 0.393588, Train loss: 0.393622\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09659354]]\n",
      "linear.bias:\n",
      " [0.39223018]\n",
      "\n",
      "Test loss: 0.393079, Train loss: 0.393113\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09641586]]\n",
      "linear.bias:\n",
      " [0.39172357]\n",
      "\n",
      "Test loss: 0.392570, Train loss: 0.392604\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09623855]]\n",
      "linear.bias:\n",
      " [0.39121696]\n",
      "\n",
      "Test loss: 0.392061, Train loss: 0.392095\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09606156]]\n",
      "linear.bias:\n",
      " [0.39071035]\n",
      "\n",
      "Test loss: 0.391552, Train loss: 0.391587\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09588591]]\n",
      "linear.bias:\n",
      " [0.39020374]\n",
      "\n",
      "Test loss: 0.391043, Train loss: 0.391078\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09571147]]\n",
      "linear.bias:\n",
      " [0.3896971]\n",
      "\n",
      "Test loss: 0.390534, Train loss: 0.390569\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0955381]]\n",
      "linear.bias:\n",
      " [0.38919047]\n",
      "\n",
      "Test loss: 0.390025, Train loss: 0.390061\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09536568]]\n",
      "linear.bias:\n",
      " [0.38868383]\n",
      "\n",
      "Test loss: 0.389516, Train loss: 0.389552\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09519411]]\n",
      "linear.bias:\n",
      " [0.38817716]\n",
      "\n",
      "Test loss: 0.389008, Train loss: 0.389044\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09502328]]\n",
      "linear.bias:\n",
      " [0.3876705]\n",
      "\n",
      "Test loss: 0.388499, Train loss: 0.388535\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09485312]]\n",
      "linear.bias:\n",
      " [0.38716382]\n",
      "\n",
      "Test loss: 0.387990, Train loss: 0.388026\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09468354]]\n",
      "linear.bias:\n",
      " [0.38665715]\n",
      "\n",
      "Test loss: 0.387481, Train loss: 0.387518\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09451447]]\n",
      "linear.bias:\n",
      " [0.38615048]\n",
      "\n",
      "Test loss: 0.386972, Train loss: 0.387009\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09434585]]\n",
      "linear.bias:\n",
      " [0.3856438]\n",
      "\n",
      "Test loss: 0.386463, Train loss: 0.386501\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09417763]]\n",
      "linear.bias:\n",
      " [0.38513714]\n",
      "\n",
      "Test loss: 0.385955, Train loss: 0.385992\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09400975]]\n",
      "linear.bias:\n",
      " [0.38463047]\n",
      "\n",
      "Test loss: 0.385446, Train loss: 0.385484\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09384216]]\n",
      "linear.bias:\n",
      " [0.3841238]\n",
      "\n",
      "Test loss: 0.384937, Train loss: 0.384976\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09367483]]\n",
      "linear.bias:\n",
      " [0.38361713]\n",
      "\n",
      "Test loss: 0.384428, Train loss: 0.384467\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09350771]]\n",
      "linear.bias:\n",
      " [0.3831105]\n",
      "\n",
      "Test loss: 0.383919, Train loss: 0.383959\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09334131]]\n",
      "linear.bias:\n",
      " [0.38260382]\n",
      "\n",
      "Test loss: 0.383411, Train loss: 0.383450\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09317552]]\n",
      "linear.bias:\n",
      " [0.38209715]\n",
      "\n",
      "Test loss: 0.382902, Train loss: 0.382942\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09301028]]\n",
      "linear.bias:\n",
      " [0.3815905]\n",
      "\n",
      "Test loss: 0.382393, Train loss: 0.382433\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09284552]]\n",
      "linear.bias:\n",
      " [0.38108382]\n",
      "\n",
      "Test loss: 0.381884, Train loss: 0.381925\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09268119]]\n",
      "linear.bias:\n",
      " [0.38057715]\n",
      "\n",
      "Test loss: 0.381376, Train loss: 0.381417\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09251723]]\n",
      "linear.bias:\n",
      " [0.38007048]\n",
      "\n",
      "Test loss: 0.380867, Train loss: 0.380908\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09235358]]\n",
      "linear.bias:\n",
      " [0.3795638]\n",
      "\n",
      "Test loss: 0.380358, Train loss: 0.380400\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09219021]]\n",
      "linear.bias:\n",
      " [0.37905717]\n",
      "\n",
      "Test loss: 0.379850, Train loss: 0.379891\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09202708]]\n",
      "linear.bias:\n",
      " [0.37855053]\n",
      "\n",
      "Test loss: 0.379341, Train loss: 0.379383\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09186415]]\n",
      "linear.bias:\n",
      " [0.3780439]\n",
      "\n",
      "Test loss: 0.378832, Train loss: 0.378875\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0917014]]\n",
      "linear.bias:\n",
      " [0.37753725]\n",
      "\n",
      "Test loss: 0.378323, Train loss: 0.378367\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09153878]]\n",
      "linear.bias:\n",
      " [0.3770306]\n",
      "\n",
      "Test loss: 0.377815, Train loss: 0.377858\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09137627]]\n",
      "linear.bias:\n",
      " [0.376524]\n",
      "\n",
      "Test loss: 0.377306, Train loss: 0.377350\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09121386]]\n",
      "linear.bias:\n",
      " [0.3760174]\n",
      "\n",
      "Test loss: 0.376798, Train loss: 0.376842\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09105151]]\n",
      "linear.bias:\n",
      " [0.37551078]\n",
      "\n",
      "Test loss: 0.376289, Train loss: 0.376333\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09088922]]\n",
      "linear.bias:\n",
      " [0.37500417]\n",
      "\n",
      "Test loss: 0.375780, Train loss: 0.375825\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09072696]]\n",
      "linear.bias:\n",
      " [0.3744976]\n",
      "\n",
      "Test loss: 0.375272, Train loss: 0.375317\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0905653]]\n",
      "linear.bias:\n",
      " [0.373991]\n",
      "\n",
      "Test loss: 0.374763, Train loss: 0.374809\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09040418]]\n",
      "linear.bias:\n",
      " [0.37348443]\n",
      "\n",
      "Test loss: 0.374255, Train loss: 0.374300\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09024351]]\n",
      "linear.bias:\n",
      " [0.37297785]\n",
      "\n",
      "Test loss: 0.373746, Train loss: 0.373792\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.09008326]]\n",
      "linear.bias:\n",
      " [0.37247127]\n",
      "\n",
      "Test loss: 0.373237, Train loss: 0.373284\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08992336]]\n",
      "linear.bias:\n",
      " [0.3719647]\n",
      "\n",
      "Test loss: 0.372729, Train loss: 0.372776\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08976377]]\n",
      "linear.bias:\n",
      " [0.3714581]\n",
      "\n",
      "Test loss: 0.372220, Train loss: 0.372268\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08960444]]\n",
      "linear.bias:\n",
      " [0.37095153]\n",
      "\n",
      "Test loss: 0.371712, Train loss: 0.371760\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08944535]]\n",
      "linear.bias:\n",
      " [0.37044495]\n",
      "\n",
      "Test loss: 0.371203, Train loss: 0.371252\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08928644]]\n",
      "linear.bias:\n",
      " [0.36993837]\n",
      "\n",
      "Test loss: 0.370695, Train loss: 0.370743\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0891277]]\n",
      "linear.bias:\n",
      " [0.36943182]\n",
      "\n",
      "Test loss: 0.370186, Train loss: 0.370235\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08896909]]\n",
      "linear.bias:\n",
      " [0.36892527]\n",
      "\n",
      "Test loss: 0.369678, Train loss: 0.369727\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08881059]]\n",
      "linear.bias:\n",
      " [0.36841872]\n",
      "\n",
      "Test loss: 0.369169, Train loss: 0.369219\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08865218]]\n",
      "linear.bias:\n",
      " [0.36791217]\n",
      "\n",
      "Test loss: 0.368661, Train loss: 0.368711\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08849383]]\n",
      "linear.bias:\n",
      " [0.36740565]\n",
      "\n",
      "Test loss: 0.368152, Train loss: 0.368203\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08833554]]\n",
      "linear.bias:\n",
      " [0.36689913]\n",
      "\n",
      "Test loss: 0.367644, Train loss: 0.367695\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08817727]]\n",
      "linear.bias:\n",
      " [0.3663926]\n",
      "\n",
      "Test loss: 0.367135, Train loss: 0.367187\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08801902]]\n",
      "linear.bias:\n",
      " [0.3658861]\n",
      "\n",
      "Test loss: 0.366627, Train loss: 0.366679\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08786078]]\n",
      "linear.bias:\n",
      " [0.3653796]\n",
      "\n",
      "Test loss: 0.366118, Train loss: 0.366171\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08770253]]\n",
      "linear.bias:\n",
      " [0.3648731]\n",
      "\n",
      "Test loss: 0.365610, Train loss: 0.365663\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08754425]]\n",
      "linear.bias:\n",
      " [0.36436662]\n",
      "\n",
      "Test loss: 0.365101, Train loss: 0.365155\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08738595]]\n",
      "linear.bias:\n",
      " [0.36386016]\n",
      "\n",
      "Test loss: 0.364593, Train loss: 0.364647\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08722761]]\n",
      "linear.bias:\n",
      " [0.3633537]\n",
      "\n",
      "Test loss: 0.364085, Train loss: 0.364139\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0870698]]\n",
      "linear.bias:\n",
      " [0.36284724]\n",
      "\n",
      "Test loss: 0.363576, Train loss: 0.363631\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08691246]]\n",
      "linear.bias:\n",
      " [0.36234078]\n",
      "\n",
      "Test loss: 0.363068, Train loss: 0.363123\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08675553]]\n",
      "linear.bias:\n",
      " [0.36183432]\n",
      "\n",
      "Test loss: 0.362560, Train loss: 0.362615\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08659896]]\n",
      "linear.bias:\n",
      " [0.36132786]\n",
      "\n",
      "Test loss: 0.362051, Train loss: 0.362107\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08644269]]\n",
      "linear.bias:\n",
      " [0.3608214]\n",
      "\n",
      "Test loss: 0.361543, Train loss: 0.361599\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0862867]]\n",
      "linear.bias:\n",
      " [0.36031494]\n",
      "\n",
      "Test loss: 0.361034, Train loss: 0.361091\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08613093]]\n",
      "linear.bias:\n",
      " [0.35980847]\n",
      "\n",
      "Test loss: 0.360526, Train loss: 0.360583\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08597536]]\n",
      "linear.bias:\n",
      " [0.35930204]\n",
      "\n",
      "Test loss: 0.360018, Train loss: 0.360075\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08581996]]\n",
      "linear.bias:\n",
      " [0.3587956]\n",
      "\n",
      "Test loss: 0.359509, Train loss: 0.359567\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08566469]]\n",
      "linear.bias:\n",
      " [0.35828918]\n",
      "\n",
      "Test loss: 0.359001, Train loss: 0.359059\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08550953]]\n",
      "linear.bias:\n",
      " [0.35778275]\n",
      "\n",
      "Test loss: 0.358493, Train loss: 0.358552\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08535446]]\n",
      "linear.bias:\n",
      " [0.35727632]\n",
      "\n",
      "Test loss: 0.357985, Train loss: 0.358044\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08519947]]\n",
      "linear.bias:\n",
      " [0.35676992]\n",
      "\n",
      "Test loss: 0.357476, Train loss: 0.357536\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08504453]]\n",
      "linear.bias:\n",
      " [0.35626352]\n",
      "\n",
      "Test loss: 0.356968, Train loss: 0.357028\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08488961]]\n",
      "linear.bias:\n",
      " [0.35575712]\n",
      "\n",
      "Test loss: 0.356460, Train loss: 0.356520\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08473472]]\n",
      "linear.bias:\n",
      " [0.35525075]\n",
      "\n",
      "Test loss: 0.355952, Train loss: 0.356012\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08457984]]\n",
      "linear.bias:\n",
      " [0.35474437]\n",
      "\n",
      "Test loss: 0.355443, Train loss: 0.355505\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08442495]]\n",
      "linear.bias:\n",
      " [0.354238]\n",
      "\n",
      "Test loss: 0.354935, Train loss: 0.354997\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08427005]]\n",
      "linear.bias:\n",
      " [0.35373163]\n",
      "\n",
      "Test loss: 0.354427, Train loss: 0.354489\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08411511]]\n",
      "linear.bias:\n",
      " [0.3532253]\n",
      "\n",
      "Test loss: 0.353919, Train loss: 0.353981\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08396015]]\n",
      "linear.bias:\n",
      " [0.35271895]\n",
      "\n",
      "Test loss: 0.353410, Train loss: 0.353474\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08380514]]\n",
      "linear.bias:\n",
      " [0.3522126]\n",
      "\n",
      "Test loss: 0.352902, Train loss: 0.352966\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08365007]]\n",
      "linear.bias:\n",
      " [0.3517063]\n",
      "\n",
      "Test loss: 0.352394, Train loss: 0.352458\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08349496]]\n",
      "linear.bias:\n",
      " [0.35119998]\n",
      "\n",
      "Test loss: 0.351886, Train loss: 0.351950\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08333978]]\n",
      "linear.bias:\n",
      " [0.35069367]\n",
      "\n",
      "Test loss: 0.351378, Train loss: 0.351442\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08318453]]\n",
      "linear.bias:\n",
      " [0.3501874]\n",
      "\n",
      "Test loss: 0.350870, Train loss: 0.350935\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08302922]]\n",
      "linear.bias:\n",
      " [0.3496811]\n",
      "\n",
      "Test loss: 0.350361, Train loss: 0.350427\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08287383]]\n",
      "linear.bias:\n",
      " [0.34917483]\n",
      "\n",
      "Test loss: 0.349853, Train loss: 0.349919\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08271836]]\n",
      "linear.bias:\n",
      " [0.34866858]\n",
      "\n",
      "Test loss: 0.349345, Train loss: 0.349412\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0825628]]\n",
      "linear.bias:\n",
      " [0.34816232]\n",
      "\n",
      "Test loss: 0.348837, Train loss: 0.348904\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08240716]]\n",
      "linear.bias:\n",
      " [0.34765607]\n",
      "\n",
      "Test loss: 0.348329, Train loss: 0.348396\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08225143]]\n",
      "linear.bias:\n",
      " [0.34714985]\n",
      "\n",
      "Test loss: 0.347821, Train loss: 0.347889\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08209561]]\n",
      "linear.bias:\n",
      " [0.34664363]\n",
      "\n",
      "Test loss: 0.347313, Train loss: 0.347381\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08193969]]\n",
      "linear.bias:\n",
      " [0.3461374]\n",
      "\n",
      "Test loss: 0.346805, Train loss: 0.346873\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08178367]]\n",
      "linear.bias:\n",
      " [0.3456312]\n",
      "\n",
      "Test loss: 0.346297, Train loss: 0.346366\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08162756]]\n",
      "linear.bias:\n",
      " [0.34512502]\n",
      "\n",
      "Test loss: 0.345789, Train loss: 0.345858\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08147135]]\n",
      "linear.bias:\n",
      " [0.34461883]\n",
      "\n",
      "Test loss: 0.345281, Train loss: 0.345350\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08131504]]\n",
      "linear.bias:\n",
      " [0.34411266]\n",
      "\n",
      "Test loss: 0.344773, Train loss: 0.344843\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08115863]]\n",
      "linear.bias:\n",
      " [0.3436065]\n",
      "\n",
      "Test loss: 0.344265, Train loss: 0.344335\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08100212]]\n",
      "linear.bias:\n",
      " [0.34310034]\n",
      "\n",
      "Test loss: 0.343757, Train loss: 0.343827\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0808455]]\n",
      "linear.bias:\n",
      " [0.3425942]\n",
      "\n",
      "Test loss: 0.343249, Train loss: 0.343320\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08068877]]\n",
      "linear.bias:\n",
      " [0.34208807]\n",
      "\n",
      "Test loss: 0.342742, Train loss: 0.342812\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08053193]]\n",
      "linear.bias:\n",
      " [0.34158194]\n",
      "\n",
      "Test loss: 0.342234, Train loss: 0.342305\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08037499]]\n",
      "linear.bias:\n",
      " [0.34107584]\n",
      "\n",
      "Test loss: 0.341726, Train loss: 0.341797\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08021794]]\n",
      "linear.bias:\n",
      " [0.34056973]\n",
      "\n",
      "Test loss: 0.341218, Train loss: 0.341289\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.08006079]]\n",
      "linear.bias:\n",
      " [0.34006363]\n",
      "\n",
      "Test loss: 0.340710, Train loss: 0.340782\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07990352]]\n",
      "linear.bias:\n",
      " [0.33955756]\n",
      "\n",
      "Test loss: 0.340202, Train loss: 0.340274\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07974614]]\n",
      "linear.bias:\n",
      " [0.33905149]\n",
      "\n",
      "Test loss: 0.339694, Train loss: 0.339767\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07958866]]\n",
      "linear.bias:\n",
      " [0.3385454]\n",
      "\n",
      "Test loss: 0.339186, Train loss: 0.339259\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07943106]]\n",
      "linear.bias:\n",
      " [0.33803937]\n",
      "\n",
      "Test loss: 0.338678, Train loss: 0.338752\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07927336]]\n",
      "linear.bias:\n",
      " [0.33753332]\n",
      "\n",
      "Test loss: 0.338170, Train loss: 0.338244\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07911554]]\n",
      "linear.bias:\n",
      " [0.33702728]\n",
      "\n",
      "Test loss: 0.337663, Train loss: 0.337736\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07895761]]\n",
      "linear.bias:\n",
      " [0.33652127]\n",
      "\n",
      "Test loss: 0.337155, Train loss: 0.337229\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07879957]]\n",
      "linear.bias:\n",
      " [0.33601525]\n",
      "\n",
      "Test loss: 0.336647, Train loss: 0.336721\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07864141]]\n",
      "linear.bias:\n",
      " [0.33550924]\n",
      "\n",
      "Test loss: 0.336139, Train loss: 0.336214\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07848315]]\n",
      "linear.bias:\n",
      " [0.33500326]\n",
      "\n",
      "Test loss: 0.335631, Train loss: 0.335706\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07832477]]\n",
      "linear.bias:\n",
      " [0.33449727]\n",
      "\n",
      "Test loss: 0.335124, Train loss: 0.335199\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07816628]]\n",
      "linear.bias:\n",
      " [0.3339913]\n",
      "\n",
      "Test loss: 0.334616, Train loss: 0.334691\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07800768]]\n",
      "linear.bias:\n",
      " [0.33348534]\n",
      "\n",
      "Test loss: 0.334108, Train loss: 0.334184\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07784896]]\n",
      "linear.bias:\n",
      " [0.33297938]\n",
      "\n",
      "Test loss: 0.333600, Train loss: 0.333676\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07769013]]\n",
      "linear.bias:\n",
      " [0.33247343]\n",
      "\n",
      "Test loss: 0.333092, Train loss: 0.333169\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07753119]]\n",
      "linear.bias:\n",
      " [0.3319675]\n",
      "\n",
      "Test loss: 0.332585, Train loss: 0.332662\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07737213]]\n",
      "linear.bias:\n",
      " [0.33146158]\n",
      "\n",
      "Test loss: 0.332077, Train loss: 0.332154\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07721297]]\n",
      "linear.bias:\n",
      " [0.33095565]\n",
      "\n",
      "Test loss: 0.331569, Train loss: 0.331647\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07705368]]\n",
      "linear.bias:\n",
      " [0.33044976]\n",
      "\n",
      "Test loss: 0.331061, Train loss: 0.331139\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07689428]]\n",
      "linear.bias:\n",
      " [0.32994387]\n",
      "\n",
      "Test loss: 0.330554, Train loss: 0.330632\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07673477]]\n",
      "linear.bias:\n",
      " [0.32943797]\n",
      "\n",
      "Test loss: 0.330046, Train loss: 0.330124\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07657515]]\n",
      "linear.bias:\n",
      " [0.3289321]\n",
      "\n",
      "Test loss: 0.329538, Train loss: 0.329617\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07641541]]\n",
      "linear.bias:\n",
      " [0.32842624]\n",
      "\n",
      "Test loss: 0.329030, Train loss: 0.329110\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07625556]]\n",
      "linear.bias:\n",
      " [0.32792038]\n",
      "\n",
      "Test loss: 0.328523, Train loss: 0.328602\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0760956]]\n",
      "linear.bias:\n",
      " [0.3274145]\n",
      "\n",
      "Test loss: 0.328015, Train loss: 0.328095\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07593552]]\n",
      "linear.bias:\n",
      " [0.32690868]\n",
      "\n",
      "Test loss: 0.327507, Train loss: 0.327587\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07577533]]\n",
      "linear.bias:\n",
      " [0.32640284]\n",
      "\n",
      "Test loss: 0.326999, Train loss: 0.327080\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07561502]]\n",
      "linear.bias:\n",
      " [0.325897]\n",
      "\n",
      "Test loss: 0.326492, Train loss: 0.326573\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0754546]]\n",
      "linear.bias:\n",
      " [0.3253912]\n",
      "\n",
      "Test loss: 0.325984, Train loss: 0.326065\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07529407]]\n",
      "linear.bias:\n",
      " [0.3248854]\n",
      "\n",
      "Test loss: 0.325476, Train loss: 0.325558\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07513342]]\n",
      "linear.bias:\n",
      " [0.3243796]\n",
      "\n",
      "Test loss: 0.324969, Train loss: 0.325051\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07497266]]\n",
      "linear.bias:\n",
      " [0.32387382]\n",
      "\n",
      "Test loss: 0.324461, Train loss: 0.324543\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07481179]]\n",
      "linear.bias:\n",
      " [0.32336804]\n",
      "\n",
      "Test loss: 0.323954, Train loss: 0.324036\n",
      "Epoch [1000/5000], Loss: 0.323528\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07465079]]\n",
      "linear.bias:\n",
      " [0.32286227]\n",
      "\n",
      "Test loss: 0.323446, Train loss: 0.323528\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07448969]]\n",
      "linear.bias:\n",
      " [0.32235652]\n",
      "\n",
      "Test loss: 0.322938, Train loss: 0.323021\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07432847]]\n",
      "linear.bias:\n",
      " [0.32185078]\n",
      "\n",
      "Test loss: 0.322431, Train loss: 0.322514\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07416714]]\n",
      "linear.bias:\n",
      " [0.32134503]\n",
      "\n",
      "Test loss: 0.321923, Train loss: 0.322007\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07400569]]\n",
      "linear.bias:\n",
      " [0.32083932]\n",
      "\n",
      "Test loss: 0.321415, Train loss: 0.321499\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07384413]]\n",
      "linear.bias:\n",
      " [0.3203336]\n",
      "\n",
      "Test loss: 0.320908, Train loss: 0.320992\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07368246]]\n",
      "linear.bias:\n",
      " [0.31982788]\n",
      "\n",
      "Test loss: 0.320400, Train loss: 0.320485\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07352068]]\n",
      "linear.bias:\n",
      " [0.31932217]\n",
      "\n",
      "Test loss: 0.319892, Train loss: 0.319977\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07335877]]\n",
      "linear.bias:\n",
      " [0.31881648]\n",
      "\n",
      "Test loss: 0.319385, Train loss: 0.319470\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07319676]]\n",
      "linear.bias:\n",
      " [0.3183108]\n",
      "\n",
      "Test loss: 0.318877, Train loss: 0.318963\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07303463]]\n",
      "linear.bias:\n",
      " [0.3178051]\n",
      "\n",
      "Test loss: 0.318370, Train loss: 0.318456\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07287239]]\n",
      "linear.bias:\n",
      " [0.31729946]\n",
      "\n",
      "Test loss: 0.317862, Train loss: 0.317948\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07271003]]\n",
      "linear.bias:\n",
      " [0.3167938]\n",
      "\n",
      "Test loss: 0.317355, Train loss: 0.317441\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07254755]]\n",
      "linear.bias:\n",
      " [0.31628814]\n",
      "\n",
      "Test loss: 0.316847, Train loss: 0.316934\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07238497]]\n",
      "linear.bias:\n",
      " [0.31578252]\n",
      "\n",
      "Test loss: 0.316340, Train loss: 0.316426\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07222227]]\n",
      "linear.bias:\n",
      " [0.3152769]\n",
      "\n",
      "Test loss: 0.315832, Train loss: 0.315919\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07205945]]\n",
      "linear.bias:\n",
      " [0.31477126]\n",
      "\n",
      "Test loss: 0.315324, Train loss: 0.315412\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07189652]]\n",
      "linear.bias:\n",
      " [0.31426564]\n",
      "\n",
      "Test loss: 0.314817, Train loss: 0.314905\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07173348]]\n",
      "linear.bias:\n",
      " [0.31376004]\n",
      "\n",
      "Test loss: 0.314309, Train loss: 0.314398\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07157101]]\n",
      "linear.bias:\n",
      " [0.31325445]\n",
      "\n",
      "Test loss: 0.313802, Train loss: 0.313890\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07140906]]\n",
      "linear.bias:\n",
      " [0.31274885]\n",
      "\n",
      "Test loss: 0.313294, Train loss: 0.313383\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07124756]]\n",
      "linear.bias:\n",
      " [0.31224325]\n",
      "\n",
      "Test loss: 0.312787, Train loss: 0.312876\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07108644]]\n",
      "linear.bias:\n",
      " [0.31173766]\n",
      "\n",
      "Test loss: 0.312279, Train loss: 0.312369\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07092567]]\n",
      "linear.bias:\n",
      " [0.31123206]\n",
      "\n",
      "Test loss: 0.311772, Train loss: 0.311862\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07076519]]\n",
      "linear.bias:\n",
      " [0.31072646]\n",
      "\n",
      "Test loss: 0.311265, Train loss: 0.311355\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07060497]]\n",
      "linear.bias:\n",
      " [0.31022087]\n",
      "\n",
      "Test loss: 0.310757, Train loss: 0.310848\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07044497]]\n",
      "linear.bias:\n",
      " [0.30971527]\n",
      "\n",
      "Test loss: 0.310250, Train loss: 0.310341\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07028516]]\n",
      "linear.bias:\n",
      " [0.30920967]\n",
      "\n",
      "Test loss: 0.309742, Train loss: 0.309833\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.07012551]]\n",
      "linear.bias:\n",
      " [0.3087041]\n",
      "\n",
      "Test loss: 0.309235, Train loss: 0.309326\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06996599]]\n",
      "linear.bias:\n",
      " [0.30819854]\n",
      "\n",
      "Test loss: 0.308727, Train loss: 0.308819\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06980658]]\n",
      "linear.bias:\n",
      " [0.30769297]\n",
      "\n",
      "Test loss: 0.308220, Train loss: 0.308312\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06964725]]\n",
      "linear.bias:\n",
      " [0.3071874]\n",
      "\n",
      "Test loss: 0.307712, Train loss: 0.307805\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06948798]]\n",
      "linear.bias:\n",
      " [0.30668184]\n",
      "\n",
      "Test loss: 0.307205, Train loss: 0.307298\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06932877]]\n",
      "linear.bias:\n",
      " [0.30617628]\n",
      "\n",
      "Test loss: 0.306698, Train loss: 0.306791\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06916959]]\n",
      "linear.bias:\n",
      " [0.30567074]\n",
      "\n",
      "Test loss: 0.306190, Train loss: 0.306284\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06901043]]\n",
      "linear.bias:\n",
      " [0.3051652]\n",
      "\n",
      "Test loss: 0.305683, Train loss: 0.305777\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06885127]]\n",
      "linear.bias:\n",
      " [0.30465966]\n",
      "\n",
      "Test loss: 0.305176, Train loss: 0.305270\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06869211]]\n",
      "linear.bias:\n",
      " [0.30415413]\n",
      "\n",
      "Test loss: 0.304668, Train loss: 0.304763\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06853294]]\n",
      "linear.bias:\n",
      " [0.30364862]\n",
      "\n",
      "Test loss: 0.304161, Train loss: 0.304256\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06837373]]\n",
      "linear.bias:\n",
      " [0.3031431]\n",
      "\n",
      "Test loss: 0.303653, Train loss: 0.303749\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0682145]]\n",
      "linear.bias:\n",
      " [0.3026376]\n",
      "\n",
      "Test loss: 0.303146, Train loss: 0.303242\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06805522]]\n",
      "linear.bias:\n",
      " [0.3021321]\n",
      "\n",
      "Test loss: 0.302639, Train loss: 0.302735\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0678959]]\n",
      "linear.bias:\n",
      " [0.30162662]\n",
      "\n",
      "Test loss: 0.302131, Train loss: 0.302228\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06773651]]\n",
      "linear.bias:\n",
      " [0.30112115]\n",
      "\n",
      "Test loss: 0.301624, Train loss: 0.301721\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06757707]]\n",
      "linear.bias:\n",
      " [0.30061567]\n",
      "\n",
      "Test loss: 0.301117, Train loss: 0.301214\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06741756]]\n",
      "linear.bias:\n",
      " [0.3001102]\n",
      "\n",
      "Test loss: 0.300609, Train loss: 0.300707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06725799]]\n",
      "linear.bias:\n",
      " [0.29960474]\n",
      "\n",
      "Test loss: 0.300102, Train loss: 0.300200\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06709833]]\n",
      "linear.bias:\n",
      " [0.2990993]\n",
      "\n",
      "Test loss: 0.299595, Train loss: 0.299693\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06693861]]\n",
      "linear.bias:\n",
      " [0.29859385]\n",
      "\n",
      "Test loss: 0.299087, Train loss: 0.299186\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0667788]]\n",
      "linear.bias:\n",
      " [0.29808843]\n",
      "\n",
      "Test loss: 0.298580, Train loss: 0.298679\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06661891]]\n",
      "linear.bias:\n",
      " [0.297583]\n",
      "\n",
      "Test loss: 0.298073, Train loss: 0.298173\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06645893]]\n",
      "linear.bias:\n",
      " [0.2970776]\n",
      "\n",
      "Test loss: 0.297566, Train loss: 0.297666\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06629886]]\n",
      "linear.bias:\n",
      " [0.29657218]\n",
      "\n",
      "Test loss: 0.297058, Train loss: 0.297159\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06613871]]\n",
      "linear.bias:\n",
      " [0.2960668]\n",
      "\n",
      "Test loss: 0.296551, Train loss: 0.296652\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06597846]]\n",
      "linear.bias:\n",
      " [0.2955614]\n",
      "\n",
      "Test loss: 0.296044, Train loss: 0.296145\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06581812]]\n",
      "linear.bias:\n",
      " [0.29505602]\n",
      "\n",
      "Test loss: 0.295537, Train loss: 0.295638\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06565768]]\n",
      "linear.bias:\n",
      " [0.29455063]\n",
      "\n",
      "Test loss: 0.295029, Train loss: 0.295131\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06549714]]\n",
      "linear.bias:\n",
      " [0.29404527]\n",
      "\n",
      "Test loss: 0.294522, Train loss: 0.294624\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0653365]]\n",
      "linear.bias:\n",
      " [0.2935399]\n",
      "\n",
      "Test loss: 0.294015, Train loss: 0.294117\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06517577]]\n",
      "linear.bias:\n",
      " [0.29303455]\n",
      "\n",
      "Test loss: 0.293508, Train loss: 0.293611\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06501494]]\n",
      "linear.bias:\n",
      " [0.29252923]\n",
      "\n",
      "Test loss: 0.293000, Train loss: 0.293104\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.064854]]\n",
      "linear.bias:\n",
      " [0.2920239]\n",
      "\n",
      "Test loss: 0.292493, Train loss: 0.292597\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06469296]]\n",
      "linear.bias:\n",
      " [0.29151857]\n",
      "\n",
      "Test loss: 0.291986, Train loss: 0.292090\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06453182]]\n",
      "linear.bias:\n",
      " [0.29101324]\n",
      "\n",
      "Test loss: 0.291479, Train loss: 0.291583\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06437057]]\n",
      "linear.bias:\n",
      " [0.29050794]\n",
      "\n",
      "Test loss: 0.290972, Train loss: 0.291076\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06420922]]\n",
      "linear.bias:\n",
      " [0.29000264]\n",
      "\n",
      "Test loss: 0.290464, Train loss: 0.290569\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06404777]]\n",
      "linear.bias:\n",
      " [0.28949735]\n",
      "\n",
      "Test loss: 0.289957, Train loss: 0.290063\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0638862]]\n",
      "linear.bias:\n",
      " [0.28899205]\n",
      "\n",
      "Test loss: 0.289450, Train loss: 0.289556\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06372453]]\n",
      "linear.bias:\n",
      " [0.28848678]\n",
      "\n",
      "Test loss: 0.288943, Train loss: 0.289049\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06356276]]\n",
      "linear.bias:\n",
      " [0.2879815]\n",
      "\n",
      "Test loss: 0.288436, Train loss: 0.288542\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06340087]]\n",
      "linear.bias:\n",
      " [0.28747624]\n",
      "\n",
      "Test loss: 0.287929, Train loss: 0.288035\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06323888]]\n",
      "linear.bias:\n",
      " [0.286971]\n",
      "\n",
      "Test loss: 0.287421, Train loss: 0.287529\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06307679]]\n",
      "linear.bias:\n",
      " [0.28646576]\n",
      "\n",
      "Test loss: 0.286914, Train loss: 0.287022\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06291458]]\n",
      "linear.bias:\n",
      " [0.28596053]\n",
      "\n",
      "Test loss: 0.286407, Train loss: 0.286515\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06275227]]\n",
      "linear.bias:\n",
      " [0.2854553]\n",
      "\n",
      "Test loss: 0.285900, Train loss: 0.286008\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06258985]]\n",
      "linear.bias:\n",
      " [0.28495008]\n",
      "\n",
      "Test loss: 0.285393, Train loss: 0.285501\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06242732]]\n",
      "linear.bias:\n",
      " [0.28444487]\n",
      "\n",
      "Test loss: 0.284886, Train loss: 0.284995\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06226468]]\n",
      "linear.bias:\n",
      " [0.28393966]\n",
      "\n",
      "Test loss: 0.284379, Train loss: 0.284488\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06210193]]\n",
      "linear.bias:\n",
      " [0.28343445]\n",
      "\n",
      "Test loss: 0.283872, Train loss: 0.283981\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06193907]]\n",
      "linear.bias:\n",
      " [0.28292927]\n",
      "\n",
      "Test loss: 0.283365, Train loss: 0.283474\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06177611]]\n",
      "linear.bias:\n",
      " [0.2824241]\n",
      "\n",
      "Test loss: 0.282857, Train loss: 0.282968\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06161303]]\n",
      "linear.bias:\n",
      " [0.2819189]\n",
      "\n",
      "Test loss: 0.282350, Train loss: 0.282461\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06144984]]\n",
      "linear.bias:\n",
      " [0.28141376]\n",
      "\n",
      "Test loss: 0.281843, Train loss: 0.281954\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06128655]]\n",
      "linear.bias:\n",
      " [0.2809086]\n",
      "\n",
      "Test loss: 0.281337, Train loss: 0.281447\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06112315]]\n",
      "linear.bias:\n",
      " [0.28040347]\n",
      "\n",
      "Test loss: 0.280830, Train loss: 0.280941\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06095963]]\n",
      "linear.bias:\n",
      " [0.27989832]\n",
      "\n",
      "Test loss: 0.280323, Train loss: 0.280434\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06079601]]\n",
      "linear.bias:\n",
      " [0.2793932]\n",
      "\n",
      "Test loss: 0.279816, Train loss: 0.279927\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06063228]]\n",
      "linear.bias:\n",
      " [0.27888808]\n",
      "\n",
      "Test loss: 0.279309, Train loss: 0.279421\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06046844]]\n",
      "linear.bias:\n",
      " [0.27838296]\n",
      "\n",
      "Test loss: 0.278802, Train loss: 0.278914\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06030448]]\n",
      "linear.bias:\n",
      " [0.27787784]\n",
      "\n",
      "Test loss: 0.278295, Train loss: 0.278407\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.06014042]]\n",
      "linear.bias:\n",
      " [0.27737275]\n",
      "\n",
      "Test loss: 0.277788, Train loss: 0.277900\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05997625]]\n",
      "linear.bias:\n",
      " [0.27686766]\n",
      "\n",
      "Test loss: 0.277281, Train loss: 0.277394\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05981196]]\n",
      "linear.bias:\n",
      " [0.27636257]\n",
      "\n",
      "Test loss: 0.276774, Train loss: 0.276887\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05964757]]\n",
      "linear.bias:\n",
      " [0.27585748]\n",
      "\n",
      "Test loss: 0.276268, Train loss: 0.276380\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05948307]]\n",
      "linear.bias:\n",
      " [0.27535242]\n",
      "\n",
      "Test loss: 0.275761, Train loss: 0.275874\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05931846]]\n",
      "linear.bias:\n",
      " [0.27484736]\n",
      "\n",
      "Test loss: 0.275254, Train loss: 0.275367\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05915374]]\n",
      "linear.bias:\n",
      " [0.2743423]\n",
      "\n",
      "Test loss: 0.274747, Train loss: 0.274860\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0589889]]\n",
      "linear.bias:\n",
      " [0.27383724]\n",
      "\n",
      "Test loss: 0.274240, Train loss: 0.274354\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05882396]]\n",
      "linear.bias:\n",
      " [0.2733322]\n",
      "\n",
      "Test loss: 0.273733, Train loss: 0.273847\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05865891]]\n",
      "linear.bias:\n",
      " [0.27282718]\n",
      "\n",
      "Test loss: 0.273226, Train loss: 0.273340\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05849374]]\n",
      "linear.bias:\n",
      " [0.27232215]\n",
      "\n",
      "Test loss: 0.272720, Train loss: 0.272834\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05832847]]\n",
      "linear.bias:\n",
      " [0.27181712]\n",
      "\n",
      "Test loss: 0.272213, Train loss: 0.272327\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0581638]]\n",
      "linear.bias:\n",
      " [0.2713121]\n",
      "\n",
      "Test loss: 0.271706, Train loss: 0.271821\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05799967]]\n",
      "linear.bias:\n",
      " [0.27080706]\n",
      "\n",
      "Test loss: 0.271199, Train loss: 0.271314\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05783676]]\n",
      "linear.bias:\n",
      " [0.27030203]\n",
      "\n",
      "Test loss: 0.270692, Train loss: 0.270807\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05767493]]\n",
      "linear.bias:\n",
      " [0.269797]\n",
      "\n",
      "Test loss: 0.270186, Train loss: 0.270301\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05751406]]\n",
      "linear.bias:\n",
      " [0.26929194]\n",
      "\n",
      "Test loss: 0.269679, Train loss: 0.269795\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05735406]]\n",
      "linear.bias:\n",
      " [0.26878688]\n",
      "\n",
      "Test loss: 0.269172, Train loss: 0.269288\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05719554]]\n",
      "linear.bias:\n",
      " [0.26828182]\n",
      "\n",
      "Test loss: 0.268665, Train loss: 0.268782\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05703836]]\n",
      "linear.bias:\n",
      " [0.26777673]\n",
      "\n",
      "Test loss: 0.268159, Train loss: 0.268275\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05688237]]\n",
      "linear.bias:\n",
      " [0.26727164]\n",
      "\n",
      "Test loss: 0.267652, Train loss: 0.267769\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05672745]]\n",
      "linear.bias:\n",
      " [0.26676655]\n",
      "\n",
      "Test loss: 0.267145, Train loss: 0.267262\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05657347]]\n",
      "linear.bias:\n",
      " [0.26626143]\n",
      "\n",
      "Test loss: 0.266639, Train loss: 0.266756\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05642034]]\n",
      "linear.bias:\n",
      " [0.2657563]\n",
      "\n",
      "Test loss: 0.266132, Train loss: 0.266250\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05626795]]\n",
      "linear.bias:\n",
      " [0.2652512]\n",
      "\n",
      "Test loss: 0.265626, Train loss: 0.265743\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05611624]]\n",
      "linear.bias:\n",
      " [0.26474607]\n",
      "\n",
      "Test loss: 0.265119, Train loss: 0.265237\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05596511]]\n",
      "linear.bias:\n",
      " [0.26424095]\n",
      "\n",
      "Test loss: 0.264612, Train loss: 0.264731\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05581451]]\n",
      "linear.bias:\n",
      " [0.26373583]\n",
      "\n",
      "Test loss: 0.264106, Train loss: 0.264224\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05566437]]\n",
      "linear.bias:\n",
      " [0.2632307]\n",
      "\n",
      "Test loss: 0.263599, Train loss: 0.263718\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05551463]]\n",
      "linear.bias:\n",
      " [0.2627256]\n",
      "\n",
      "Test loss: 0.263093, Train loss: 0.263212\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05536525]]\n",
      "linear.bias:\n",
      " [0.26222047]\n",
      "\n",
      "Test loss: 0.262586, Train loss: 0.262705\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05521617]]\n",
      "linear.bias:\n",
      " [0.26171535]\n",
      "\n",
      "Test loss: 0.262080, Train loss: 0.262199\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05506736]]\n",
      "linear.bias:\n",
      " [0.26121023]\n",
      "\n",
      "Test loss: 0.261573, Train loss: 0.261693\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05491959]]\n",
      "linear.bias:\n",
      " [0.2607051]\n",
      "\n",
      "Test loss: 0.261067, Train loss: 0.261187\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05477272]]\n",
      "linear.bias:\n",
      " [0.2602]\n",
      "\n",
      "Test loss: 0.260560, Train loss: 0.260680\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05462668]]\n",
      "linear.bias:\n",
      " [0.25969484]\n",
      "\n",
      "Test loss: 0.260054, Train loss: 0.260174\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05448136]]\n",
      "linear.bias:\n",
      " [0.2591897]\n",
      "\n",
      "Test loss: 0.259547, Train loss: 0.259668\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05433669]]\n",
      "linear.bias:\n",
      " [0.25868455]\n",
      "\n",
      "Test loss: 0.259041, Train loss: 0.259162\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05419259]]\n",
      "linear.bias:\n",
      " [0.2581794]\n",
      "\n",
      "Test loss: 0.258534, Train loss: 0.258656\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.054049]]\n",
      "linear.bias:\n",
      " [0.25767425]\n",
      "\n",
      "Test loss: 0.258028, Train loss: 0.258149\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05390584]]\n",
      "linear.bias:\n",
      " [0.2571691]\n",
      "\n",
      "Test loss: 0.257521, Train loss: 0.257643\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05376309]]\n",
      "linear.bias:\n",
      " [0.25666395]\n",
      "\n",
      "Test loss: 0.257015, Train loss: 0.257137\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05362067]]\n",
      "linear.bias:\n",
      " [0.2561588]\n",
      "\n",
      "Test loss: 0.256508, Train loss: 0.256631\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05347856]]\n",
      "linear.bias:\n",
      " [0.25565365]\n",
      "\n",
      "Test loss: 0.256002, Train loss: 0.256125\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05333671]]\n",
      "linear.bias:\n",
      " [0.2551485]\n",
      "\n",
      "Test loss: 0.255495, Train loss: 0.255619\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05319508]]\n",
      "linear.bias:\n",
      " [0.25464338]\n",
      "\n",
      "Test loss: 0.254989, Train loss: 0.255112\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05305364]]\n",
      "linear.bias:\n",
      " [0.25413826]\n",
      "\n",
      "Test loss: 0.254482, Train loss: 0.254606\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05291319]]\n",
      "linear.bias:\n",
      " [0.25363314]\n",
      "\n",
      "Test loss: 0.253976, Train loss: 0.254100\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05277361]]\n",
      "linear.bias:\n",
      " [0.253128]\n",
      "\n",
      "Test loss: 0.253469, Train loss: 0.253594\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0526348]]\n",
      "linear.bias:\n",
      " [0.25262284]\n",
      "\n",
      "Test loss: 0.252963, Train loss: 0.253088\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05249668]]\n",
      "linear.bias:\n",
      " [0.2521177]\n",
      "\n",
      "Test loss: 0.252456, Train loss: 0.252582\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05235918]]\n",
      "linear.bias:\n",
      " [0.25161254]\n",
      "\n",
      "Test loss: 0.251950, Train loss: 0.252076\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05222222]]\n",
      "linear.bias:\n",
      " [0.2511074]\n",
      "\n",
      "Test loss: 0.251444, Train loss: 0.251570\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05208574]]\n",
      "linear.bias:\n",
      " [0.25060225]\n",
      "\n",
      "Test loss: 0.250937, Train loss: 0.251064\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05194968]]\n",
      "linear.bias:\n",
      " [0.2500971]\n",
      "\n",
      "Test loss: 0.250431, Train loss: 0.250558\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.051814]]\n",
      "linear.bias:\n",
      " [0.24959195]\n",
      "\n",
      "Test loss: 0.249924, Train loss: 0.250052\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05167942]]\n",
      "linear.bias:\n",
      " [0.2490868]\n",
      "\n",
      "Test loss: 0.249418, Train loss: 0.249546\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05154584]]\n",
      "linear.bias:\n",
      " [0.24858163]\n",
      "\n",
      "Test loss: 0.248911, Train loss: 0.249040\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05141313]]\n",
      "linear.bias:\n",
      " [0.24807647]\n",
      "\n",
      "Test loss: 0.248405, Train loss: 0.248534\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05128121]]\n",
      "linear.bias:\n",
      " [0.2475713]\n",
      "\n",
      "Test loss: 0.247899, Train loss: 0.248028\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05114999]]\n",
      "linear.bias:\n",
      " [0.24706614]\n",
      "\n",
      "Test loss: 0.247392, Train loss: 0.247522\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05101939]]\n",
      "linear.bias:\n",
      " [0.24656098]\n",
      "\n",
      "Test loss: 0.246886, Train loss: 0.247016\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05088934]]\n",
      "linear.bias:\n",
      " [0.24605581]\n",
      "\n",
      "Test loss: 0.246380, Train loss: 0.246510\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05075978]]\n",
      "linear.bias:\n",
      " [0.24555065]\n",
      "\n",
      "Test loss: 0.245873, Train loss: 0.246004\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05063065]]\n",
      "linear.bias:\n",
      " [0.24504548]\n",
      "\n",
      "Test loss: 0.245367, Train loss: 0.245499\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0505019]]\n",
      "linear.bias:\n",
      " [0.24454032]\n",
      "\n",
      "Test loss: 0.244861, Train loss: 0.244993\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05037348]]\n",
      "linear.bias:\n",
      " [0.24403515]\n",
      "\n",
      "Test loss: 0.244354, Train loss: 0.244487\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05024536]]\n",
      "linear.bias:\n",
      " [0.24352999]\n",
      "\n",
      "Test loss: 0.243848, Train loss: 0.243981\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.05011749]]\n",
      "linear.bias:\n",
      " [0.24302483]\n",
      "\n",
      "Test loss: 0.243341, Train loss: 0.243475\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04998983]]\n",
      "linear.bias:\n",
      " [0.24251966]\n",
      "\n",
      "Test loss: 0.242835, Train loss: 0.242969\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04986237]]\n",
      "linear.bias:\n",
      " [0.24201451]\n",
      "\n",
      "Test loss: 0.242329, Train loss: 0.242463\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04973508]]\n",
      "linear.bias:\n",
      " [0.24150936]\n",
      "\n",
      "Test loss: 0.241823, Train loss: 0.241957\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04960792]]\n",
      "linear.bias:\n",
      " [0.24100421]\n",
      "\n",
      "Test loss: 0.241316, Train loss: 0.241452\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04948087]]\n",
      "linear.bias:\n",
      " [0.24049908]\n",
      "\n",
      "Test loss: 0.240810, Train loss: 0.240946\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04935393]]\n",
      "linear.bias:\n",
      " [0.23999394]\n",
      "\n",
      "Test loss: 0.240304, Train loss: 0.240440\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04922706]]\n",
      "linear.bias:\n",
      " [0.23948881]\n",
      "\n",
      "Test loss: 0.239797, Train loss: 0.239934\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04910025]]\n",
      "linear.bias:\n",
      " [0.23898369]\n",
      "\n",
      "Test loss: 0.239291, Train loss: 0.239428\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04897349]]\n",
      "linear.bias:\n",
      " [0.23847857]\n",
      "\n",
      "Test loss: 0.238785, Train loss: 0.238922\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04884676]]\n",
      "linear.bias:\n",
      " [0.23797347]\n",
      "\n",
      "Test loss: 0.238278, Train loss: 0.238417\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04872006]]\n",
      "linear.bias:\n",
      " [0.23746836]\n",
      "\n",
      "Test loss: 0.237772, Train loss: 0.237911\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04859336]]\n",
      "linear.bias:\n",
      " [0.23696326]\n",
      "\n",
      "Test loss: 0.237266, Train loss: 0.237405\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04846667]]\n",
      "linear.bias:\n",
      " [0.23645817]\n",
      "\n",
      "Test loss: 0.236760, Train loss: 0.236899\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04833997]]\n",
      "linear.bias:\n",
      " [0.23595308]\n",
      "\n",
      "Test loss: 0.236253, Train loss: 0.236394\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04821325]]\n",
      "linear.bias:\n",
      " [0.235448]\n",
      "\n",
      "Test loss: 0.235747, Train loss: 0.235888\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04808652]]\n",
      "linear.bias:\n",
      " [0.23494293]\n",
      "\n",
      "Test loss: 0.235241, Train loss: 0.235382\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04795975]]\n",
      "linear.bias:\n",
      " [0.23443787]\n",
      "\n",
      "Test loss: 0.234735, Train loss: 0.234876\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04783295]]\n",
      "linear.bias:\n",
      " [0.23393281]\n",
      "\n",
      "Test loss: 0.234228, Train loss: 0.234370\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04770612]]\n",
      "linear.bias:\n",
      " [0.23342776]\n",
      "\n",
      "Test loss: 0.233722, Train loss: 0.233865\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04757923]]\n",
      "linear.bias:\n",
      " [0.23292272]\n",
      "\n",
      "Test loss: 0.233216, Train loss: 0.233359\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0474523]]\n",
      "linear.bias:\n",
      " [0.23241767]\n",
      "\n",
      "Test loss: 0.232710, Train loss: 0.232853\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04732532]]\n",
      "linear.bias:\n",
      " [0.23191264]\n",
      "\n",
      "Test loss: 0.232204, Train loss: 0.232347\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04719829]]\n",
      "linear.bias:\n",
      " [0.23140761]\n",
      "\n",
      "Test loss: 0.231697, Train loss: 0.231842\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04707119]]\n",
      "linear.bias:\n",
      " [0.2309026]\n",
      "\n",
      "Test loss: 0.231191, Train loss: 0.231336\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04694404]]\n",
      "linear.bias:\n",
      " [0.23039758]\n",
      "\n",
      "Test loss: 0.230685, Train loss: 0.230830\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04681682]]\n",
      "linear.bias:\n",
      " [0.22989258]\n",
      "\n",
      "Test loss: 0.230179, Train loss: 0.230325\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04668953]]\n",
      "linear.bias:\n",
      " [0.22938758]\n",
      "\n",
      "Test loss: 0.229673, Train loss: 0.229819\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04656218]]\n",
      "linear.bias:\n",
      " [0.2288826]\n",
      "\n",
      "Test loss: 0.229167, Train loss: 0.229313\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04643476]]\n",
      "linear.bias:\n",
      " [0.22837761]\n",
      "\n",
      "Test loss: 0.228660, Train loss: 0.228808\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04630727]]\n",
      "linear.bias:\n",
      " [0.22787264]\n",
      "\n",
      "Test loss: 0.228154, Train loss: 0.228302\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0461797]]\n",
      "linear.bias:\n",
      " [0.22736767]\n",
      "\n",
      "Test loss: 0.227648, Train loss: 0.227796\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04605206]]\n",
      "linear.bias:\n",
      " [0.22686271]\n",
      "\n",
      "Test loss: 0.227142, Train loss: 0.227290\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04592435]]\n",
      "linear.bias:\n",
      " [0.22635776]\n",
      "\n",
      "Test loss: 0.226636, Train loss: 0.226785\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04579656]]\n",
      "linear.bias:\n",
      " [0.22585282]\n",
      "\n",
      "Test loss: 0.226130, Train loss: 0.226279\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0456687]]\n",
      "linear.bias:\n",
      " [0.22534788]\n",
      "\n",
      "Test loss: 0.225624, Train loss: 0.225774\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04554075]]\n",
      "linear.bias:\n",
      " [0.22484295]\n",
      "\n",
      "Test loss: 0.225118, Train loss: 0.225268\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04541273]]\n",
      "linear.bias:\n",
      " [0.22433802]\n",
      "\n",
      "Test loss: 0.224611, Train loss: 0.224762\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04528463]]\n",
      "linear.bias:\n",
      " [0.22383311]\n",
      "\n",
      "Test loss: 0.224105, Train loss: 0.224257\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04515644]]\n",
      "linear.bias:\n",
      " [0.2233282]\n",
      "\n",
      "Test loss: 0.223599, Train loss: 0.223751\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04502903]]\n",
      "linear.bias:\n",
      " [0.22282329]\n",
      "\n",
      "Test loss: 0.223093, Train loss: 0.223245\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04490231]]\n",
      "linear.bias:\n",
      " [0.22231838]\n",
      "\n",
      "Test loss: 0.222587, Train loss: 0.222740\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04477619]]\n",
      "linear.bias:\n",
      " [0.22181347]\n",
      "\n",
      "Test loss: 0.222081, Train loss: 0.222234\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04465062]]\n",
      "linear.bias:\n",
      " [0.22130856]\n",
      "\n",
      "Test loss: 0.221575, Train loss: 0.221729\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04452553]]\n",
      "linear.bias:\n",
      " [0.22080365]\n",
      "\n",
      "Test loss: 0.221069, Train loss: 0.221223\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04440086]]\n",
      "linear.bias:\n",
      " [0.22029874]\n",
      "\n",
      "Test loss: 0.220563, Train loss: 0.220718\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04427657]]\n",
      "linear.bias:\n",
      " [0.21979383]\n",
      "\n",
      "Test loss: 0.220057, Train loss: 0.220212\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04415261]]\n",
      "linear.bias:\n",
      " [0.21928892]\n",
      "\n",
      "Test loss: 0.219551, Train loss: 0.219706\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04402893]]\n",
      "linear.bias:\n",
      " [0.218784]\n",
      "\n",
      "Test loss: 0.219045, Train loss: 0.219201\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0439055]]\n",
      "linear.bias:\n",
      " [0.2182791]\n",
      "\n",
      "Test loss: 0.218539, Train loss: 0.218695\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0437823]]\n",
      "linear.bias:\n",
      " [0.21777418]\n",
      "\n",
      "Test loss: 0.218033, Train loss: 0.218190\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04365928]]\n",
      "linear.bias:\n",
      " [0.21726929]\n",
      "\n",
      "Test loss: 0.217527, Train loss: 0.217684\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04353642]]\n",
      "linear.bias:\n",
      " [0.21676439]\n",
      "\n",
      "Test loss: 0.217021, Train loss: 0.217179\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04341371]]\n",
      "linear.bias:\n",
      " [0.2162595]\n",
      "\n",
      "Test loss: 0.216515, Train loss: 0.216673\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0432911]]\n",
      "linear.bias:\n",
      " [0.21575461]\n",
      "\n",
      "Test loss: 0.216009, Train loss: 0.216168\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0431686]]\n",
      "linear.bias:\n",
      " [0.21524973]\n",
      "\n",
      "Test loss: 0.215503, Train loss: 0.215662\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04304617]]\n",
      "linear.bias:\n",
      " [0.21474485]\n",
      "\n",
      "Test loss: 0.214997, Train loss: 0.215157\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0429238]]\n",
      "linear.bias:\n",
      " [0.21423998]\n",
      "\n",
      "Test loss: 0.214491, Train loss: 0.214651\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04280149]]\n",
      "linear.bias:\n",
      " [0.21373512]\n",
      "\n",
      "Test loss: 0.213985, Train loss: 0.214146\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04267921]]\n",
      "linear.bias:\n",
      " [0.21323025]\n",
      "\n",
      "Test loss: 0.213479, Train loss: 0.213640\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04255695]]\n",
      "linear.bias:\n",
      " [0.2127254]\n",
      "\n",
      "Test loss: 0.212973, Train loss: 0.213135\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0424347]]\n",
      "linear.bias:\n",
      " [0.21222055]\n",
      "\n",
      "Test loss: 0.212468, Train loss: 0.212630\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04231247]]\n",
      "linear.bias:\n",
      " [0.21171571]\n",
      "\n",
      "Test loss: 0.211962, Train loss: 0.212124\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04219022]]\n",
      "linear.bias:\n",
      " [0.21121088]\n",
      "\n",
      "Test loss: 0.211456, Train loss: 0.211619\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04206796]]\n",
      "linear.bias:\n",
      " [0.21070604]\n",
      "\n",
      "Test loss: 0.210950, Train loss: 0.211113\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04194569]]\n",
      "linear.bias:\n",
      " [0.21020122]\n",
      "\n",
      "Test loss: 0.210444, Train loss: 0.210608\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04182339]]\n",
      "linear.bias:\n",
      " [0.2096964]\n",
      "\n",
      "Test loss: 0.209938, Train loss: 0.210102\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04170106]]\n",
      "linear.bias:\n",
      " [0.20919159]\n",
      "\n",
      "Test loss: 0.209432, Train loss: 0.209597\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04157869]]\n",
      "linear.bias:\n",
      " [0.20868678]\n",
      "\n",
      "Test loss: 0.208926, Train loss: 0.209092\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04145629]]\n",
      "linear.bias:\n",
      " [0.20818198]\n",
      "\n",
      "Test loss: 0.208420, Train loss: 0.208586\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04133384]]\n",
      "linear.bias:\n",
      " [0.20767719]\n",
      "\n",
      "Test loss: 0.207915, Train loss: 0.208081\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04121134]]\n",
      "linear.bias:\n",
      " [0.2071724]\n",
      "\n",
      "Test loss: 0.207409, Train loss: 0.207575\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04108879]]\n",
      "linear.bias:\n",
      " [0.20666762]\n",
      "\n",
      "Test loss: 0.206903, Train loss: 0.207070\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04096618]]\n",
      "linear.bias:\n",
      " [0.20616284]\n",
      "\n",
      "Test loss: 0.206397, Train loss: 0.206565\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04084352]]\n",
      "linear.bias:\n",
      " [0.20565808]\n",
      "\n",
      "Test loss: 0.205891, Train loss: 0.206059\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0407208]]\n",
      "linear.bias:\n",
      " [0.20515332]\n",
      "\n",
      "Test loss: 0.205385, Train loss: 0.205554\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04059802]]\n",
      "linear.bias:\n",
      " [0.20464857]\n",
      "\n",
      "Test loss: 0.204880, Train loss: 0.205048\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04047517]]\n",
      "linear.bias:\n",
      " [0.20414382]\n",
      "\n",
      "Test loss: 0.204374, Train loss: 0.204543\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04035226]]\n",
      "linear.bias:\n",
      " [0.20363908]\n",
      "\n",
      "Test loss: 0.203868, Train loss: 0.204038\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04022928]]\n",
      "linear.bias:\n",
      " [0.20313434]\n",
      "\n",
      "Test loss: 0.203362, Train loss: 0.203532\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.04010623]]\n",
      "linear.bias:\n",
      " [0.20262961]\n",
      "\n",
      "Test loss: 0.202856, Train loss: 0.203027\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03998312]]\n",
      "linear.bias:\n",
      " [0.2021249]\n",
      "\n",
      "Test loss: 0.202351, Train loss: 0.202522\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03985993]]\n",
      "linear.bias:\n",
      " [0.20162018]\n",
      "\n",
      "Test loss: 0.201845, Train loss: 0.202016\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03973667]]\n",
      "linear.bias:\n",
      " [0.20111547]\n",
      "\n",
      "Test loss: 0.201339, Train loss: 0.201511\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03961334]]\n",
      "linear.bias:\n",
      " [0.20061077]\n",
      "\n",
      "Test loss: 0.200833, Train loss: 0.201006\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03948993]]\n",
      "linear.bias:\n",
      " [0.20010608]\n",
      "\n",
      "Test loss: 0.200328, Train loss: 0.200500\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03936645]]\n",
      "linear.bias:\n",
      " [0.1996014]\n",
      "\n",
      "Test loss: 0.199822, Train loss: 0.199995\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03924289]]\n",
      "linear.bias:\n",
      " [0.19909671]\n",
      "\n",
      "Test loss: 0.199316, Train loss: 0.199490\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03911926]]\n",
      "linear.bias:\n",
      " [0.19859204]\n",
      "\n",
      "Test loss: 0.198810, Train loss: 0.198984\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03899555]]\n",
      "linear.bias:\n",
      " [0.19808736]\n",
      "\n",
      "Test loss: 0.198305, Train loss: 0.198479\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03887177]]\n",
      "linear.bias:\n",
      " [0.1975827]\n",
      "\n",
      "Test loss: 0.197799, Train loss: 0.197974\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0387479]]\n",
      "linear.bias:\n",
      " [0.19707805]\n",
      "\n",
      "Test loss: 0.197293, Train loss: 0.197469\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03862396]]\n",
      "linear.bias:\n",
      " [0.1965734]\n",
      "\n",
      "Test loss: 0.196787, Train loss: 0.196963\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03849994]]\n",
      "linear.bias:\n",
      " [0.19606876]\n",
      "\n",
      "Test loss: 0.196282, Train loss: 0.196458\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03837585]]\n",
      "linear.bias:\n",
      " [0.19556412]\n",
      "\n",
      "Test loss: 0.195776, Train loss: 0.195953\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03825167]]\n",
      "linear.bias:\n",
      " [0.1950595]\n",
      "\n",
      "Test loss: 0.195270, Train loss: 0.195447\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03812742]]\n",
      "linear.bias:\n",
      " [0.19455487]\n",
      "\n",
      "Test loss: 0.194764, Train loss: 0.194942\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03800308]]\n",
      "linear.bias:\n",
      " [0.19405025]\n",
      "\n",
      "Test loss: 0.194259, Train loss: 0.194437\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03787867]]\n",
      "linear.bias:\n",
      " [0.19354564]\n",
      "\n",
      "Test loss: 0.193753, Train loss: 0.193932\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03775417]]\n",
      "linear.bias:\n",
      " [0.19304104]\n",
      "\n",
      "Test loss: 0.193247, Train loss: 0.193426\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0376296]]\n",
      "linear.bias:\n",
      " [0.19253644]\n",
      "\n",
      "Test loss: 0.192742, Train loss: 0.192921\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03750494]]\n",
      "linear.bias:\n",
      " [0.19203186]\n",
      "\n",
      "Test loss: 0.192236, Train loss: 0.192416\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03738021]]\n",
      "linear.bias:\n",
      " [0.19152728]\n",
      "\n",
      "Test loss: 0.191730, Train loss: 0.191911\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0372554]]\n",
      "linear.bias:\n",
      " [0.1910227]\n",
      "\n",
      "Test loss: 0.191225, Train loss: 0.191406\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0371305]]\n",
      "linear.bias:\n",
      " [0.19051813]\n",
      "\n",
      "Test loss: 0.190719, Train loss: 0.190900\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03700553]]\n",
      "linear.bias:\n",
      " [0.19001356]\n",
      "\n",
      "Test loss: 0.190213, Train loss: 0.190395\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03688047]]\n",
      "linear.bias:\n",
      " [0.189509]\n",
      "\n",
      "Test loss: 0.189708, Train loss: 0.189890\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03675533]]\n",
      "linear.bias:\n",
      " [0.18900445]\n",
      "\n",
      "Test loss: 0.189202, Train loss: 0.189385\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03663011]]\n",
      "linear.bias:\n",
      " [0.18849991]\n",
      "\n",
      "Test loss: 0.188697, Train loss: 0.188880\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03650481]]\n",
      "linear.bias:\n",
      " [0.18799537]\n",
      "\n",
      "Test loss: 0.188191, Train loss: 0.188374\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03637943]]\n",
      "linear.bias:\n",
      " [0.18749084]\n",
      "\n",
      "Test loss: 0.187685, Train loss: 0.187869\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03625396]]\n",
      "linear.bias:\n",
      " [0.18698631]\n",
      "\n",
      "Test loss: 0.187180, Train loss: 0.187364\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03612842]]\n",
      "linear.bias:\n",
      " [0.18648179]\n",
      "\n",
      "Test loss: 0.186674, Train loss: 0.186859\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03600279]]\n",
      "linear.bias:\n",
      " [0.18597728]\n",
      "\n",
      "Test loss: 0.186169, Train loss: 0.186354\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03587709]]\n",
      "linear.bias:\n",
      " [0.18547277]\n",
      "\n",
      "Test loss: 0.185663, Train loss: 0.185849\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0357513]]\n",
      "linear.bias:\n",
      " [0.18496828]\n",
      "\n",
      "Test loss: 0.185157, Train loss: 0.185343\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03562543]]\n",
      "linear.bias:\n",
      " [0.18446378]\n",
      "\n",
      "Test loss: 0.184652, Train loss: 0.184838\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03549948]]\n",
      "linear.bias:\n",
      " [0.18395929]\n",
      "\n",
      "Test loss: 0.184146, Train loss: 0.184333\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03537344]]\n",
      "linear.bias:\n",
      " [0.18345481]\n",
      "\n",
      "Test loss: 0.183641, Train loss: 0.183828\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03524733]]\n",
      "linear.bias:\n",
      " [0.18295033]\n",
      "\n",
      "Test loss: 0.183135, Train loss: 0.183323\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03512113]]\n",
      "linear.bias:\n",
      " [0.18244587]\n",
      "\n",
      "Test loss: 0.182629, Train loss: 0.182818\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03499485]]\n",
      "linear.bias:\n",
      " [0.1819414]\n",
      "\n",
      "Test loss: 0.182124, Train loss: 0.182313\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03486849]]\n",
      "linear.bias:\n",
      " [0.18143696]\n",
      "\n",
      "Test loss: 0.181618, Train loss: 0.181807\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03474204]]\n",
      "linear.bias:\n",
      " [0.1809325]\n",
      "\n",
      "Test loss: 0.181113, Train loss: 0.181302\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03461552]]\n",
      "linear.bias:\n",
      " [0.18042806]\n",
      "\n",
      "Test loss: 0.180607, Train loss: 0.180797\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03448891]]\n",
      "linear.bias:\n",
      " [0.17992362]\n",
      "\n",
      "Test loss: 0.180102, Train loss: 0.180292\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03436223]]\n",
      "linear.bias:\n",
      " [0.17941919]\n",
      "\n",
      "Test loss: 0.179596, Train loss: 0.179787\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03423546]]\n",
      "linear.bias:\n",
      " [0.17891477]\n",
      "\n",
      "Test loss: 0.179091, Train loss: 0.179282\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03410861]]\n",
      "linear.bias:\n",
      " [0.17841035]\n",
      "\n",
      "Test loss: 0.178585, Train loss: 0.178777\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03398168]]\n",
      "linear.bias:\n",
      " [0.17790593]\n",
      "\n",
      "Test loss: 0.178080, Train loss: 0.178272\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03385466]]\n",
      "linear.bias:\n",
      " [0.17740153]\n",
      "\n",
      "Test loss: 0.177574, Train loss: 0.177767\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03372757]]\n",
      "linear.bias:\n",
      " [0.17689712]\n",
      "\n",
      "Test loss: 0.177069, Train loss: 0.177262\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03360039]]\n",
      "linear.bias:\n",
      " [0.17639273]\n",
      "\n",
      "Test loss: 0.176563, Train loss: 0.176756\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03347313]]\n",
      "linear.bias:\n",
      " [0.17588834]\n",
      "\n",
      "Test loss: 0.176058, Train loss: 0.176251\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03334578]]\n",
      "linear.bias:\n",
      " [0.17538397]\n",
      "\n",
      "Test loss: 0.175552, Train loss: 0.175746\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03321835]]\n",
      "linear.bias:\n",
      " [0.1748796]\n",
      "\n",
      "Test loss: 0.175047, Train loss: 0.175241\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03309084]]\n",
      "linear.bias:\n",
      " [0.17437522]\n",
      "\n",
      "Test loss: 0.174541, Train loss: 0.174736\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03296325]]\n",
      "linear.bias:\n",
      " [0.17387086]\n",
      "\n",
      "Test loss: 0.174036, Train loss: 0.174231\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03283558]]\n",
      "linear.bias:\n",
      " [0.1733665]\n",
      "\n",
      "Test loss: 0.173530, Train loss: 0.173726\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03270783]]\n",
      "linear.bias:\n",
      " [0.17286216]\n",
      "\n",
      "Test loss: 0.173025, Train loss: 0.173221\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03257999]]\n",
      "linear.bias:\n",
      " [0.17235781]\n",
      "\n",
      "Test loss: 0.172519, Train loss: 0.172716\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03245207]]\n",
      "linear.bias:\n",
      " [0.17185347]\n",
      "\n",
      "Test loss: 0.172014, Train loss: 0.172211\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03232407]]\n",
      "linear.bias:\n",
      " [0.17134914]\n",
      "\n",
      "Test loss: 0.171508, Train loss: 0.171706\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03219599]]\n",
      "linear.bias:\n",
      " [0.17084481]\n",
      "\n",
      "Test loss: 0.171003, Train loss: 0.171201\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03206782]]\n",
      "linear.bias:\n",
      " [0.1703405]\n",
      "\n",
      "Test loss: 0.170498, Train loss: 0.170696\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03193957]]\n",
      "linear.bias:\n",
      " [0.16983618]\n",
      "\n",
      "Test loss: 0.169992, Train loss: 0.170191\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03181124]]\n",
      "linear.bias:\n",
      " [0.16933186]\n",
      "\n",
      "Test loss: 0.169487, Train loss: 0.169686\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03168283]]\n",
      "linear.bias:\n",
      " [0.16882756]\n",
      "\n",
      "Test loss: 0.168981, Train loss: 0.169181\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03155434]]\n",
      "linear.bias:\n",
      " [0.16832326]\n",
      "\n",
      "Test loss: 0.168476, Train loss: 0.168676\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03142576]]\n",
      "linear.bias:\n",
      " [0.16781898]\n",
      "\n",
      "Test loss: 0.167971, Train loss: 0.168171\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0312971]]\n",
      "linear.bias:\n",
      " [0.1673147]\n",
      "\n",
      "Test loss: 0.167465, Train loss: 0.167666\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03116836]]\n",
      "linear.bias:\n",
      " [0.16681041]\n",
      "\n",
      "Test loss: 0.166960, Train loss: 0.167161\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03103954]]\n",
      "linear.bias:\n",
      " [0.16630614]\n",
      "\n",
      "Test loss: 0.166454, Train loss: 0.166656\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03091063]]\n",
      "linear.bias:\n",
      " [0.16580187]\n",
      "\n",
      "Test loss: 0.165949, Train loss: 0.166151\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03078164]]\n",
      "linear.bias:\n",
      " [0.16529761]\n",
      "\n",
      "Test loss: 0.165444, Train loss: 0.165646\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03065257]]\n",
      "linear.bias:\n",
      " [0.16479336]\n",
      "\n",
      "Test loss: 0.164938, Train loss: 0.165141\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03052342]]\n",
      "linear.bias:\n",
      " [0.1642891]\n",
      "\n",
      "Test loss: 0.164433, Train loss: 0.164636\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03039418]]\n",
      "linear.bias:\n",
      " [0.16378486]\n",
      "\n",
      "Test loss: 0.163927, Train loss: 0.164131\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03026487]]\n",
      "linear.bias:\n",
      " [0.16328062]\n",
      "\n",
      "Test loss: 0.163422, Train loss: 0.163626\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03013547]]\n",
      "linear.bias:\n",
      " [0.1627764]\n",
      "\n",
      "Test loss: 0.162917, Train loss: 0.163121\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.03000598]]\n",
      "linear.bias:\n",
      " [0.16227217]\n",
      "\n",
      "Test loss: 0.162411, Train loss: 0.162616\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02987642]]\n",
      "linear.bias:\n",
      " [0.16176794]\n",
      "\n",
      "Test loss: 0.161906, Train loss: 0.162112\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02974774]]\n",
      "linear.bias:\n",
      " [0.16126372]\n",
      "\n",
      "Test loss: 0.161401, Train loss: 0.161607\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02961986]]\n",
      "linear.bias:\n",
      " [0.1607595]\n",
      "\n",
      "Test loss: 0.160895, Train loss: 0.161102\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02949269]]\n",
      "linear.bias:\n",
      " [0.16025527]\n",
      "\n",
      "Test loss: 0.160390, Train loss: 0.160597\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02936615]]\n",
      "linear.bias:\n",
      " [0.15975103]\n",
      "\n",
      "Test loss: 0.159885, Train loss: 0.160092\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02924016]]\n",
      "linear.bias:\n",
      " [0.15924679]\n",
      "\n",
      "Test loss: 0.159379, Train loss: 0.159587\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02911467]]\n",
      "linear.bias:\n",
      " [0.15874255]\n",
      "\n",
      "Test loss: 0.158874, Train loss: 0.159082\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02899062]]\n",
      "linear.bias:\n",
      " [0.1582383]\n",
      "\n",
      "Test loss: 0.158369, Train loss: 0.158577\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02886786]]\n",
      "linear.bias:\n",
      " [0.15773405]\n",
      "\n",
      "Test loss: 0.157863, Train loss: 0.158073\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02874624]]\n",
      "linear.bias:\n",
      " [0.1572298]\n",
      "\n",
      "Test loss: 0.157358, Train loss: 0.157568\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02862565]]\n",
      "linear.bias:\n",
      " [0.15672553]\n",
      "\n",
      "Test loss: 0.156853, Train loss: 0.157063\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02850598]]\n",
      "linear.bias:\n",
      " [0.15622126]\n",
      "\n",
      "Test loss: 0.156347, Train loss: 0.156558\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02838713]]\n",
      "linear.bias:\n",
      " [0.15571699]\n",
      "\n",
      "Test loss: 0.155842, Train loss: 0.156053\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02827008]]\n",
      "linear.bias:\n",
      " [0.1552127]\n",
      "\n",
      "Test loss: 0.155337, Train loss: 0.155549\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02815463]]\n",
      "linear.bias:\n",
      " [0.1547084]\n",
      "\n",
      "Test loss: 0.154832, Train loss: 0.155044\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02804062]]\n",
      "linear.bias:\n",
      " [0.1542041]\n",
      "\n",
      "Test loss: 0.154326, Train loss: 0.154539\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02792791]]\n",
      "linear.bias:\n",
      " [0.15369979]\n",
      "\n",
      "Test loss: 0.153821, Train loss: 0.154035\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02781635]]\n",
      "linear.bias:\n",
      " [0.15319547]\n",
      "\n",
      "Test loss: 0.153316, Train loss: 0.153530\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02770582]]\n",
      "linear.bias:\n",
      " [0.15269114]\n",
      "\n",
      "Test loss: 0.152810, Train loss: 0.153025\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02759623]]\n",
      "linear.bias:\n",
      " [0.15218681]\n",
      "\n",
      "Test loss: 0.152305, Train loss: 0.152520\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02748745]]\n",
      "linear.bias:\n",
      " [0.15168248]\n",
      "\n",
      "Test loss: 0.151800, Train loss: 0.152016\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02737942]]\n",
      "linear.bias:\n",
      " [0.15117815]\n",
      "\n",
      "Test loss: 0.151295, Train loss: 0.151511\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02727205]]\n",
      "linear.bias:\n",
      " [0.15067382]\n",
      "\n",
      "Test loss: 0.150789, Train loss: 0.151006\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02716526]]\n",
      "linear.bias:\n",
      " [0.15016948]\n",
      "\n",
      "Test loss: 0.150284, Train loss: 0.150502\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.027059]]\n",
      "linear.bias:\n",
      " [0.14966513]\n",
      "\n",
      "Test loss: 0.149779, Train loss: 0.149997\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02695319]]\n",
      "linear.bias:\n",
      " [0.1491608]\n",
      "\n",
      "Test loss: 0.149274, Train loss: 0.149492\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0268478]]\n",
      "linear.bias:\n",
      " [0.14865647]\n",
      "\n",
      "Test loss: 0.148768, Train loss: 0.148988\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02674278]]\n",
      "linear.bias:\n",
      " [0.14815214]\n",
      "\n",
      "Test loss: 0.148263, Train loss: 0.148483\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02663807]]\n",
      "linear.bias:\n",
      " [0.14764781]\n",
      "\n",
      "Test loss: 0.147758, Train loss: 0.147978\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02653365]]\n",
      "linear.bias:\n",
      " [0.14714348]\n",
      "\n",
      "Test loss: 0.147253, Train loss: 0.147474\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02642947]]\n",
      "linear.bias:\n",
      " [0.14663915]\n",
      "\n",
      "Test loss: 0.146747, Train loss: 0.146969\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02632551]]\n",
      "linear.bias:\n",
      " [0.14613482]\n",
      "\n",
      "Test loss: 0.146242, Train loss: 0.146464\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02622174]]\n",
      "linear.bias:\n",
      " [0.14563051]\n",
      "\n",
      "Test loss: 0.145737, Train loss: 0.145960\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02611813]]\n",
      "linear.bias:\n",
      " [0.1451262]\n",
      "\n",
      "Test loss: 0.145232, Train loss: 0.145455\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02601466]]\n",
      "linear.bias:\n",
      " [0.14462188]\n",
      "\n",
      "Test loss: 0.144727, Train loss: 0.144951\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02591131]]\n",
      "linear.bias:\n",
      " [0.14411756]\n",
      "\n",
      "Test loss: 0.144221, Train loss: 0.144446\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02580807]]\n",
      "linear.bias:\n",
      " [0.14361326]\n",
      "\n",
      "Test loss: 0.143716, Train loss: 0.143941\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0257049]]\n",
      "linear.bias:\n",
      " [0.14310896]\n",
      "\n",
      "Test loss: 0.143211, Train loss: 0.143437\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02560181]]\n",
      "linear.bias:\n",
      " [0.14260466]\n",
      "\n",
      "Test loss: 0.142706, Train loss: 0.142932\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02549878]]\n",
      "linear.bias:\n",
      " [0.14210038]\n",
      "\n",
      "Test loss: 0.142201, Train loss: 0.142428\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02539578]]\n",
      "linear.bias:\n",
      " [0.1415961]\n",
      "\n",
      "Test loss: 0.141695, Train loss: 0.141923\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02529282]]\n",
      "linear.bias:\n",
      " [0.14109181]\n",
      "\n",
      "Test loss: 0.141190, Train loss: 0.141418\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02518989]]\n",
      "linear.bias:\n",
      " [0.14058754]\n",
      "\n",
      "Test loss: 0.140685, Train loss: 0.140914\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02508697]]\n",
      "linear.bias:\n",
      " [0.14008327]\n",
      "\n",
      "Test loss: 0.140180, Train loss: 0.140409\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02498405]]\n",
      "linear.bias:\n",
      " [0.139579]\n",
      "\n",
      "Test loss: 0.139675, Train loss: 0.139905\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02488114]]\n",
      "linear.bias:\n",
      " [0.13907474]\n",
      "\n",
      "Test loss: 0.139170, Train loss: 0.139400\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02477822]]\n",
      "linear.bias:\n",
      " [0.13857049]\n",
      "\n",
      "Test loss: 0.138664, Train loss: 0.138896\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02467528]]\n",
      "linear.bias:\n",
      " [0.13806623]\n",
      "\n",
      "Test loss: 0.138159, Train loss: 0.138391\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02457233]]\n",
      "linear.bias:\n",
      " [0.13756199]\n",
      "\n",
      "Test loss: 0.137654, Train loss: 0.137886\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02446935]]\n",
      "linear.bias:\n",
      " [0.13705775]\n",
      "\n",
      "Test loss: 0.137149, Train loss: 0.137382\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02436635]]\n",
      "linear.bias:\n",
      " [0.13655351]\n",
      "\n",
      "Test loss: 0.136644, Train loss: 0.136877\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02426331]]\n",
      "linear.bias:\n",
      " [0.13604929]\n",
      "\n",
      "Test loss: 0.136139, Train loss: 0.136373\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02416024]]\n",
      "linear.bias:\n",
      " [0.13554506]\n",
      "\n",
      "Test loss: 0.135634, Train loss: 0.135868\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02405713]]\n",
      "linear.bias:\n",
      " [0.13504083]\n",
      "\n",
      "Test loss: 0.135128, Train loss: 0.135364\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02395399]]\n",
      "linear.bias:\n",
      " [0.13453662]\n",
      "\n",
      "Test loss: 0.134623, Train loss: 0.134859\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02385079]]\n",
      "linear.bias:\n",
      " [0.13403241]\n",
      "\n",
      "Test loss: 0.134118, Train loss: 0.134355\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02374756]]\n",
      "linear.bias:\n",
      " [0.13352822]\n",
      "\n",
      "Test loss: 0.133613, Train loss: 0.133850\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02364427]]\n",
      "linear.bias:\n",
      " [0.13302402]\n",
      "\n",
      "Test loss: 0.133108, Train loss: 0.133346\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02354094]]\n",
      "linear.bias:\n",
      " [0.13251983]\n",
      "\n",
      "Test loss: 0.132603, Train loss: 0.132841\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02343755]]\n",
      "linear.bias:\n",
      " [0.13201565]\n",
      "\n",
      "Test loss: 0.132098, Train loss: 0.132337\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02333412]]\n",
      "linear.bias:\n",
      " [0.13151146]\n",
      "\n",
      "Test loss: 0.131593, Train loss: 0.131832\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02323062]]\n",
      "linear.bias:\n",
      " [0.13100728]\n",
      "\n",
      "Test loss: 0.131088, Train loss: 0.131328\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02312642]]\n",
      "linear.bias:\n",
      " [0.13050312]\n",
      "\n",
      "Test loss: 0.130583, Train loss: 0.130823\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02302157]]\n",
      "linear.bias:\n",
      " [0.12999897]\n",
      "\n",
      "Test loss: 0.130078, Train loss: 0.130319\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02291612]]\n",
      "linear.bias:\n",
      " [0.12949483]\n",
      "\n",
      "Test loss: 0.129573, Train loss: 0.129814\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02281014]]\n",
      "linear.bias:\n",
      " [0.12899071]\n",
      "\n",
      "Test loss: 0.129068, Train loss: 0.129310\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02270366]]\n",
      "linear.bias:\n",
      " [0.1284866]\n",
      "\n",
      "Test loss: 0.128563, Train loss: 0.128806\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02259673]]\n",
      "linear.bias:\n",
      " [0.12798251]\n",
      "\n",
      "Test loss: 0.128058, Train loss: 0.128301\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02248939]]\n",
      "linear.bias:\n",
      " [0.12747844]\n",
      "\n",
      "Test loss: 0.127553, Train loss: 0.127797\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02238167]]\n",
      "linear.bias:\n",
      " [0.12697436]\n",
      "\n",
      "Test loss: 0.127047, Train loss: 0.127292\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02227361]]\n",
      "linear.bias:\n",
      " [0.1264703]\n",
      "\n",
      "Test loss: 0.126542, Train loss: 0.126788\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02216522]]\n",
      "linear.bias:\n",
      " [0.12596625]\n",
      "\n",
      "Test loss: 0.126038, Train loss: 0.126284\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02205624]]\n",
      "linear.bias:\n",
      " [0.12546222]\n",
      "\n",
      "Test loss: 0.125533, Train loss: 0.125779\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02194673]]\n",
      "linear.bias:\n",
      " [0.1249582]\n",
      "\n",
      "Test loss: 0.125028, Train loss: 0.125275\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02183673]]\n",
      "linear.bias:\n",
      " [0.1244542]\n",
      "\n",
      "Test loss: 0.124523, Train loss: 0.124771\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02172627]]\n",
      "linear.bias:\n",
      " [0.12395022]\n",
      "\n",
      "Test loss: 0.124018, Train loss: 0.124266\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.021615]]\n",
      "linear.bias:\n",
      " [0.12344626]\n",
      "\n",
      "Test loss: 0.123513, Train loss: 0.123762\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02150298]]\n",
      "linear.bias:\n",
      " [0.12294233]\n",
      "\n",
      "Test loss: 0.123008, Train loss: 0.123258\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02139027]]\n",
      "linear.bias:\n",
      " [0.12243842]\n",
      "\n",
      "Test loss: 0.122503, Train loss: 0.122754\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02127695]]\n",
      "linear.bias:\n",
      " [0.12193452]\n",
      "\n",
      "Test loss: 0.121998, Train loss: 0.122250\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02116305]]\n",
      "linear.bias:\n",
      " [0.12143064]\n",
      "\n",
      "Test loss: 0.121493, Train loss: 0.121746\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02104864]]\n",
      "linear.bias:\n",
      " [0.12092678]\n",
      "\n",
      "Test loss: 0.120988, Train loss: 0.121241\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02093376]]\n",
      "linear.bias:\n",
      " [0.12042294]\n",
      "\n",
      "Test loss: 0.120484, Train loss: 0.120737\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02081845]]\n",
      "linear.bias:\n",
      " [0.11991911]\n",
      "\n",
      "Test loss: 0.119979, Train loss: 0.120233\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02070273]]\n",
      "linear.bias:\n",
      " [0.11941529]\n",
      "\n",
      "Test loss: 0.119474, Train loss: 0.119729\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02058665]]\n",
      "linear.bias:\n",
      " [0.11891149]\n",
      "\n",
      "Test loss: 0.118969, Train loss: 0.119225\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02047023]]\n",
      "linear.bias:\n",
      " [0.1184077]\n",
      "\n",
      "Test loss: 0.118464, Train loss: 0.118721\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02035351]]\n",
      "linear.bias:\n",
      " [0.11790392]\n",
      "\n",
      "Test loss: 0.117959, Train loss: 0.118217\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02023649]]\n",
      "linear.bias:\n",
      " [0.11740015]\n",
      "\n",
      "Test loss: 0.117455, Train loss: 0.117713\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02011921]]\n",
      "linear.bias:\n",
      " [0.11689638]\n",
      "\n",
      "Test loss: 0.116950, Train loss: 0.117209\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.02000168]]\n",
      "linear.bias:\n",
      " [0.11639263]\n",
      "\n",
      "Test loss: 0.116445, Train loss: 0.116705\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01988392]]\n",
      "linear.bias:\n",
      " [0.11588889]\n",
      "\n",
      "Test loss: 0.115940, Train loss: 0.116201\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01976595]]\n",
      "linear.bias:\n",
      " [0.11538516]\n",
      "\n",
      "Test loss: 0.115436, Train loss: 0.115696\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01964778]]\n",
      "linear.bias:\n",
      " [0.11488143]\n",
      "\n",
      "Test loss: 0.114931, Train loss: 0.115192\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01952941]]\n",
      "linear.bias:\n",
      " [0.11437771]\n",
      "\n",
      "Test loss: 0.114426, Train loss: 0.114688\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01941087]]\n",
      "linear.bias:\n",
      " [0.113874]\n",
      "\n",
      "Test loss: 0.113921, Train loss: 0.114184\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01929217]]\n",
      "linear.bias:\n",
      " [0.1133703]\n",
      "\n",
      "Test loss: 0.113417, Train loss: 0.113680\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01917331]]\n",
      "linear.bias:\n",
      " [0.1128666]\n",
      "\n",
      "Test loss: 0.112912, Train loss: 0.113176\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0190543]]\n",
      "linear.bias:\n",
      " [0.11236291]\n",
      "\n",
      "Test loss: 0.112407, Train loss: 0.112672\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01893514]]\n",
      "linear.bias:\n",
      " [0.11185923]\n",
      "\n",
      "Test loss: 0.111902, Train loss: 0.112168\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01881585]]\n",
      "linear.bias:\n",
      " [0.11135556]\n",
      "\n",
      "Test loss: 0.111398, Train loss: 0.111664\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01869644]]\n",
      "linear.bias:\n",
      " [0.11085189]\n",
      "\n",
      "Test loss: 0.110893, Train loss: 0.111160\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0185769]]\n",
      "linear.bias:\n",
      " [0.11034823]\n",
      "\n",
      "Test loss: 0.110388, Train loss: 0.110656\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01845724]]\n",
      "linear.bias:\n",
      " [0.10984457]\n",
      "\n",
      "Test loss: 0.109884, Train loss: 0.110152\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01833746]]\n",
      "linear.bias:\n",
      " [0.10934092]\n",
      "\n",
      "Test loss: 0.109379, Train loss: 0.109648\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01821758]]\n",
      "linear.bias:\n",
      " [0.10883728]\n",
      "\n",
      "Test loss: 0.108874, Train loss: 0.109144\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01809891]]\n",
      "linear.bias:\n",
      " [0.10833362]\n",
      "\n",
      "Test loss: 0.108370, Train loss: 0.108640\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01798135]]\n",
      "linear.bias:\n",
      " [0.10782997]\n",
      "\n",
      "Test loss: 0.107865, Train loss: 0.108136\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01786471]]\n",
      "linear.bias:\n",
      " [0.10732633]\n",
      "\n",
      "Test loss: 0.107360, Train loss: 0.107632\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0177489]]\n",
      "linear.bias:\n",
      " [0.10682269]\n",
      "\n",
      "Test loss: 0.106856, Train loss: 0.107129\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01763383]]\n",
      "linear.bias:\n",
      " [0.10631906]\n",
      "\n",
      "Test loss: 0.106351, Train loss: 0.106625\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01751941]]\n",
      "linear.bias:\n",
      " [0.10581544]\n",
      "\n",
      "Test loss: 0.105846, Train loss: 0.106121\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01740558]]\n",
      "linear.bias:\n",
      " [0.10531183]\n",
      "\n",
      "Test loss: 0.105342, Train loss: 0.105617\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01729228]]\n",
      "linear.bias:\n",
      " [0.10480821]\n",
      "\n",
      "Test loss: 0.104837, Train loss: 0.105114\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01717944]]\n",
      "linear.bias:\n",
      " [0.1043046]\n",
      "\n",
      "Test loss: 0.104333, Train loss: 0.104610\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.017067]]\n",
      "linear.bias:\n",
      " [0.103801]\n",
      "\n",
      "Test loss: 0.103828, Train loss: 0.104106\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01695493]]\n",
      "linear.bias:\n",
      " [0.10329741]\n",
      "\n",
      "Test loss: 0.103323, Train loss: 0.103603\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01684318]]\n",
      "linear.bias:\n",
      " [0.10279383]\n",
      "\n",
      "Test loss: 0.102819, Train loss: 0.103099\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01673345]]\n",
      "linear.bias:\n",
      " [0.10229024]\n",
      "\n",
      "Test loss: 0.102314, Train loss: 0.102595\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01662553]]\n",
      "linear.bias:\n",
      " [0.10178664]\n",
      "\n",
      "Test loss: 0.101810, Train loss: 0.102092\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01651924]]\n",
      "linear.bias:\n",
      " [0.10128304]\n",
      "\n",
      "Test loss: 0.101305, Train loss: 0.101588\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0164144]]\n",
      "linear.bias:\n",
      " [0.10077944]\n",
      "\n",
      "Test loss: 0.100801, Train loss: 0.101085\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01631088]]\n",
      "linear.bias:\n",
      " [0.10027584]\n",
      "\n",
      "Test loss: 0.100296, Train loss: 0.100581\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01620852]]\n",
      "linear.bias:\n",
      " [0.09977223]\n",
      "\n",
      "Test loss: 0.099792, Train loss: 0.100077\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01610722]]\n",
      "linear.bias:\n",
      " [0.09926862]\n",
      "\n",
      "Test loss: 0.099287, Train loss: 0.099574\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01600685]]\n",
      "linear.bias:\n",
      " [0.09876502]\n",
      "\n",
      "Test loss: 0.098783, Train loss: 0.099070\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01590733]]\n",
      "linear.bias:\n",
      " [0.09826142]\n",
      "\n",
      "Test loss: 0.098278, Train loss: 0.098567\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01580856]]\n",
      "linear.bias:\n",
      " [0.09775782]\n",
      "\n",
      "Test loss: 0.097774, Train loss: 0.098063\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01571046]]\n",
      "linear.bias:\n",
      " [0.09725422]\n",
      "\n",
      "Test loss: 0.097270, Train loss: 0.097560\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01561295]]\n",
      "linear.bias:\n",
      " [0.09675062]\n",
      "\n",
      "Test loss: 0.096765, Train loss: 0.097057\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01551598]]\n",
      "linear.bias:\n",
      " [0.09624702]\n",
      "\n",
      "Test loss: 0.096261, Train loss: 0.096553\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01541949]]\n",
      "linear.bias:\n",
      " [0.09574343]\n",
      "\n",
      "Test loss: 0.095756, Train loss: 0.096050\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01532341]]\n",
      "linear.bias:\n",
      " [0.09523983]\n",
      "\n",
      "Test loss: 0.095252, Train loss: 0.095546\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01522771]]\n",
      "linear.bias:\n",
      " [0.09473624]\n",
      "\n",
      "Test loss: 0.094747, Train loss: 0.095043\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01513234]]\n",
      "linear.bias:\n",
      " [0.09423266]\n",
      "\n",
      "Test loss: 0.094243, Train loss: 0.094539\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01503726]]\n",
      "linear.bias:\n",
      " [0.09372907]\n",
      "\n",
      "Test loss: 0.093739, Train loss: 0.094036\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01494244]]\n",
      "linear.bias:\n",
      " [0.09322549]\n",
      "\n",
      "Test loss: 0.093234, Train loss: 0.093532\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01484785]]\n",
      "linear.bias:\n",
      " [0.09272192]\n",
      "\n",
      "Test loss: 0.092730, Train loss: 0.093029\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01475345]]\n",
      "linear.bias:\n",
      " [0.09221835]\n",
      "\n",
      "Test loss: 0.092225, Train loss: 0.092526\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01465923]]\n",
      "linear.bias:\n",
      " [0.09171478]\n",
      "\n",
      "Test loss: 0.091721, Train loss: 0.092022\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01456515]]\n",
      "linear.bias:\n",
      " [0.09121121]\n",
      "\n",
      "Test loss: 0.091217, Train loss: 0.091519\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0144712]]\n",
      "linear.bias:\n",
      " [0.09070765]\n",
      "\n",
      "Test loss: 0.090712, Train loss: 0.091015\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01437802]]\n",
      "linear.bias:\n",
      " [0.0902041]\n",
      "\n",
      "Test loss: 0.090208, Train loss: 0.090512\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01428552]]\n",
      "linear.bias:\n",
      " [0.08970057]\n",
      "\n",
      "Test loss: 0.089704, Train loss: 0.090009\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01419364]]\n",
      "linear.bias:\n",
      " [0.08919705]\n",
      "\n",
      "Test loss: 0.089199, Train loss: 0.089506\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0141038]]\n",
      "linear.bias:\n",
      " [0.08869354]\n",
      "\n",
      "Test loss: 0.088695, Train loss: 0.089003\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01401582]]\n",
      "linear.bias:\n",
      " [0.08819002]\n",
      "\n",
      "Test loss: 0.088191, Train loss: 0.088499\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0139295]]\n",
      "linear.bias:\n",
      " [0.0876865]\n",
      "\n",
      "Test loss: 0.087686, Train loss: 0.087996\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01384466]]\n",
      "linear.bias:\n",
      " [0.08718298]\n",
      "\n",
      "Test loss: 0.087182, Train loss: 0.087493\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01376117]]\n",
      "linear.bias:\n",
      " [0.08667947]\n",
      "\n",
      "Test loss: 0.086678, Train loss: 0.086990\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01367887]]\n",
      "linear.bias:\n",
      " [0.08617596]\n",
      "\n",
      "Test loss: 0.086174, Train loss: 0.086487\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01359766]]\n",
      "linear.bias:\n",
      " [0.08567246]\n",
      "\n",
      "Test loss: 0.085669, Train loss: 0.085984\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0135174]]\n",
      "linear.bias:\n",
      " [0.08516896]\n",
      "\n",
      "Test loss: 0.085165, Train loss: 0.085481\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01343801]]\n",
      "linear.bias:\n",
      " [0.08466545]\n",
      "\n",
      "Test loss: 0.084661, Train loss: 0.084978\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0133594]]\n",
      "linear.bias:\n",
      " [0.08416196]\n",
      "\n",
      "Test loss: 0.084157, Train loss: 0.084475\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01328147]]\n",
      "linear.bias:\n",
      " [0.08365846]\n",
      "\n",
      "Test loss: 0.083653, Train loss: 0.083972\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01320417]]\n",
      "linear.bias:\n",
      " [0.08315498]\n",
      "\n",
      "Test loss: 0.083149, Train loss: 0.083469\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01312742]]\n",
      "linear.bias:\n",
      " [0.08265149]\n",
      "\n",
      "Test loss: 0.082644, Train loss: 0.082966\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01305078]]\n",
      "linear.bias:\n",
      " [0.08214802]\n",
      "\n",
      "Test loss: 0.082140, Train loss: 0.082463\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01297368]]\n",
      "linear.bias:\n",
      " [0.08164456]\n",
      "\n",
      "Test loss: 0.081636, Train loss: 0.081960\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01289552]]\n",
      "linear.bias:\n",
      " [0.08114115]\n",
      "\n",
      "Test loss: 0.081132, Train loss: 0.081457\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01281639]]\n",
      "linear.bias:\n",
      " [0.08063776]\n",
      "\n",
      "Test loss: 0.080628, Train loss: 0.080955\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01273638]]\n",
      "linear.bias:\n",
      " [0.0801344]\n",
      "\n",
      "Test loss: 0.080124, Train loss: 0.080452\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01265559]]\n",
      "linear.bias:\n",
      " [0.07963106]\n",
      "\n",
      "Test loss: 0.079621, Train loss: 0.079949\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01257407]]\n",
      "linear.bias:\n",
      " [0.07912774]\n",
      "\n",
      "Test loss: 0.079117, Train loss: 0.079447\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01249189]]\n",
      "linear.bias:\n",
      " [0.07862445]\n",
      "\n",
      "Test loss: 0.078613, Train loss: 0.078944\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01240913]]\n",
      "linear.bias:\n",
      " [0.07812118]\n",
      "\n",
      "Test loss: 0.078109, Train loss: 0.078441\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01232652]]\n",
      "linear.bias:\n",
      " [0.07761793]\n",
      "\n",
      "Test loss: 0.077606, Train loss: 0.077939\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01224406]]\n",
      "linear.bias:\n",
      " [0.07711471]\n",
      "\n",
      "Test loss: 0.077102, Train loss: 0.077436\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01216172]]\n",
      "linear.bias:\n",
      " [0.07661151]\n",
      "\n",
      "Test loss: 0.076598, Train loss: 0.076934\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01207949]]\n",
      "linear.bias:\n",
      " [0.07610834]\n",
      "\n",
      "Test loss: 0.076095, Train loss: 0.076432\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01199734]]\n",
      "linear.bias:\n",
      " [0.07560518]\n",
      "\n",
      "Test loss: 0.075591, Train loss: 0.075929\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01191527]]\n",
      "linear.bias:\n",
      " [0.07510204]\n",
      "\n",
      "Test loss: 0.075088, Train loss: 0.075427\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01183327]]\n",
      "linear.bias:\n",
      " [0.07459892]\n",
      "\n",
      "Test loss: 0.074584, Train loss: 0.074924\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01175089]]\n",
      "linear.bias:\n",
      " [0.07409582]\n",
      "\n",
      "Test loss: 0.074081, Train loss: 0.074422\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01166778]]\n",
      "linear.bias:\n",
      " [0.07359275]\n",
      "\n",
      "Test loss: 0.073578, Train loss: 0.073920\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01158261]]\n",
      "linear.bias:\n",
      " [0.07308975]\n",
      "\n",
      "Test loss: 0.073075, Train loss: 0.073418\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0114956]]\n",
      "linear.bias:\n",
      " [0.0725868]\n",
      "\n",
      "Test loss: 0.072572, Train loss: 0.072916\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01140691]]\n",
      "linear.bias:\n",
      " [0.07208391]\n",
      "\n",
      "Test loss: 0.072069, Train loss: 0.072414\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01131671]]\n",
      "linear.bias:\n",
      " [0.07158107]\n",
      "\n",
      "Test loss: 0.071566, Train loss: 0.071912\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01122515]]\n",
      "linear.bias:\n",
      " [0.07107829]\n",
      "\n",
      "Test loss: 0.071063, Train loss: 0.071411\n",
      "Epoch [1500/5000], Loss: 0.070909\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01113235]]\n",
      "linear.bias:\n",
      " [0.07057554]\n",
      "\n",
      "Test loss: 0.070561, Train loss: 0.070909\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01103844]]\n",
      "linear.bias:\n",
      " [0.07007284]\n",
      "\n",
      "Test loss: 0.070058, Train loss: 0.070408\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01094263]]\n",
      "linear.bias:\n",
      " [0.06957021]\n",
      "\n",
      "Test loss: 0.069555, Train loss: 0.069906\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01084511]]\n",
      "linear.bias:\n",
      " [0.06906762]\n",
      "\n",
      "Test loss: 0.069053, Train loss: 0.069405\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01074605]]\n",
      "linear.bias:\n",
      " [0.06856508]\n",
      "\n",
      "Test loss: 0.068550, Train loss: 0.068903\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01064582]]\n",
      "linear.bias:\n",
      " [0.06806259]\n",
      "\n",
      "Test loss: 0.068047, Train loss: 0.068402\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01054428]]\n",
      "linear.bias:\n",
      " [0.06756016]\n",
      "\n",
      "Test loss: 0.067545, Train loss: 0.067901\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01044131]]\n",
      "linear.bias:\n",
      " [0.0670578]\n",
      "\n",
      "Test loss: 0.067042, Train loss: 0.067400\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01033706]]\n",
      "linear.bias:\n",
      " [0.06655549]\n",
      "\n",
      "Test loss: 0.066540, Train loss: 0.066899\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01023168]]\n",
      "linear.bias:\n",
      " [0.06605325]\n",
      "\n",
      "Test loss: 0.066038, Train loss: 0.066398\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01012528]]\n",
      "linear.bias:\n",
      " [0.06555106]\n",
      "\n",
      "Test loss: 0.065535, Train loss: 0.065897\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.01001797]]\n",
      "linear.bias:\n",
      " [0.06504893]\n",
      "\n",
      "Test loss: 0.065033, Train loss: 0.065397\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00990985]]\n",
      "linear.bias:\n",
      " [0.06454687]\n",
      "\n",
      "Test loss: 0.064531, Train loss: 0.064896\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.009801]]\n",
      "linear.bias:\n",
      " [0.06404486]\n",
      "\n",
      "Test loss: 0.064029, Train loss: 0.064396\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00969107]]\n",
      "linear.bias:\n",
      " [0.06354293]\n",
      "\n",
      "Test loss: 0.063526, Train loss: 0.063896\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00958015]]\n",
      "linear.bias:\n",
      " [0.06304109]\n",
      "\n",
      "Test loss: 0.063024, Train loss: 0.063396\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00946835]]\n",
      "linear.bias:\n",
      " [0.06253932]\n",
      "\n",
      "Test loss: 0.062522, Train loss: 0.062896\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00935498]]\n",
      "linear.bias:\n",
      " [0.06203768]\n",
      "\n",
      "Test loss: 0.062021, Train loss: 0.062396\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0092402]]\n",
      "linear.bias:\n",
      " [0.06153616]\n",
      "\n",
      "Test loss: 0.061519, Train loss: 0.061897\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00912422]]\n",
      "linear.bias:\n",
      " [0.06103474]\n",
      "\n",
      "Test loss: 0.061017, Train loss: 0.061398\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00900688]]\n",
      "linear.bias:\n",
      " [0.06053344]\n",
      "\n",
      "Test loss: 0.060516, Train loss: 0.060899\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00888802]]\n",
      "linear.bias:\n",
      " [0.06003227]\n",
      "\n",
      "Test loss: 0.060014, Train loss: 0.060401\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0087672]]\n",
      "linear.bias:\n",
      " [0.05953124]\n",
      "\n",
      "Test loss: 0.059513, Train loss: 0.059903\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00864461]]\n",
      "linear.bias:\n",
      " [0.05903033]\n",
      "\n",
      "Test loss: 0.059012, Train loss: 0.059405\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00852042]]\n",
      "linear.bias:\n",
      " [0.05852953]\n",
      "\n",
      "Test loss: 0.058511, Train loss: 0.058907\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00839467]]\n",
      "linear.bias:\n",
      " [0.05802885]\n",
      "\n",
      "Test loss: 0.058010, Train loss: 0.058409\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00826706]]\n",
      "linear.bias:\n",
      " [0.05752827]\n",
      "\n",
      "Test loss: 0.057509, Train loss: 0.057911\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00813742]]\n",
      "linear.bias:\n",
      " [0.0570278]\n",
      "\n",
      "Test loss: 0.057008, Train loss: 0.057414\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00800564]]\n",
      "linear.bias:\n",
      " [0.05652744]\n",
      "\n",
      "Test loss: 0.056507, Train loss: 0.056916\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00787132]]\n",
      "linear.bias:\n",
      " [0.0560272]\n",
      "\n",
      "Test loss: 0.056006, Train loss: 0.056419\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0077347]]\n",
      "linear.bias:\n",
      " [0.05552706]\n",
      "\n",
      "Test loss: 0.055506, Train loss: 0.055922\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00759635]]\n",
      "linear.bias:\n",
      " [0.05502703]\n",
      "\n",
      "Test loss: 0.055006, Train loss: 0.055425\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00745642]]\n",
      "linear.bias:\n",
      " [0.0545271]\n",
      "\n",
      "Test loss: 0.054506, Train loss: 0.054928\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00731505]]\n",
      "linear.bias:\n",
      " [0.05402725]\n",
      "\n",
      "Test loss: 0.054006, Train loss: 0.054432\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00717239]]\n",
      "linear.bias:\n",
      " [0.05352747]\n",
      "\n",
      "Test loss: 0.053507, Train loss: 0.053935\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00702818]]\n",
      "linear.bias:\n",
      " [0.0530278]\n",
      "\n",
      "Test loss: 0.053007, Train loss: 0.053438\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00688223]]\n",
      "linear.bias:\n",
      " [0.05252823]\n",
      "\n",
      "Test loss: 0.052508, Train loss: 0.052942\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00673595]]\n",
      "linear.bias:\n",
      " [0.05202876]\n",
      "\n",
      "Test loss: 0.052009, Train loss: 0.052446\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00658926]]\n",
      "linear.bias:\n",
      " [0.05152939]\n",
      "\n",
      "Test loss: 0.051510, Train loss: 0.051951\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00644194]]\n",
      "linear.bias:\n",
      " [0.05103013]\n",
      "\n",
      "Test loss: 0.051011, Train loss: 0.051455\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00629405]]\n",
      "linear.bias:\n",
      " [0.05053097]\n",
      "\n",
      "Test loss: 0.050512, Train loss: 0.050960\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00614564]]\n",
      "linear.bias:\n",
      " [0.0500319]\n",
      "\n",
      "Test loss: 0.050013, Train loss: 0.050465\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00599683]]\n",
      "linear.bias:\n",
      " [0.04953293]\n",
      "\n",
      "Test loss: 0.049515, Train loss: 0.049970\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00584741]]\n",
      "linear.bias:\n",
      " [0.04903407]\n",
      "\n",
      "Test loss: 0.049016, Train loss: 0.049475\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00569756]]\n",
      "linear.bias:\n",
      " [0.04853532]\n",
      "\n",
      "Test loss: 0.048518, Train loss: 0.048981\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00554731]]\n",
      "linear.bias:\n",
      " [0.04803667]\n",
      "\n",
      "Test loss: 0.048021, Train loss: 0.048486\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00539645]]\n",
      "linear.bias:\n",
      " [0.04753813]\n",
      "\n",
      "Test loss: 0.047523, Train loss: 0.047992\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00524484]]\n",
      "linear.bias:\n",
      " [0.04703971]\n",
      "\n",
      "Test loss: 0.047025, Train loss: 0.047498\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00509256]]\n",
      "linear.bias:\n",
      " [0.0465414]\n",
      "\n",
      "Test loss: 0.046528, Train loss: 0.047005\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00493969]]\n",
      "linear.bias:\n",
      " [0.04604322]\n",
      "\n",
      "Test loss: 0.046031, Train loss: 0.046511\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00478609]]\n",
      "linear.bias:\n",
      " [0.04554519]\n",
      "\n",
      "Test loss: 0.045534, Train loss: 0.046018\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00463102]]\n",
      "linear.bias:\n",
      " [0.04504732]\n",
      "\n",
      "Test loss: 0.045038, Train loss: 0.045526\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00447427]]\n",
      "linear.bias:\n",
      " [0.0445496]\n",
      "\n",
      "Test loss: 0.044541, Train loss: 0.045033\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00431618]]\n",
      "linear.bias:\n",
      " [0.04405209]\n",
      "\n",
      "Test loss: 0.044045, Train loss: 0.044542\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00415655]]\n",
      "linear.bias:\n",
      " [0.04355476]\n",
      "\n",
      "Test loss: 0.043549, Train loss: 0.044050\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00399739]]\n",
      "linear.bias:\n",
      " [0.04305761]\n",
      "\n",
      "Test loss: 0.043054, Train loss: 0.043559\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00383867]]\n",
      "linear.bias:\n",
      " [0.04256065]\n",
      "\n",
      "Test loss: 0.042558, Train loss: 0.043069\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00368054]]\n",
      "linear.bias:\n",
      " [0.04206388]\n",
      "\n",
      "Test loss: 0.042063, Train loss: 0.042579\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00352301]]\n",
      "linear.bias:\n",
      " [0.04156731]\n",
      "\n",
      "Test loss: 0.041568, Train loss: 0.042089\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00336541]]\n",
      "linear.bias:\n",
      " [0.04107095]\n",
      "\n",
      "Test loss: 0.041073, Train loss: 0.041600\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00320756]]\n",
      "linear.bias:\n",
      " [0.04057479]\n",
      "\n",
      "Test loss: 0.040578, Train loss: 0.041111\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00304929]]\n",
      "linear.bias:\n",
      " [0.04007883]\n",
      "\n",
      "Test loss: 0.040084, Train loss: 0.040623\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00289075]]\n",
      "linear.bias:\n",
      " [0.03958306]\n",
      "\n",
      "Test loss: 0.039590, Train loss: 0.040135\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00273194]]\n",
      "linear.bias:\n",
      " [0.03908753]\n",
      "\n",
      "Test loss: 0.039096, Train loss: 0.039647\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00257371]]\n",
      "linear.bias:\n",
      " [0.03859226]\n",
      "\n",
      "Test loss: 0.038603, Train loss: 0.039161\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00241692]]\n",
      "linear.bias:\n",
      " [0.03809725]\n",
      "\n",
      "Test loss: 0.038109, Train loss: 0.038675\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00226123]]\n",
      "linear.bias:\n",
      " [0.03760253]\n",
      "\n",
      "Test loss: 0.037617, Train loss: 0.038189\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00210841]]\n",
      "linear.bias:\n",
      " [0.0371081]\n",
      "\n",
      "Test loss: 0.037125, Train loss: 0.037705\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00196165]]\n",
      "linear.bias:\n",
      " [0.03661397]\n",
      "\n",
      "Test loss: 0.036634, Train loss: 0.037222\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0018238]]\n",
      "linear.bias:\n",
      " [0.03612019]\n",
      "\n",
      "Test loss: 0.036143, Train loss: 0.036740\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00169392]]\n",
      "linear.bias:\n",
      " [0.03562674]\n",
      "\n",
      "Test loss: 0.035653, Train loss: 0.036259\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0015711]]\n",
      "linear.bias:\n",
      " [0.03513362]\n",
      "\n",
      "Test loss: 0.035164, Train loss: 0.035779\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00145548]]\n",
      "linear.bias:\n",
      " [0.03464087]\n",
      "\n",
      "Test loss: 0.034675, Train loss: 0.035300\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00134689]]\n",
      "linear.bias:\n",
      " [0.03414851]\n",
      "\n",
      "Test loss: 0.034188, Train loss: 0.034822\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0012473]]\n",
      "linear.bias:\n",
      " [0.0336566]\n",
      "\n",
      "Test loss: 0.033701, Train loss: 0.034346\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00115569]]\n",
      "linear.bias:\n",
      " [0.03316515]\n",
      "\n",
      "Test loss: 0.033215, Train loss: 0.033870\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00107066]]\n",
      "linear.bias:\n",
      " [0.03267416]\n",
      "\n",
      "Test loss: 0.032730, Train loss: 0.033396\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0009922]]\n",
      "linear.bias:\n",
      " [0.03218364]\n",
      "\n",
      "Test loss: 0.032245, Train loss: 0.032923\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00091951]]\n",
      "linear.bias:\n",
      " [0.03169363]\n",
      "\n",
      "Test loss: 0.031761, Train loss: 0.032451\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00085268]]\n",
      "linear.bias:\n",
      " [0.03120418]\n",
      "\n",
      "Test loss: 0.031278, Train loss: 0.031980\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00079463]]\n",
      "linear.bias:\n",
      " [0.03071536]\n",
      "\n",
      "Test loss: 0.030796, Train loss: 0.031511\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00074411]]\n",
      "linear.bias:\n",
      " [0.03022715]\n",
      "\n",
      "Test loss: 0.030315, Train loss: 0.031044\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00070075]]\n",
      "linear.bias:\n",
      " [0.02973963]\n",
      "\n",
      "Test loss: 0.029836, Train loss: 0.030578\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0006651]]\n",
      "linear.bias:\n",
      " [0.02925284]\n",
      "\n",
      "Test loss: 0.029358, Train loss: 0.030114\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00063716]]\n",
      "linear.bias:\n",
      " [0.02876678]\n",
      "\n",
      "Test loss: 0.028880, Train loss: 0.029652\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00061496]]\n",
      "linear.bias:\n",
      " [0.02828149]\n",
      "\n",
      "Test loss: 0.028404, Train loss: 0.029191\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00059843]]\n",
      "linear.bias:\n",
      " [0.02779702]\n",
      "\n",
      "Test loss: 0.027930, Train loss: 0.028732\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00058706]]\n",
      "linear.bias:\n",
      " [0.02731338]\n",
      "\n",
      "Test loss: 0.027456, Train loss: 0.028275\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00058035]]\n",
      "linear.bias:\n",
      " [0.02683056]\n",
      "\n",
      "Test loss: 0.026984, Train loss: 0.027820\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00057718]]\n",
      "linear.bias:\n",
      " [0.0263486]\n",
      "\n",
      "Test loss: 0.026513, Train loss: 0.027366\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00057744]]\n",
      "linear.bias:\n",
      " [0.02586746]\n",
      "\n",
      "Test loss: 0.026043, Train loss: 0.026914\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00058057]]\n",
      "linear.bias:\n",
      " [0.0253872]\n",
      "\n",
      "Test loss: 0.025574, Train loss: 0.026464\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00058716]]\n",
      "linear.bias:\n",
      " [0.02490782]\n",
      "\n",
      "Test loss: 0.025107, Train loss: 0.026015\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00059703]]\n",
      "linear.bias:\n",
      " [0.02442942]\n",
      "\n",
      "Test loss: 0.024641, Train loss: 0.025569\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00060794]]\n",
      "linear.bias:\n",
      " [0.023952]\n",
      "\n",
      "Test loss: 0.024176, Train loss: 0.025125\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00061887]]\n",
      "linear.bias:\n",
      " [0.02347558]\n",
      "\n",
      "Test loss: 0.023714, Train loss: 0.024683\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0006304]]\n",
      "linear.bias:\n",
      " [0.02300019]\n",
      "\n",
      "Test loss: 0.023255, Train loss: 0.024243\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00064211]]\n",
      "linear.bias:\n",
      " [0.0225259]\n",
      "\n",
      "Test loss: 0.022798, Train loss: 0.023806\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00065514]]\n",
      "linear.bias:\n",
      " [0.02205284]\n",
      "\n",
      "Test loss: 0.022344, Train loss: 0.023372\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00066997]]\n",
      "linear.bias:\n",
      " [0.02158111]\n",
      "\n",
      "Test loss: 0.021891, Train loss: 0.022941\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00068391]]\n",
      "linear.bias:\n",
      " [0.02111083]\n",
      "\n",
      "Test loss: 0.021442, Train loss: 0.022513\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00069685]]\n",
      "linear.bias:\n",
      " [0.02064203]\n",
      "\n",
      "Test loss: 0.020996, Train loss: 0.022089\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00070875]]\n",
      "linear.bias:\n",
      " [0.02017478]\n",
      "\n",
      "Test loss: 0.020551, Train loss: 0.021668\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00071756]]\n",
      "linear.bias:\n",
      " [0.01970922]\n",
      "\n",
      "Test loss: 0.020110, Train loss: 0.021251\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00072279]]\n",
      "linear.bias:\n",
      " [0.01924543]\n",
      "\n",
      "Test loss: 0.019671, Train loss: 0.020838\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00072493]]\n",
      "linear.bias:\n",
      " [0.01878352]\n",
      "\n",
      "Test loss: 0.019235, Train loss: 0.020429\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00072219]]\n",
      "linear.bias:\n",
      " [0.01832352]\n",
      "\n",
      "Test loss: 0.018802, Train loss: 0.020024\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00071488]]\n",
      "linear.bias:\n",
      " [0.0178655]\n",
      "\n",
      "Test loss: 0.018371, Train loss: 0.019624\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00070588]]\n",
      "linear.bias:\n",
      " [0.01740954]\n",
      "\n",
      "Test loss: 0.017943, Train loss: 0.019227\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00069423]]\n",
      "linear.bias:\n",
      " [0.01695566]\n",
      "\n",
      "Test loss: 0.017519, Train loss: 0.018834\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00068011]]\n",
      "linear.bias:\n",
      " [0.01650397]\n",
      "\n",
      "Test loss: 0.017099, Train loss: 0.018446\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.000666]]\n",
      "linear.bias:\n",
      " [0.01605471]\n",
      "\n",
      "Test loss: 0.016683, Train loss: 0.018064\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00065401]]\n",
      "linear.bias:\n",
      " [0.01560809]\n",
      "\n",
      "Test loss: 0.016274, Train loss: 0.017688\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00064722]]\n",
      "linear.bias:\n",
      " [0.01516426]\n",
      "\n",
      "Test loss: 0.015869, Train loss: 0.017317\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00064325]]\n",
      "linear.bias:\n",
      " [0.01472333]\n",
      "\n",
      "Test loss: 0.015471, Train loss: 0.016953\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00064112]]\n",
      "linear.bias:\n",
      " [0.01428528]\n",
      "\n",
      "Test loss: 0.015079, Train loss: 0.016594\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00064082]]\n",
      "linear.bias:\n",
      " [0.01385019]\n",
      "\n",
      "Test loss: 0.014691, Train loss: 0.016240\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00064266]]\n",
      "linear.bias:\n",
      " [0.01341808]\n",
      "\n",
      "Test loss: 0.014310, Train loss: 0.015891\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00064703]]\n",
      "linear.bias:\n",
      " [0.01298904]\n",
      "\n",
      "Test loss: 0.013933, Train loss: 0.015548\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00065072]]\n",
      "linear.bias:\n",
      " [0.01256328]\n",
      "\n",
      "Test loss: 0.013561, Train loss: 0.015211\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00065654]]\n",
      "linear.bias:\n",
      " [0.01214089]\n",
      "\n",
      "Test loss: 0.013195, Train loss: 0.014881\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.000666]]\n",
      "linear.bias:\n",
      " [0.01172204]\n",
      "\n",
      "Test loss: 0.012837, Train loss: 0.014558\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00067616]]\n",
      "linear.bias:\n",
      " [0.01130687]\n",
      "\n",
      "Test loss: 0.012485, Train loss: 0.014241\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00068503]]\n",
      "linear.bias:\n",
      " [0.01089553]\n",
      "\n",
      "Test loss: 0.012142, Train loss: 0.013931\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0006937]]\n",
      "linear.bias:\n",
      " [0.0104882]\n",
      "\n",
      "Test loss: 0.011805, Train loss: 0.013629\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0007009]]\n",
      "linear.bias:\n",
      " [0.01008522]\n",
      "\n",
      "Test loss: 0.011478, Train loss: 0.013335\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00070871]]\n",
      "linear.bias:\n",
      " [0.00968669]\n",
      "\n",
      "Test loss: 0.011161, Train loss: 0.013050\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00071685]]\n",
      "linear.bias:\n",
      " [0.00929283]\n",
      "\n",
      "Test loss: 0.010853, Train loss: 0.012772\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00072757]]\n",
      "linear.bias:\n",
      " [0.00890374]\n",
      "\n",
      "Test loss: 0.010553, Train loss: 0.012502\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00074202]]\n",
      "linear.bias:\n",
      " [0.00851957]\n",
      "\n",
      "Test loss: 0.010265, Train loss: 0.012240\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00075678]]\n",
      "linear.bias:\n",
      " [0.00814062]\n",
      "\n",
      "Test loss: 0.009988, Train loss: 0.011987\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00077399]]\n",
      "linear.bias:\n",
      " [0.0077671]\n",
      "\n",
      "Test loss: 0.009721, Train loss: 0.011744\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0007952]]\n",
      "linear.bias:\n",
      " [0.00739911]\n",
      "\n",
      "Test loss: 0.009467, Train loss: 0.011509\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00081827]]\n",
      "linear.bias:\n",
      " [0.00703685]\n",
      "\n",
      "Test loss: 0.009223, Train loss: 0.011282\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00084293]]\n",
      "linear.bias:\n",
      " [0.00668049]\n",
      "\n",
      "Test loss: 0.008990, Train loss: 0.011064\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00086669]]\n",
      "linear.bias:\n",
      " [0.0063303]\n",
      "\n",
      "Test loss: 0.008768, Train loss: 0.010856\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00088988]]\n",
      "linear.bias:\n",
      " [0.00598644]\n",
      "\n",
      "Test loss: 0.008557, Train loss: 0.010657\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00091254]]\n",
      "linear.bias:\n",
      " [0.0056492]\n",
      "\n",
      "Test loss: 0.008358, Train loss: 0.010468\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00093442]]\n",
      "linear.bias:\n",
      " [0.00531888]\n",
      "\n",
      "Test loss: 0.008172, Train loss: 0.010288\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00095564]]\n",
      "linear.bias:\n",
      " [0.00499582]\n",
      "\n",
      "Test loss: 0.007996, Train loss: 0.010118\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00097538]]\n",
      "linear.bias:\n",
      " [0.00468025]\n",
      "\n",
      "Test loss: 0.007833, Train loss: 0.009959\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0009924]]\n",
      "linear.bias:\n",
      " [0.00437264]\n",
      "\n",
      "Test loss: 0.007679, Train loss: 0.009810\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010093]]\n",
      "linear.bias:\n",
      " [0.00407346]\n",
      "\n",
      "Test loss: 0.007538, Train loss: 0.009673\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00102623]]\n",
      "linear.bias:\n",
      " [0.00378302]\n",
      "\n",
      "Test loss: 0.007410, Train loss: 0.009546\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010409]]\n",
      "linear.bias:\n",
      " [0.00350179]\n",
      "\n",
      "Test loss: 0.007293, Train loss: 0.009430\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00105029]]\n",
      "linear.bias:\n",
      " [0.00322998]\n",
      "\n",
      "Test loss: 0.007187, Train loss: 0.009325\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00105576]]\n",
      "linear.bias:\n",
      " [0.00296797]\n",
      "\n",
      "Test loss: 0.007093, Train loss: 0.009230\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00105674]]\n",
      "linear.bias:\n",
      " [0.00271609]\n",
      "\n",
      "Test loss: 0.007010, Train loss: 0.009145\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00105434]]\n",
      "linear.bias:\n",
      " [0.00247458]\n",
      "\n",
      "Test loss: 0.006937, Train loss: 0.009070\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00105156]]\n",
      "linear.bias:\n",
      " [0.00224391]\n",
      "\n",
      "Test loss: 0.006874, Train loss: 0.009004\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00104647]]\n",
      "linear.bias:\n",
      " [0.00202429]\n",
      "\n",
      "Test loss: 0.006820, Train loss: 0.008948\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00103829]]\n",
      "linear.bias:\n",
      " [0.0018157]\n",
      "\n",
      "Test loss: 0.006775, Train loss: 0.008899\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.001028]]\n",
      "linear.bias:\n",
      " [0.00161808]\n",
      "\n",
      "Test loss: 0.006737, Train loss: 0.008857\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00101678]]\n",
      "linear.bias:\n",
      " [0.00143146]\n",
      "\n",
      "Test loss: 0.006704, Train loss: 0.008822\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00100568]]\n",
      "linear.bias:\n",
      " [0.00125575]\n",
      "\n",
      "Test loss: 0.006677, Train loss: 0.008792\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00099608]]\n",
      "linear.bias:\n",
      " [0.00109103]\n",
      "\n",
      "Test loss: 0.006655, Train loss: 0.008768\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00099004]]\n",
      "linear.bias:\n",
      " [0.00093717]\n",
      "\n",
      "Test loss: 0.006636, Train loss: 0.008749\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00098788]]\n",
      "linear.bias:\n",
      " [0.00079432]\n",
      "\n",
      "Test loss: 0.006621, Train loss: 0.008735\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00098985]]\n",
      "linear.bias:\n",
      " [0.00066235]\n",
      "\n",
      "Test loss: 0.006610, Train loss: 0.008724\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00099596]]\n",
      "linear.bias:\n",
      " [0.00054103]\n",
      "\n",
      "Test loss: 0.006602, Train loss: 0.008717\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00100668]]\n",
      "linear.bias:\n",
      " [0.00042999]\n",
      "\n",
      "Test loss: 0.006598, Train loss: 0.008712\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00102137]]\n",
      "linear.bias:\n",
      " [0.00032899]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008709\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00104011]]\n",
      "linear.bias:\n",
      " [0.00023763]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00106168]]\n",
      "linear.bias:\n",
      " [0.00015557]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010842]]\n",
      "linear.bias:\n",
      " [8.246277e-05]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008708\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00110551]]\n",
      "linear.bias:\n",
      " [1.8035935e-05]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008709\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00112421]]\n",
      "linear.bias:\n",
      " [-3.8170587e-05]\n",
      "\n",
      "Test loss: 0.006596, Train loss: 0.008711\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00113879]]\n",
      "linear.bias:\n",
      " [-8.6487096e-05]\n",
      "\n",
      "Test loss: 0.006598, Train loss: 0.008713\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00114777]]\n",
      "linear.bias:\n",
      " [-0.00012749]\n",
      "\n",
      "Test loss: 0.006599, Train loss: 0.008716\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00114896]]\n",
      "linear.bias:\n",
      " [-0.0001616]\n",
      "\n",
      "Test loss: 0.006601, Train loss: 0.008718\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00114232]]\n",
      "linear.bias:\n",
      " [-0.00018926]\n",
      "\n",
      "Test loss: 0.006602, Train loss: 0.008720\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00112829]]\n",
      "linear.bias:\n",
      " [-0.00021092]\n",
      "\n",
      "Test loss: 0.006603, Train loss: 0.008721\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00110768]]\n",
      "linear.bias:\n",
      " [-0.0002271]\n",
      "\n",
      "Test loss: 0.006604, Train loss: 0.008723\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108405]]\n",
      "linear.bias:\n",
      " [-0.00023828]\n",
      "\n",
      "Test loss: 0.006604, Train loss: 0.008724\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00106023]]\n",
      "linear.bias:\n",
      " [-0.00024495]\n",
      "\n",
      "Test loss: 0.006605, Train loss: 0.008724\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010378]]\n",
      "linear.bias:\n",
      " [-0.00024749]\n",
      "\n",
      "Test loss: 0.006605, Train loss: 0.008725\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010171]]\n",
      "linear.bias:\n",
      " [-0.00024631]\n",
      "\n",
      "Test loss: 0.006605, Train loss: 0.008725\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00099939]]\n",
      "linear.bias:\n",
      " [-0.00024177]\n",
      "\n",
      "Test loss: 0.006605, Train loss: 0.008725\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00098503]]\n",
      "linear.bias:\n",
      " [-0.0002342]\n",
      "\n",
      "Test loss: 0.006605, Train loss: 0.008724\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00097429]]\n",
      "linear.bias:\n",
      " [-0.00022394]\n",
      "\n",
      "Test loss: 0.006604, Train loss: 0.008724\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00096724]]\n",
      "linear.bias:\n",
      " [-0.00021134]\n",
      "\n",
      "Test loss: 0.006604, Train loss: 0.008723\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00096342]]\n",
      "linear.bias:\n",
      " [-0.00019668]\n",
      "\n",
      "Test loss: 0.006603, Train loss: 0.008723\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00096263]]\n",
      "linear.bias:\n",
      " [-0.00018024]\n",
      "\n",
      "Test loss: 0.006602, Train loss: 0.008722\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00096526]]\n",
      "linear.bias:\n",
      " [-0.00016225]\n",
      "\n",
      "Test loss: 0.006601, Train loss: 0.008721\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0009708]]\n",
      "linear.bias:\n",
      " [-0.0001431]\n",
      "\n",
      "Test loss: 0.006600, Train loss: 0.008720\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00097891]]\n",
      "linear.bias:\n",
      " [-0.00012302]\n",
      "\n",
      "Test loss: 0.006600, Train loss: 0.008719\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00098927]]\n",
      "linear.bias:\n",
      " [-0.00010226]\n",
      "\n",
      "Test loss: 0.006599, Train loss: 0.008717\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00100085]]\n",
      "linear.bias:\n",
      " [-8.101036e-05]\n",
      "\n",
      "Test loss: 0.006598, Train loss: 0.008716\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00101359]]\n",
      "linear.bias:\n",
      " [-5.939936e-05]\n",
      "\n",
      "Test loss: 0.006597, Train loss: 0.008715\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00102716]]\n",
      "linear.bias:\n",
      " [-3.7594895e-05]\n",
      "\n",
      "Test loss: 0.006597, Train loss: 0.008714\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00104047]]\n",
      "linear.bias:\n",
      " [-1.5724305e-05]\n",
      "\n",
      "Test loss: 0.006596, Train loss: 0.008713\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00105505]]\n",
      "linear.bias:\n",
      " [6.020873e-06]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008712\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00106951]]\n",
      "linear.bias:\n",
      " [2.7478896e-05]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008712\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108378]]\n",
      "linear.bias:\n",
      " [4.856973e-05]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008711\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00109721]]\n",
      "linear.bias:\n",
      " [6.922117e-05]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008710\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00110962]]\n",
      "linear.bias:\n",
      " [8.933508e-05]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008710\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00111993]]\n",
      "linear.bias:\n",
      " [0.00010883]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008709\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00112693]]\n",
      "linear.bias:\n",
      " [0.0001276]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008709\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00113161]]\n",
      "linear.bias:\n",
      " [0.00014563]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008708\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00113384]]\n",
      "linear.bias:\n",
      " [0.00016282]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008708\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00113424]]\n",
      "linear.bias:\n",
      " [0.0001791]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00113254]]\n",
      "linear.bias:\n",
      " [0.00019443]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00112915]]\n",
      "linear.bias:\n",
      " [0.00020874]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00112503]]\n",
      "linear.bias:\n",
      " [0.00022201]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00112036]]\n",
      "linear.bias:\n",
      " [0.00023426]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00111541]]\n",
      "linear.bias:\n",
      " [0.00024547]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00111053]]\n",
      "linear.bias:\n",
      " [0.00025566]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00110584]]\n",
      "linear.bias:\n",
      " [0.00026487]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00110124]]\n",
      "linear.bias:\n",
      " [0.0002732]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00109681]]\n",
      "linear.bias:\n",
      " [0.0002807]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00109273]]\n",
      "linear.bias:\n",
      " [0.00028742]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108893]]\n",
      "linear.bias:\n",
      " [0.00029338]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108552]]\n",
      "linear.bias:\n",
      " [0.00029862]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108308]]\n",
      "linear.bias:\n",
      " [0.00030317]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00030705]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108077]]\n",
      "linear.bias:\n",
      " [0.00031028]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010805]]\n",
      "linear.bias:\n",
      " [0.0003129]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108205]]\n",
      "linear.bias:\n",
      " [0.00031494]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108396]]\n",
      "linear.bias:\n",
      " [0.00031649]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.001086]]\n",
      "linear.bias:\n",
      " [0.00031759]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108785]]\n",
      "linear.bias:\n",
      " [0.00031826]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108955]]\n",
      "linear.bias:\n",
      " [0.00031853]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00109063]]\n",
      "linear.bias:\n",
      " [0.00031843]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00109117]]\n",
      "linear.bias:\n",
      " [0.000318]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010912]]\n",
      "linear.bias:\n",
      " [0.00031726]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00109091]]\n",
      "linear.bias:\n",
      " [0.00031626]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00109032]]\n",
      "linear.bias:\n",
      " [0.00031503]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108981]]\n",
      "linear.bias:\n",
      " [0.00031361]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108936]]\n",
      "linear.bias:\n",
      " [0.00031202]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108897]]\n",
      "linear.bias:\n",
      " [0.00031028]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108904]]\n",
      "linear.bias:\n",
      " [0.00030842]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108951]]\n",
      "linear.bias:\n",
      " [0.00030647]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00109035]]\n",
      "linear.bias:\n",
      " [0.00030445]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00109099]]\n",
      "linear.bias:\n",
      " [0.0003024]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00109142]]\n",
      "linear.bias:\n",
      " [0.00030034]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00109157]]\n",
      "linear.bias:\n",
      " [0.00029829]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00109149]]\n",
      "linear.bias:\n",
      " [0.00029629]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00109136]]\n",
      "linear.bias:\n",
      " [0.00029436]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00109124]]\n",
      "linear.bias:\n",
      " [0.00029249]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00109111]]\n",
      "linear.bias:\n",
      " [0.00029069]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00109092]]\n",
      "linear.bias:\n",
      " [0.00028897]\n",
      "\n",
      "Test loss: 0.006595, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00109067]]\n",
      "linear.bias:\n",
      " [0.00028733]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00109034]]\n",
      "linear.bias:\n",
      " [0.00028575]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108991]]\n",
      "linear.bias:\n",
      " [0.00028424]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108944]]\n",
      "linear.bias:\n",
      " [0.00028281]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108899]]\n",
      "linear.bias:\n",
      " [0.00028147]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108848]]\n",
      "linear.bias:\n",
      " [0.00028022]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108792]]\n",
      "linear.bias:\n",
      " [0.00027904]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108731]]\n",
      "linear.bias:\n",
      " [0.00027794]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010867]]\n",
      "linear.bias:\n",
      " [0.00027693]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010861]]\n",
      "linear.bias:\n",
      " [0.00027599]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108554]]\n",
      "linear.bias:\n",
      " [0.00027513]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108501]]\n",
      "linear.bias:\n",
      " [0.00027435]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108451]]\n",
      "linear.bias:\n",
      " [0.00027363]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108404]]\n",
      "linear.bias:\n",
      " [0.00027298]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010836]]\n",
      "linear.bias:\n",
      " [0.00027238]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108318]]\n",
      "linear.bias:\n",
      " [0.00027182]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108279]]\n",
      "linear.bias:\n",
      " [0.00027131]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108241]]\n",
      "linear.bias:\n",
      " [0.00027084]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108205]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027002]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00026967]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108109]]\n",
      "linear.bias:\n",
      " [0.00026938]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108089]]\n",
      "linear.bias:\n",
      " [0.00026912]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108073]]\n",
      "linear.bias:\n",
      " [0.0002689]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108061]]\n",
      "linear.bias:\n",
      " [0.00026872]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108053]]\n",
      "linear.bias:\n",
      " [0.00026856]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108049]]\n",
      "linear.bias:\n",
      " [0.00026844]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108048]]\n",
      "linear.bias:\n",
      " [0.00026833]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010805]]\n",
      "linear.bias:\n",
      " [0.00026825]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108054]]\n",
      "linear.bias:\n",
      " [0.00026819]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108061]]\n",
      "linear.bias:\n",
      " [0.00026814]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108069]]\n",
      "linear.bias:\n",
      " [0.00026811]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010808]]\n",
      "linear.bias:\n",
      " [0.0002681]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108092]]\n",
      "linear.bias:\n",
      " [0.0002681]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108105]]\n",
      "linear.bias:\n",
      " [0.00026811]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010812]]\n",
      "linear.bias:\n",
      " [0.00026813]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00026816]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.0002682]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108172]]\n",
      "linear.bias:\n",
      " [0.00026824]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108191]]\n",
      "linear.bias:\n",
      " [0.0002683]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108211]]\n",
      "linear.bias:\n",
      " [0.00026836]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108232]]\n",
      "linear.bias:\n",
      " [0.00026842]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108253]]\n",
      "linear.bias:\n",
      " [0.00026849]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108275]]\n",
      "linear.bias:\n",
      " [0.00026857]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108298]]\n",
      "linear.bias:\n",
      " [0.00026865]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108321]]\n",
      "linear.bias:\n",
      " [0.00026873]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108344]]\n",
      "linear.bias:\n",
      " [0.00026881]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108368]]\n",
      "linear.bias:\n",
      " [0.0002689]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108393]]\n",
      "linear.bias:\n",
      " [0.000269]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108417]]\n",
      "linear.bias:\n",
      " [0.00026909]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108442]]\n",
      "linear.bias:\n",
      " [0.00026919]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108467]]\n",
      "linear.bias:\n",
      " [0.00026929]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108492]]\n",
      "linear.bias:\n",
      " [0.00026939]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010851]]\n",
      "linear.bias:\n",
      " [0.00026948]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108519]]\n",
      "linear.bias:\n",
      " [0.00026956]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108522]]\n",
      "linear.bias:\n",
      " [0.00026963]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010852]]\n",
      "linear.bias:\n",
      " [0.0002697]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108511]]\n",
      "linear.bias:\n",
      " [0.00026976]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108498]]\n",
      "linear.bias:\n",
      " [0.00026981]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010848]]\n",
      "linear.bias:\n",
      " [0.00026986]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108459]]\n",
      "linear.bias:\n",
      " [0.0002699]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108434]]\n",
      "linear.bias:\n",
      " [0.00026994]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108405]]\n",
      "linear.bias:\n",
      " [0.00026998]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108374]]\n",
      "linear.bias:\n",
      " [0.00027001]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010834]]\n",
      "linear.bias:\n",
      " [0.00027004]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108304]]\n",
      "linear.bias:\n",
      " [0.00027006]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108265]]\n",
      "linear.bias:\n",
      " [0.00027009]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108234]]\n",
      "linear.bias:\n",
      " [0.00027012]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108208]]\n",
      "linear.bias:\n",
      " [0.00027016]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108187]]\n",
      "linear.bias:\n",
      " [0.00027021]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108172]]\n",
      "linear.bias:\n",
      " [0.00027027]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027033]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.0002706]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027064]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027067]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027068]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108117]]\n",
      "linear.bias:\n",
      " [0.00027068]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108107]]\n",
      "linear.bias:\n",
      " [0.00027066]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108095]]\n",
      "linear.bias:\n",
      " [0.00027064]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108091]]\n",
      "linear.bias:\n",
      " [0.00027061]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108094]]\n",
      "linear.bias:\n",
      " [0.00027059]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108103]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108118]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108172]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108125]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "Epoch [2000/5000], Loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108177]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108176]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108124]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108124]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108173]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108177]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108173]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108125]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108124]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108172]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108175]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108122]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108125]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108173]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108176]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108175]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "Epoch [2500/5000], Loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108121]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108123]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108173]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108172]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108179]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108186]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108183]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108123]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108117]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108122]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108173]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108178]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010818]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108177]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108124]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108123]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108183]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108186]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108179]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108125]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108118]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108116]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108125]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108125]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108179]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108177]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108125]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108183]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108187]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010818]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108122]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010812]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108123]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108117]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108115]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108124]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108191]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108195]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108189]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108173]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108173]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108173]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108117]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108177]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108181]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108178]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108181]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108172]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108178]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108125]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108112]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108113]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108173]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "Epoch [3000/5000], Loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108124]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108124]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108122]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108173]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108172]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108122]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108123]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010818]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108192]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108192]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010818]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010811]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108109]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108121]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108124]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108186]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108199]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.001082]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108188]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108112]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108105]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108111]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108182]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108191]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108187]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108123]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108173]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108175]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108183]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108187]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108179]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108125]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108178]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108182]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108192]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108197]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108188]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108115]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108109]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108118]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108124]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108177]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108181]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108172]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010818]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108175]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108118]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108122]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108173]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108185]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108192]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108184]]\n",
      "linear.bias:\n",
      " [0.00027059]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108172]]\n",
      "linear.bias:\n",
      " [0.0002706]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027059]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108116]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108111]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108113]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108121]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108172]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010812]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108114]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108123]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108182]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108191]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108194]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108191]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108184]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108109]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108105]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108116]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108182]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010818]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108172]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108122]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108123]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108177]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108176]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010818]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108179]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108118]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108117]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108116]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108179]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108125]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108172]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108173]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108178]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108181]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108125]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108115]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108113]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108191]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108191]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108187]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108177]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108104]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108097]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "Epoch [3500/5000], Loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108106]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108122]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108177]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108179]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108172]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108182]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108185]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108173]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010812]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010811]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108179]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108187]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108188]]\n",
      "linear.bias:\n",
      " [0.00027059]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108185]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108176]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108118]]\n",
      "linear.bias:\n",
      " [0.00027038]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108109]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108109]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108116]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108173]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.0002706]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108173]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108123]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108172]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108176]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108121]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108123]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108124]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108122]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108177]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108185]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108178]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010812]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108173]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108181]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108172]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108172]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108194]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108197]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108183]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108122]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108111]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010811]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108116]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108181]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108201]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108202]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108197]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108186]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010812]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108091]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108084]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108097]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108176]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108172]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108197]]\n",
      "linear.bias:\n",
      " [0.00027059]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108211]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108218]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108207]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010818]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108111]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108093]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108096]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108118]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108184]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108203]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108203]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108186]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108123]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108122]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027059]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108173]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108195]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108197]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108182]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108118]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108107]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108116]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108186]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108208]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010821]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108196]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010812]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108099]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108099]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108175]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108112]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108109]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108125]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108177]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108186]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108176]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010812]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108112]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108125]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108177]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108179]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108173]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108113]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108118]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108185]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108187]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010818]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108176]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108115]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108121]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108189]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108191]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108175]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108184]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108175]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108173]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108178]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108175]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108124]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108196]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.001082]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108185]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108117]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108105]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108115]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "Epoch [4000/5000], Loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108191]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108201]]\n",
      "linear.bias:\n",
      " [0.00027059]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108203]]\n",
      "linear.bias:\n",
      " [0.00027061]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108198]]\n",
      "linear.bias:\n",
      " [0.0002706]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108187]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027038]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108114]]\n",
      "linear.bias:\n",
      " [0.00027035]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108103]]\n",
      "linear.bias:\n",
      " [0.00027036]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108103]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108111]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027059]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027036]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027033]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027034]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108125]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108193]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108207]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.001082]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108104]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108099]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108116]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108179]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108195]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108191]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108101]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108086]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108094]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108123]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108205]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108217]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108209]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108183]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108123]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108189]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108188]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108176]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108117]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108112]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108117]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108177]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010818]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108181]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108196]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108188]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108118]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108101]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108108]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108182]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108185]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108189]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108192]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108175]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108124]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108122]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108183]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108196]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108201]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108184]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010811]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108097]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108109]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108192]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108203]]\n",
      "linear.bias:\n",
      " [0.0002706]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108206]]\n",
      "linear.bias:\n",
      " [0.00027063]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.001082]]\n",
      "linear.bias:\n",
      " [0.00027061]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108188]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027037]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108108]]\n",
      "linear.bias:\n",
      " [0.00027034]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108096]]\n",
      "linear.bias:\n",
      " [0.00027035]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108096]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108105]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.0002706]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027035]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027032]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108124]]\n",
      "linear.bias:\n",
      " [0.00027033]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108176]]\n",
      "linear.bias:\n",
      " [0.0002706]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108191]]\n",
      "linear.bias:\n",
      " [0.00027067]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108197]]\n",
      "linear.bias:\n",
      " [0.00027069]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108195]]\n",
      "linear.bias:\n",
      " [0.00027067]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108186]]\n",
      "linear.bias:\n",
      " [0.0002706]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108112]]\n",
      "linear.bias:\n",
      " [0.00027037]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108103]]\n",
      "linear.bias:\n",
      " [0.00027038]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108104]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108193]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010819]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010818]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027038]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108122]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108125]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010818]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108193]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108184]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108121]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108114]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108184]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108175]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027038]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108179]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108179]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108189]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108176]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108118]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108117]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010818]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108191]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108179]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108187]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108183]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108121]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108172]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108101]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108101]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108172]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108192]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108188]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108115]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108122]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108182]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108114]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108114]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108124]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108181]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108193]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108182]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108192]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108188]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108117]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108087]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108085]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108108]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108186]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108193]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108177]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108117]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108108]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108124]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027062]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108192]]\n",
      "linear.bias:\n",
      " [0.00027063]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108208]]\n",
      "linear.bias:\n",
      " [0.00027059]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108215]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108198]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027036]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027035]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027038]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027063]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027063]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027059]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108107]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108114]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108192]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.001082]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.001082]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010819]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108123]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108102]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108093]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108111]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108175]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108177]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108191]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108199]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108183]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108104]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108092]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108107]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108192]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108211]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108204]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108175]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108108]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108117]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108173]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108184]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027038]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027036]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108181]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108112]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010811]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027059]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027059]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108114]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108117]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108197]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010822]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108217]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108191]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027037]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108114]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108098]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108109]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027061]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027061]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108182]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108185]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108177]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108125]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108122]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108186]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108184]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108173]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027038]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027037]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108172]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108206]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108212]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "Epoch [4500/5000], Loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108194]]\n",
      "linear.bias:\n",
      " [0.00027061]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027062]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.001081]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108094]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108114]]\n",
      "linear.bias:\n",
      " [0.00027034]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027032]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108183]]\n",
      "linear.bias:\n",
      " [0.00027036]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010823]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108247]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108239]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108208]]\n",
      "linear.bias:\n",
      " [0.00027064]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027065]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.0002706]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108081]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108066]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108078]]\n",
      "linear.bias:\n",
      " [0.00027036]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108102]]\n",
      "linear.bias:\n",
      " [0.00027034]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108134]]\n",
      "linear.bias:\n",
      " [0.00027037]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108175]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108188]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108175]]\n",
      "linear.bias:\n",
      " [0.00027059]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.0002706]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108094]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010809]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108099]]\n",
      "linear.bias:\n",
      " [0.00027038]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108186]]\n",
      "linear.bias:\n",
      " [0.00027061]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108212]]\n",
      "linear.bias:\n",
      " [0.00027066]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108226]]\n",
      "linear.bias:\n",
      " [0.00027067]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108229]]\n",
      "linear.bias:\n",
      " [0.00027062]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108223]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108193]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027037]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108108]]\n",
      "linear.bias:\n",
      " [0.00027035]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108089]]\n",
      "linear.bias:\n",
      " [0.00027038]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108083]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108106]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.0002706]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108187]]\n",
      "linear.bias:\n",
      " [0.00027061]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108208]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108202]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108173]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108122]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108103]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108114]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108185]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108177]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108177]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108123]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108184]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108187]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108107]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108107]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108189]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108212]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108208]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108179]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108102]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108093]]\n",
      "linear.bias:\n",
      " [0.00027035]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108097]]\n",
      "linear.bias:\n",
      " [0.00027033]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108113]]\n",
      "linear.bias:\n",
      " [0.00027037]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108139]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108175]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108198]]\n",
      "linear.bias:\n",
      " [0.00027065]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108209]]\n",
      "linear.bias:\n",
      " [0.00027065]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010821]]\n",
      "linear.bias:\n",
      " [0.0002706]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108202]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108113]]\n",
      "linear.bias:\n",
      " [0.00027035]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108076]]\n",
      "linear.bias:\n",
      " [0.00027033]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108055]]\n",
      "linear.bias:\n",
      " [0.00027037]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108063]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.001081]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010819]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108191]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108182]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108183]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108111]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108096]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108112]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108181]]\n",
      "linear.bias:\n",
      " [0.00027059]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108177]]\n",
      "linear.bias:\n",
      " [0.0002706]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108181]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108191]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108172]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108131]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108148]]\n",
      "linear.bias:\n",
      " [0.0002706]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027061]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108136]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010818]]\n",
      "linear.bias:\n",
      " [0.00027037]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108218]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108225]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108206]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108163]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108098]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108069]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108071]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108102]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108159]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108201]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108213]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108197]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108124]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.00027061]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027062]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108192]]\n",
      "linear.bias:\n",
      " [0.00027037]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108193]]\n",
      "linear.bias:\n",
      " [0.00027035]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027033]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027037]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108161]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108107]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108109]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027037]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027035]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027038]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108138]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108113]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108198]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108212]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108197]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108112]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108101]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010812]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108191]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.001082]]\n",
      "linear.bias:\n",
      " [0.00027062]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108199]]\n",
      "linear.bias:\n",
      " [0.00027063]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108188]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108123]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108096]]\n",
      "linear.bias:\n",
      " [0.00027037]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108085]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108104]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108206]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108229]]\n",
      "linear.bias:\n",
      " [0.00027061]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010824]]\n",
      "linear.bias:\n",
      " [0.00027062]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010824]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108213]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108089]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108053]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108051]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108079]]\n",
      "linear.bias:\n",
      " [0.00027038]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108117]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108179]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108183]]\n",
      "linear.bias:\n",
      " [0.00027059]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108177]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010812]]\n",
      "linear.bias:\n",
      " [0.00027037]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108097]]\n",
      "linear.bias:\n",
      " [0.00027035]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108088]]\n",
      "linear.bias:\n",
      " [0.00027038]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108093]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010819]]\n",
      "linear.bias:\n",
      " [0.00027062]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108235]]\n",
      "linear.bias:\n",
      " [0.00027063]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108266]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108266]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010824]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108189]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108116]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108063]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108046]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108061]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108104]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108226]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108246]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108237]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108201]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108142]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108102]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108096]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108121]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108194]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108185]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108167]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108123]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108114]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108102]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108118]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108192]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108192]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108164]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108147]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108171]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108168]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108115]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010811]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108165]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108115]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108199]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108211]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108195]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108104]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108091]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010811]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108182]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108209]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108205]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108192]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108087]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010806]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108067]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108105]]\n",
      "linear.bias:\n",
      " [0.00027037]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108208]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108231]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108223]]\n",
      "linear.bias:\n",
      " [0.00027065]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108205]]\n",
      "linear.bias:\n",
      " [0.00027065]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108179]]\n",
      "linear.bias:\n",
      " [0.0002706]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108145]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108104]]\n",
      "linear.bias:\n",
      " [0.00027035]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108081]]\n",
      "linear.bias:\n",
      " [0.00027027]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108074]]\n",
      "linear.bias:\n",
      " [0.00027026]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108081]]\n",
      "linear.bias:\n",
      " [0.0002703]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108101]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027063]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027064]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027059]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108182]]\n",
      "linear.bias:\n",
      " [0.00027037]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108222]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010823]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108209]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108091]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108058]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108061]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108094]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108156]]\n",
      "linear.bias:\n",
      " [0.00027059]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108202]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108214]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108197]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108181]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108176]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108144]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108146]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108152]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108158]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108175]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108169]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108137]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108186]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108189]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108129]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108162]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108206]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108216]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108197]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108098]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108083]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108102]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108151]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108209]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108232]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108224]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108188]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108126]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108103]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108114]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108184]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108198]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108182]]\n",
      "linear.bias:\n",
      " [0.00027035]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108181]]\n",
      "linear.bias:\n",
      " [0.00027033]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108195]]\n",
      "linear.bias:\n",
      " [0.00027037]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108178]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108107]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108116]]\n",
      "linear.bias:\n",
      " [0.00027061]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108113]]\n",
      "linear.bias:\n",
      " [0.00027062]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.001081]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108121]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108172]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108188]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108174]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108132]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108108]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108101]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.0002706]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027061]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010814]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010813]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108153]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108184]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108201]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108205]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010818]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108127]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108094]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108097]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108198]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108227]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108223]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010819]]\n",
      "linear.bias:\n",
      " [0.0002706]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010815]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108102]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108093]]\n",
      "linear.bias:\n",
      " [0.00027035]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108098]]\n",
      "linear.bias:\n",
      " [0.00027033]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108117]]\n",
      "linear.bias:\n",
      " [0.00027037]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108149]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108192]]\n",
      "linear.bias:\n",
      " [0.00027062]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010822]]\n",
      "linear.bias:\n",
      " [0.00027069]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108233]]\n",
      "linear.bias:\n",
      " [0.00027069]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108235]]\n",
      "linear.bias:\n",
      " [0.00027064]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108225]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108187]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108122]]\n",
      "linear.bias:\n",
      " [0.00027033]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108079]]\n",
      "linear.bias:\n",
      " [0.00027031]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108054]]\n",
      "linear.bias:\n",
      " [0.00027035]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108064]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108107]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010816]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108197]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.001082]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108192]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108091]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108067]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108079]]\n",
      "linear.bias:\n",
      " [0.00027038]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108104]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108207]]\n",
      "linear.bias:\n",
      " [0.00027059]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108256]]\n",
      "linear.bias:\n",
      " [0.00027061]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010827]]\n",
      "linear.bias:\n",
      " [0.00027062]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108253]]\n",
      "linear.bias:\n",
      " [0.00027063]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108207]]\n",
      "linear.bias:\n",
      " [0.00027064]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027059]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108097]]\n",
      "linear.bias:\n",
      " [0.00027048]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108078]]\n",
      "linear.bias:\n",
      " [0.00027038]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108075]]\n",
      "linear.bias:\n",
      " [0.00027036]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108088]]\n",
      "linear.bias:\n",
      " [0.0002704]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108113]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010821]]\n",
      "linear.bias:\n",
      " [0.00027059]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108234]]\n",
      "linear.bias:\n",
      " [0.00027055]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108226]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108189]]\n",
      "linear.bias:\n",
      " [0.00027047]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108125]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108082]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108077]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108106]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108166]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108209]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108217]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108194]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108143]]\n",
      "linear.bias:\n",
      " [0.00027039]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108111]]\n",
      "linear.bias:\n",
      " [0.00027043]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108098]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108119]]\n",
      "linear.bias:\n",
      " [0.00027061]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108128]]\n",
      "linear.bias:\n",
      " [0.00027062]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108124]]\n",
      "linear.bias:\n",
      " [0.00027057]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027052]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027042]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108154]]\n",
      "linear.bias:\n",
      " [0.00027033]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108155]]\n",
      "linear.bias:\n",
      " [0.00027031]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010817]]\n",
      "linear.bias:\n",
      " [0.00027035]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108198]]\n",
      "linear.bias:\n",
      " [0.00027045]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108193]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108157]]\n",
      "linear.bias:\n",
      " [0.00027063]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108114]]\n",
      "linear.bias:\n",
      " [0.00027064]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108064]]\n",
      "linear.bias:\n",
      " [0.00027058]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108053]]\n",
      "linear.bias:\n",
      " [0.00027054]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108077]]\n",
      "linear.bias:\n",
      " [0.0002705]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108133]]\n",
      "linear.bias:\n",
      " [0.00027046]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108198]]\n",
      "linear.bias:\n",
      " [0.00027049]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108226]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108221]]\n",
      "linear.bias:\n",
      " [0.00027053]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108185]]\n",
      "linear.bias:\n",
      " [0.00027056]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108141]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108091]]\n",
      "linear.bias:\n",
      " [0.00027041]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010808]]\n",
      "linear.bias:\n",
      " [0.00027032]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108084]]\n",
      "linear.bias:\n",
      " [0.0002703]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108103]]\n",
      "linear.bias:\n",
      " [0.00027034]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108135]]\n",
      "linear.bias:\n",
      " [0.00027044]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108179]]\n",
      "linear.bias:\n",
      " [0.0002706]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108207]]\n",
      "linear.bias:\n",
      " [0.00027068]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.0010822]]\n",
      "linear.bias:\n",
      " [0.00027068]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108221]]\n",
      "linear.bias:\n",
      " [0.00027063]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n",
      "Epoch [5000/5000], Loss: 0.008707\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight:\n",
      " [[-0.00108211]]\n",
      "linear.bias:\n",
      " [0.00027051]\n",
      "\n",
      "Test loss: 0.006594, Train loss: 0.008707\n"
     ]
    }
   ],
   "execution_count": 546
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Looking at the weight, which is negative, the model picked up a mean reversion adoption.",
   "id": "3d59a254c16e1ff8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Test Trading performance\n",
    "* Create trade results from test data"
   ],
   "id": "989016e0270bb908"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:23:04.223143Z",
     "start_time": "2025-11-06T12:23:04.216058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trade_results = pl.DataFrame({\n",
    "    'y_hat': y_hat.squeeze(),\n",
    "    'y': Y_test.squeeze(),\n",
    "}).with_columns(\n",
    "    (pl.col('y_hat').sign()==pl.col('y').sign()).alias('is_won'),\n",
    "    pl.col('y_hat').sign().alias('signal'),\n",
    ").with_columns(\n",
    "    (pl.col('signal') * pl.col('y')).alias('trade_log_return')\n",
    "    ).with_columns(\n",
    "        pl.col('trade_log_return').cum_sum().alias('equity_curve')\n",
    "    )\n",
    "trade_results"
   ],
   "id": "13cb199752ed99c2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (3_204, 6)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ y_hat     â”† y         â”† is_won â”† signal â”† trade_log_return â”† equity_curve â”‚\n",
       "â”‚ ---       â”† ---       â”† ---    â”† ---    â”† ---              â”† ---          â”‚\n",
       "â”‚ f32       â”† f32       â”† bool   â”† f32    â”† f32              â”† f32          â”‚\n",
       "â•žâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 0.001715  â”† -0.007778 â”† false  â”† 1.0    â”† -0.007778        â”† -0.007778    â”‚\n",
       "â”‚ 0.000889  â”† 0.012369  â”† true   â”† 1.0    â”† 0.012369         â”† 0.004592     â”‚\n",
       "â”‚ -0.000665 â”† -0.00275  â”† true   â”† -1.0   â”† 0.00275          â”† 0.007341     â”‚\n",
       "â”‚ 0.000501  â”† -0.003447 â”† false  â”† 1.0    â”† -0.003447        â”† 0.003894     â”‚\n",
       "â”‚ 0.000555  â”† 0.000303  â”† true   â”† 1.0    â”† 0.000303         â”† 0.004197     â”‚\n",
       "â”‚ â€¦         â”† â€¦         â”† â€¦      â”† â€¦      â”† â€¦                â”† â€¦            â”‚\n",
       "â”‚ 0.000081  â”† -0.001183 â”† false  â”† 1.0    â”† -0.001183        â”† 0.97953      â”‚\n",
       "â”‚ 0.00038   â”† -0.002401 â”† false  â”† 1.0    â”† -0.002401        â”† 0.977129     â”‚\n",
       "â”‚ 0.000474  â”† -0.004357 â”† false  â”† 1.0    â”† -0.004357        â”† 0.972772     â”‚\n",
       "â”‚ 0.000625  â”† 0.000408  â”† true   â”† 1.0    â”† 0.000408         â”† 0.97318      â”‚\n",
       "â”‚ 0.000257  â”† -0.003499 â”† false  â”† 1.0    â”† -0.003499        â”† 0.969682     â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3_204, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>y_hat</th><th>y</th><th>is_won</th><th>signal</th><th>trade_log_return</th><th>equity_curve</th></tr><tr><td>f32</td><td>f32</td><td>bool</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>0.001715</td><td>-0.007778</td><td>false</td><td>1.0</td><td>-0.007778</td><td>-0.007778</td></tr><tr><td>0.000889</td><td>0.012369</td><td>true</td><td>1.0</td><td>0.012369</td><td>0.004592</td></tr><tr><td>-0.000665</td><td>-0.00275</td><td>true</td><td>-1.0</td><td>0.00275</td><td>0.007341</td></tr><tr><td>0.000501</td><td>-0.003447</td><td>false</td><td>1.0</td><td>-0.003447</td><td>0.003894</td></tr><tr><td>0.000555</td><td>0.000303</td><td>true</td><td>1.0</td><td>0.000303</td><td>0.004197</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>0.000081</td><td>-0.001183</td><td>false</td><td>1.0</td><td>-0.001183</td><td>0.97953</td></tr><tr><td>0.00038</td><td>-0.002401</td><td>false</td><td>1.0</td><td>-0.002401</td><td>0.977129</td></tr><tr><td>0.000474</td><td>-0.004357</td><td>false</td><td>1.0</td><td>-0.004357</td><td>0.972772</td></tr><tr><td>0.000625</td><td>0.000408</td><td>true</td><td>1.0</td><td>0.000408</td><td>0.97318</td></tr><tr><td>0.000257</td><td>-0.003499</td><td>false</td><td>1.0</td><td>-0.003499</td><td>0.969682</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 547
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:23:04.369028Z",
     "start_time": "2025-11-06T12:23:04.277733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# plot equity curve\n",
    "# Convert the Series to a NumPy array for plotting\n",
    "y = trade_results['equity_curve'].to_numpy()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(y)\n",
    "plt.title(\"Equity Curve\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Equity\")\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "cdd15c9299fa973a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 548
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:23:04.412402Z",
     "start_time": "2025-11-06T12:23:04.407533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# performance checking\n",
    "trade_results = trade_results.with_columns(\n",
    "    (pl.col('equity_curve')-pl.col('equity_curve').cum_max()).alias('drawdown_log')\n",
    ")\n",
    "trade_results"
   ],
   "id": "4b8d56cc8198e1d1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (3_204, 7)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ y_hat     â”† y         â”† is_won â”† signal â”† trade_log_return â”† equity_curve â”† drawdown_log â”‚\n",
       "â”‚ ---       â”† ---       â”† ---    â”† ---    â”† ---              â”† ---          â”† ---          â”‚\n",
       "â”‚ f32       â”† f32       â”† bool   â”† f32    â”† f32              â”† f32          â”† f32          â”‚\n",
       "â•žâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 0.001715  â”† -0.007778 â”† false  â”† 1.0    â”† -0.007778        â”† -0.007778    â”† 0.0          â”‚\n",
       "â”‚ 0.000889  â”† 0.012369  â”† true   â”† 1.0    â”† 0.012369         â”† 0.004592     â”† 0.0          â”‚\n",
       "â”‚ -0.000665 â”† -0.00275  â”† true   â”† -1.0   â”† 0.00275          â”† 0.007341     â”† 0.0          â”‚\n",
       "â”‚ 0.000501  â”† -0.003447 â”† false  â”† 1.0    â”† -0.003447        â”† 0.003894     â”† -0.003447    â”‚\n",
       "â”‚ 0.000555  â”† 0.000303  â”† true   â”† 1.0    â”† 0.000303         â”† 0.004197     â”† -0.003144    â”‚\n",
       "â”‚ â€¦         â”† â€¦         â”† â€¦      â”† â€¦      â”† â€¦                â”† â€¦            â”† â€¦            â”‚\n",
       "â”‚ 0.000081  â”† -0.001183 â”† false  â”† 1.0    â”† -0.001183        â”† 0.97953      â”† -0.138885    â”‚\n",
       "â”‚ 0.00038   â”† -0.002401 â”† false  â”† 1.0    â”† -0.002401        â”† 0.977129     â”† -0.141286    â”‚\n",
       "â”‚ 0.000474  â”† -0.004357 â”† false  â”† 1.0    â”† -0.004357        â”† 0.972772     â”† -0.145643    â”‚\n",
       "â”‚ 0.000625  â”† 0.000408  â”† true   â”† 1.0    â”† 0.000408         â”† 0.97318      â”† -0.145235    â”‚\n",
       "â”‚ 0.000257  â”† -0.003499 â”† false  â”† 1.0    â”† -0.003499        â”† 0.969682     â”† -0.148734    â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3_204, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>y_hat</th><th>y</th><th>is_won</th><th>signal</th><th>trade_log_return</th><th>equity_curve</th><th>drawdown_log</th></tr><tr><td>f32</td><td>f32</td><td>bool</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>0.001715</td><td>-0.007778</td><td>false</td><td>1.0</td><td>-0.007778</td><td>-0.007778</td><td>0.0</td></tr><tr><td>0.000889</td><td>0.012369</td><td>true</td><td>1.0</td><td>0.012369</td><td>0.004592</td><td>0.0</td></tr><tr><td>-0.000665</td><td>-0.00275</td><td>true</td><td>-1.0</td><td>0.00275</td><td>0.007341</td><td>0.0</td></tr><tr><td>0.000501</td><td>-0.003447</td><td>false</td><td>1.0</td><td>-0.003447</td><td>0.003894</td><td>-0.003447</td></tr><tr><td>0.000555</td><td>0.000303</td><td>true</td><td>1.0</td><td>0.000303</td><td>0.004197</td><td>-0.003144</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>0.000081</td><td>-0.001183</td><td>false</td><td>1.0</td><td>-0.001183</td><td>0.97953</td><td>-0.138885</td></tr><tr><td>0.00038</td><td>-0.002401</td><td>false</td><td>1.0</td><td>-0.002401</td><td>0.977129</td><td>-0.141286</td></tr><tr><td>0.000474</td><td>-0.004357</td><td>false</td><td>1.0</td><td>-0.004357</td><td>0.972772</td><td>-0.145643</td></tr><tr><td>0.000625</td><td>0.000408</td><td>true</td><td>1.0</td><td>0.000408</td><td>0.97318</td><td>-0.145235</td></tr><tr><td>0.000257</td><td>-0.003499</td><td>false</td><td>1.0</td><td>-0.003499</td><td>0.969682</td><td>-0.148734</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 549
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:23:04.540849Z",
     "start_time": "2025-11-06T12:23:04.535544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Max DD log\n",
    "\n",
    "max_drawdown_log = trade_results['drawdown_log'].min()\n",
    "max_drawdown_log"
   ],
   "id": "dc4d312e4393d46f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3377569913864136"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 550
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:23:04.657457Z",
     "start_time": "2025-11-06T12:23:04.652796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Putting into simple returns\n",
    "drawdown_pct = np.exp(max_drawdown_log)-1\n",
    "drawdown_pct"
   ],
   "id": "16249152c479d0b2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-0.28663137844365627)"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 551
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:23:04.724506Z",
     "start_time": "2025-11-06T12:23:04.719202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "equity_peak = 1000\n",
    "equity_peak * drawdown_pct"
   ],
   "id": "4631e8c4e23860c7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-286.6313784436563)"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 552
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:23:04.754198Z",
     "start_time": "2025-11-06T12:23:04.749197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Win rate\n",
    "win_rate = trade_results['is_won'].mean()\n",
    "win_rate"
   ],
   "id": "2d9b507adaac0064",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5271535580524345"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 553
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:23:04.793233Z",
     "start_time": "2025-11-06T12:23:04.786342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Expected Value\n",
    "avg_win = trade_results.filter(pl.col('is_won')==True)['trade_log_return'].mean()\n",
    "avg_loss = trade_results.filter(pl.col('is_won')==False)['trade_log_return'].mean()\n",
    "ev= win_rate * avg_win + (1-win_rate) * avg_loss\n",
    "ev"
   ],
   "id": "d468061f8e991eff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003026475168193313"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 554
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:23:04.814782Z",
     "start_time": "2025-11-06T12:23:04.810562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# total log return\n",
    "total_log_return = trade_results['trade_log_return'].sum()\n",
    "total_log_return"
   ],
   "id": "2f17e20df0efa498",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9696826934814453"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 555
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:23:04.837546Z",
     "start_time": "2025-11-06T12:23:04.832845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "compound_return = np.exp(total_log_return)\n",
    "compound_return"
   ],
   "id": "56891aaf997e9185",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.6371075551662133)"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 556
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:23:04.884933Z",
     "start_time": "2025-11-06T12:23:04.880026Z"
    }
   },
   "cell_type": "code",
   "source": "1000*compound_return",
   "id": "252ec1fb55cf7c37",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2637.1075551662134)"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 557
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:23:04.932431Z",
     "start_time": "2025-11-06T12:23:04.927423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Equity trough\n",
    "\n",
    "equity_trough = trade_results['equity_curve'].min()\n",
    "equity_trough"
   ],
   "id": "5834b73f78d2269f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.21833735704421997"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 558
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:23:04.976023Z",
     "start_time": "2025-11-06T12:23:04.969013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Equity peak\n",
    "\n",
    "equity_peak = trade_results['equity_curve'].max()\n",
    "equity_peak"
   ],
   "id": "93602c49574cca65",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1184154748916626"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 559
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:23:05.034361Z",
     "start_time": "2025-11-06T12:23:05.029361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# std\n",
    "std = trade_results['trade_log_return'].std()\n",
    "std"
   ],
   "id": "3160c0be5c9d61bd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009925328195095062"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 560
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T13:43:08.212885Z",
     "start_time": "2025-11-06T13:43:08.208200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sharpe\n",
    "annualized_rate = np.sqrt(365*24/4)\n",
    "sharpe = ev / std * annualized_rate\n",
    "sharpe\n"
   ],
   "id": "fa947f0b16eac6a7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.4269682037930358)"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 565
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:23:05.117748Z",
     "start_time": "2025-11-06T12:23:05.087915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Highlighting itertools\n",
    "import itertools\n",
    "max_lags = 4\n",
    "benchmarks = []\n",
    "feature_pool = [f'{target}_lag_{i}' for i in range(1, max_lags + 1)]\n",
    "combos = list(itertools.combinations(feature_pool, 1))\n",
    "\n",
    "for features in combos:\n",
    "    model = LinearModel(len(features))\n",
    "    benchmarks.append()(ts,list(features),target, model, annualized_rate,no_epochs=200,loss=nn.L1Loss())\n",
    "\n",
    "    benchmarks = pl.DataFrame(benchmarks)\n",
    "\n",
    "\n"
   ],
   "id": "2e28eb9d7d5c2d6a",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list.append() takes exactly one argument (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[562]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m features \u001B[38;5;129;01min\u001B[39;00m combos:\n\u001B[32m      9\u001B[39m     model = LinearModel(\u001B[38;5;28mlen\u001B[39m(features))\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m     \u001B[43mbenchmarks\u001B[49m\u001B[43m.\u001B[49m\u001B[43mappend\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m(ts,\u001B[38;5;28mlist\u001B[39m(features),target, model, annualized_rate,no_epochs=\u001B[32m200\u001B[39m,loss=nn.L1Loss())\n\u001B[32m     12\u001B[39m     benchmarks = pl.DataFrame(benchmarks)\n",
      "\u001B[31mTypeError\u001B[39m: list.append() takes exactly one argument (0 given)"
     ]
    }
   ],
   "execution_count": 562
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:23:05.131752500Z",
     "start_time": "2025-11-06T11:24:19.279Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7c3c0b8533005d13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Adding Transaction Fees",
   "id": "ce996a2972c59427"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:24:05.108174Z",
     "start_time": "2025-11-06T12:24:05.101141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2 types of fees:\n",
    "# 1: maker fee and taker fee\n",
    "\n",
    "maker_fee = 0.0001\n",
    "taker_fee = 0.0003\n",
    "\n",
    "roundtrip_fee_log = np.log(1-2*maker_fee)\n",
    "\n",
    "trade_results = trade_results.with_columns(pl.lit(roundtrip_fee_log).alias('tx_fee_log'))\n",
    "trade_results = trade_results.with_columns((pl.col('trade_log_return')+pl.col('tx_fee_log')).alias('trade_log_return_net'))\n",
    "trade_results = trade_results.with_columns(pl.col('trade_log_return_net').cum_sum().alias('equity_curve_net'))\n",
    "trade_results\n",
    "\n",
    "roundtrip_fee_log\n",
    "#trade_results\n"
   ],
   "id": "61d4cfb19a2fb542",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-0.0002000200026670447)"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 563
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Our Initial time_interval resulted in a negative equity after fees. I adjusted in the following ways\n",
    "### tweaks\n",
    "* Increased time horizon from 1H to 4H and used the lag_1 feature.After scanning through all single features, lag_1 resulted in highest performance\n",
    "* We can add further features next\n",
    "* We can explore correlation next\n",
    "\n",
    "### I have adjusted the lag to 1 and put the time_interval to 4h. This has increased the sharpe and net equity."
   ],
   "id": "9b67de23ef499fc1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:24:06.951504Z",
     "start_time": "2025-11-06T12:24:06.879957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = trade_results['equity_curve_net'].to_numpy()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(y)\n",
    "plt.title(\"Equity Curve Net\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Equity\")\n",
    "plt.show()"
   ],
   "id": "8687ebc481d14742",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 564
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T12:23:05.141888600Z",
     "start_time": "2025-11-06T12:13:04.979301Z"
    }
   },
   "cell_type": "code",
   "source": "trade_results['is_won'].mean()",
   "id": "4463ead69a70ef05",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5271535580524345"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 398
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Now to create my own research module to adjust models and their functions without changing the full model.\n",
    "### This will assist in identifying strength of model between features."
   ],
   "id": "d62229645e191fff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%sql\n",
    "# Save"
   ],
   "id": "73821151e70cad6a",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "75605fb220a9a9be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
